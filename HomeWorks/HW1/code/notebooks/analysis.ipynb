{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0bd2e2c4",
      "metadata": {},
      "source": [
        "# HW1 Complete Notebook: Generalization, Robustness, Theory, Code, and Results\n",
        "\n",
        "This notebook is intentionally complete and includes all major parts of the HW1 deliverable:\n",
        "\n",
        "- full theoretical foundations,\n",
        "- complete code walkthrough by module,\n",
        "- reproducibility protocol and optional rerun command,\n",
        "- quantitative metrics/tables loaded from generated artifacts,\n",
        "- every core report plot with a full interpretation paragraph,\n",
        "- assignment requirement coverage matrix,\n",
        "- extended appendix theory and limitations.\n",
        "\n",
        "Use this notebook as the single, end-to-end technical reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940a80a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and helper utilities\n",
        "import csv\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    start = start.resolve()\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / 'code').exists() and (p / 'report').exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "\n",
        "repo_root = find_repo_root(Path.cwd())\n",
        "code_dir = repo_root / 'code'\n",
        "report_dir = repo_root / 'report'\n",
        "fig_dir = report_dir / 'figures'\n",
        "summary_dir = code_dir / 'checkpoints' / 'report_summary'\n",
        "metrics_path = fig_dir / 'metrics_summary.json'\n",
        "\n",
        "\n",
        "def load_json(path: Path):\n",
        "    return json.loads(path.read_text()) if path.exists() else None\n",
        "\n",
        "\n",
        "def load_csv(path: Path):\n",
        "    rows = []\n",
        "    if not path.exists():\n",
        "        return rows\n",
        "    with path.open('r', newline='') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        rows.extend(reader)\n",
        "    return rows\n",
        "\n",
        "\n",
        "def markdown_table(rows, columns):\n",
        "    header = '| ' + ' | '.join(columns) + ' |\\n'\n",
        "    sep = '| ' + ' | '.join(['---'] * len(columns)) + ' |\\n'\n",
        "    body = ''.join('| ' + ' | '.join(str(r.get(c, '')) for c in columns) + ' |\\n' for r in rows)\n",
        "    return header + sep + body\n",
        "\n",
        "\n",
        "def show_image(filename: str, width: int = 11):\n",
        "    path = fig_dir / filename\n",
        "    if not path.exists():\n",
        "        print(f'[MISSING] {path}')\n",
        "        return\n",
        "    img = plt.imread(path)\n",
        "    plt.figure(figsize=(width, 4.5))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(filename)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_snippet(rel_path: str, start: int, end: int):\n",
        "    p = repo_root / rel_path\n",
        "    if not p.exists():\n",
        "        print(f'[MISSING] {rel_path}')\n",
        "        return\n",
        "    lines = p.read_text().splitlines()\n",
        "    start = max(1, start)\n",
        "    end = min(len(lines), end)\n",
        "    print(f'\\n--- {rel_path}:{start}-{end} ---')\n",
        "    for i in range(start, end + 1):\n",
        "        print(f'{i:4d}: {lines[i-1]}')\n",
        "\n",
        "\n",
        "print('repo_root:', repo_root)\n",
        "print('metrics :', metrics_path)\n",
        "print('figures :', fig_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66ea2a0",
      "metadata": {},
      "source": [
        "## 1) Problem Scope and Notebook Plan\n",
        "\n",
        "The notebook follows the same complete logic as the IEEE report:\n",
        "\n",
        "1. Theory for generalization, robustness, calibration, and representation geometry.\n",
        "2. Full implementation explanation for `train.py`, `eval.py`, `attacks.py`, `losses.py`, `datasets.py`, `run_report_pipeline.py`, and model code.\n",
        "3. Reproducibility instructions and runnable command cells.\n",
        "4. Quantitative diagnostics from saved artifacts.\n",
        "5. Plot-by-plot interpretation with one full paragraph each.\n",
        "6. Requirement coverage matrix and appendix-level theory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7327bbb",
      "metadata": {},
      "source": [
        "### 1.1 Dataset Statistics and Transform Contract\nThis table makes the preprocessing contract explicit because input scale, channel count, and normalization statistics directly condition optimization and affect comparability across runs. The fallback FakeData path inherits the same normalization logic so that pipeline behavior stays consistent even when external datasets are unavailable.\n\n| Dataset | Size | Channels | Classes | Normalization (mean/std) |\n|---|---|---|---|---|\n| SVHN | 32x32 | 3 | 10 | (0.4377, 0.4438, 0.4728) / (0.1980, 0.2010, 0.1970) |\n| MNIST | 32x32 | 1->3 | 10 | (0.1307) / (0.3081) |\n| CIFAR10 | 32x32 | 3 | 10 | (0.4914, 0.4822, 0.4465) / (0.2470, 0.2435, 0.2616) |\n| Fallback FakeData | 32x32 | 3 | 10 | inherits SVHN/MNIST transform |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b138667f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# High-level assignment coverage table\n",
        "coverage = [\n",
        "    {'Requirement': 'Custom ResNet18', 'Status': 'Complete', 'Evidence': 'code/models/resnet18_custom.py'},\n",
        "    {'Requirement': 'BatchNorm ablation', 'Status': 'Complete', 'Evidence': 'training_summary.csv + report sections'},\n",
        "    {'Requirement': 'Label smoothing', 'Status': 'Complete', 'Evidence': 'code/losses.py + experiment matrix'},\n",
        "    {'Requirement': 'Optimizer comparison', 'Status': 'Complete', 'Evidence': 'SGD/Adam rows in training summary'},\n",
        "    {'Requirement': 'Cross-domain transfer (both directions)', 'Status': 'Complete', 'Evidence': 'cross_domain_summary.csv'},\n",
        "    {'Requirement': 'FGSM + PGD robustness evaluation', 'Status': 'Complete', 'Evidence': 'eval.py + sweep plots'},\n",
        "    {'Requirement': 'Adversarial training variants', 'Status': 'Complete', 'Evidence': 'run matrix + robustness summary'},\n",
        "    {'Requirement': 'Circle Loss theory/code', 'Status': 'Complete', 'Evidence': 'losses.py + theory sections'},\n",
        "    {'Requirement': 'Full plot explanations', 'Status': 'Complete', 'Evidence': 'sections below for all figures'},\n",
        "    {'Requirement': 'Reproducibility protocol', 'Status': 'Complete', 'Evidence': 'command cells below'},\n",
        "]\n",
        "\n",
        "display(Markdown(markdown_table(coverage, ['Requirement', 'Status', 'Evidence'])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f59a2fc",
      "metadata": {},
      "source": [
        "## 2) Theoretical Foundations (Complete)\n",
        "\n",
        "### 2.1 Generalization and domain shift\n",
        "For source and target domains, the practical transfer view is:\n",
        "\n",
        "$$\n",
        "R_t(f) \\\\le R_s(f) + \\\\frac{1}{2} d_{\\\\mathcal{H}\\\\Delta\\\\mathcal{H}}(D_s,D_t) + \\\\lambda^*.\n",
        "$$\n",
        "\n",
        "So low source risk alone is insufficient; mismatch between source and target distributions must be controlled or measured.\n",
        "\n",
        "### 2.2 Robust optimization and adversarial attacks\n",
        "Robust learning is naturally written as:\n",
        "\n",
        "$$\n",
        "\\\\min_\\\\theta \\\\; \\\\mathbb{E}_{(x,y)}\\\\left[\\\\max_{\\\\|\\\\delta\\\\|_\\\\infty \\\\le \\\\epsilon} \\\\ell(f_\\\\theta(x+\\\\delta), y)\\\\right].\n",
        "$$\n",
        "\n",
        "FGSM approximates the inner maximization with a single sign-gradient step; PGD performs iterative projected ascent and is therefore a stronger attack.\n",
        "\n",
        "### 2.3 Calibration and selective prediction\n",
        "Expected Calibration Error (ECE):\n",
        "\n",
        "$$\n",
        "\\\\mathrm{ECE}=\\\\sum_{m=1}^{M}\\\\frac{|B_m|}{n}\\\\left|\\\\mathrm{acc}(B_m)-\\\\mathrm{conf}(B_m)\\\\right|.\n",
        "$$\n",
        "\n",
        "Selective prediction evaluates risk as a function of retained coverage; AURC summarizes this operational curve.\n",
        "\n",
        "### 2.4 Why top-k and PRF1 are required\n",
        "Top-k reveals ranking quality beyond top-1. PRF1 reveals class-conditional error structure (false positives vs false negatives). Together they provide a more complete trust profile than accuracy alone.\n",
        "\n",
        "### 2.5 Circle Loss perspective\n",
        "Circle Loss encourages geometric structure in embedding space by improving intra-class compactness and inter-class separation, which is theoretically aligned with robust decision boundaries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce58bb7",
      "metadata": {},
      "source": [
        "### 2.4 Structured Error Taxonomy\nFor deployment-oriented interpretation, errors are categorized by operational impact. False positives correspond to spurious alarms and can trigger unnecessary interventions, while false negatives correspond to missed detections and can be safety-critical in high-stakes contexts. Misranked but top-k-correct cases indicate partial evidence without decisive boundaries, suggesting that reranking or calibration could recover utility without full retraining. Overconfident errors indicate a mismatch between uncertainty and correctness and are particularly harmful because they undermine selective prediction and human-override policies. This taxonomy connects PRF1, top-k, and calibration diagnostics into a single actionable framework for triaging model improvements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "248a336c",
      "metadata": {},
      "source": [
        "## 3) Full Code Explanation by Module\n",
        "\n",
        "This section inspects the real code to explain how each part contributes to the final pipeline and report outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007fad68",
      "metadata": {},
      "source": [
        "### 3.0 Module Input/Output Contract\nThis table makes each module auditable by stating its inputs and the artifacts it produces. It is the quickest way to understand how data, gradients, and evaluation outputs flow through the pipeline.\n\n| Module | Inputs | Outputs / Side Effects |\n|---|---|---|\n| `datasets.py` | dataset name, batch size, augment flag | dataloaders, class count, channel count |\n| `models/resnet18_custom.py` | input tensors, BN flag | logits, optional features |\n| `losses.py` | logits + labels | loss scalar (CE/LS/Circle) |\n| `attacks.py` | model, inputs, epsilon, alpha | adversarial inputs (clipped) |\n| `train.py` | args, dataloaders | checkpoints, history CSV/JSON, training curves |\n| `eval.py` | checkpoint, dataloader | plots, metrics JSON, sweeps |\n| `run_report_pipeline.py` | args | copied report figures + metrics summary |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25c820d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Module inventory and function summary\n",
        "module_files = [\n",
        "    'code/models/resnet18_custom.py',\n",
        "    'code/datasets.py',\n",
        "    'code/losses.py',\n",
        "    'code/attacks.py',\n",
        "    'code/train.py',\n",
        "    'code/eval.py',\n",
        "    'code/run_report_pipeline.py',\n",
        "    'code/utils.py',\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for rel in module_files:\n",
        "    p = repo_root / rel\n",
        "    if not p.exists():\n",
        "        rows.append({'Module': rel, 'Lines': 'MISSING', 'Functions': '-', 'Role': 'Missing file'})\n",
        "        continue\n",
        "    text = p.read_text()\n",
        "    funcs = re.findall(r'^def\\s+([a-zA-Z0-9_]+)\\(', text, flags=re.MULTILINE)\n",
        "    role = {\n",
        "        'code/models/resnet18_custom.py': 'Custom architecture (ResNet18 blocks, BN toggle, feature extraction path)',\n",
        "        'code/datasets.py': 'Transforms/loaders and deterministic fallback dataset behavior',\n",
        "        'code/losses.py': 'Label smoothing and Circle Loss definitions',\n",
        "        'code/attacks.py': 'FGSM/PGD perturbation generation with projection/clamp',\n",
        "        'code/train.py': 'Training/validation loop, scheduler, checkpointing, history export',\n",
        "        'code/eval.py': 'All diagnostic figures and scalar metrics export',\n",
        "        'code/run_report_pipeline.py': 'One-command automation for report artifacts',\n",
        "        'code/utils.py': 'Seeds and checkpoint helper utilities',\n",
        "    }.get(rel, '')\n",
        "    rows.append({'Module': rel, 'Lines': len(text.splitlines()), 'Functions': ', '.join(funcs[:10]) + (' ...' if len(funcs) > 10 else ''), 'Role': role})\n",
        "\n",
        "display(Markdown(markdown_table(rows, ['Module', 'Lines', 'Functions', 'Role'])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9984832",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key snippets for complete implementation understanding\n",
        "snippet_requests = [\n",
        "    ('code/models/resnet18_custom.py', 1, 220),\n",
        "    ('code/train.py', 1, 260),\n",
        "    ('code/losses.py', 1, 220),\n",
        "    ('code/attacks.py', 1, 220),\n",
        "    ('code/eval.py', 1, 340),\n",
        "    ('code/eval.py', 340, 760),\n",
        "    ('code/run_report_pipeline.py', 1, 220),\n",
        "]\n",
        "for rel, s, e in snippet_requests:\n",
        "    print_snippet(rel, s, e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e813eeb",
      "metadata": {},
      "source": [
        "### 3.1 Code-Flow Interpretation\n",
        "\n",
        "The operational flow is: data/transforms -> model/loss/attacks -> train loop -> checkpointing -> evaluation diagnostics -> report artifact copy. This separation is important because it allows each requirement (generalization, robustness, calibration, class-conditional behavior) to be validated independently while still being generated from one consistent pipeline. The notebook intentionally inspects both architecture/training code and evaluation/reporting code so conceptual claims and generated evidence stay tightly coupled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f6f36dc",
      "metadata": {},
      "source": [
        "## 4) Reproducibility Commands\n",
        "\n",
        "Use the same environment and command contract as the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a4599d",
      "metadata": {},
      "outputs": [],
      "source": [
        "commands = [\n",
        "    'source /Users/tahamajs/Documents/uni/venv/bin/activate',\n",
        "    'export MPLCONFIGDIR=/tmp/mplconfig',\n",
        "    'python code/run_report_pipeline.py --epochs 3',\n",
        "    'python code/run_report_pipeline.py --full-run --epochs 80 --dataset svhn',\n",
        "]\n",
        "print('Reproducibility commands:')\n",
        "for c in commands:\n",
        "    print('-', c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa90f9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional rerun cell (disabled by default for fast notebook execution)\n",
        "RUN_PIPELINE = False\n",
        "\n",
        "if RUN_PIPELINE:\n",
        "    cmd = (\n",
        "        'source /Users/tahamajs/Documents/uni/venv/bin/activate && '\n",
        "        'export MPLCONFIGDIR=/tmp/mplconfig && '\n",
        "        'python code/run_report_pipeline.py --epochs 3'\n",
        "    )\n",
        "    subprocess.run(['bash', '-lc', cmd], cwd=repo_root, check=True)\n",
        "    print('Pipeline run completed.')\n",
        "else:\n",
        "    print('Pipeline run skipped. Set RUN_PIPELINE=True to regenerate artifacts.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "056a138c",
      "metadata": {},
      "source": [
        "## 5) Load Metrics and Summary Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a0ee8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = load_json(metrics_path)\n",
        "training_summary = load_csv(summary_dir / 'training_summary.csv')\n",
        "cross_domain_summary = load_csv(summary_dir / 'cross_domain_summary.csv')\n",
        "robustness_summary = load_csv(summary_dir / 'robustness_summary.csv')\n",
        "\n",
        "print('metrics loaded:', metrics is not None)\n",
        "print('training rows:', len(training_summary))\n",
        "print('cross-domain rows:', len(cross_domain_summary))\n",
        "print('robustness rows:', len(robustness_summary))\n",
        "\n",
        "if metrics is None:\n",
        "    raise FileNotFoundError(f'Missing {metrics_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f5205d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core metric dashboard and derived indicators\n",
        "topk = metrics.get('topk_accuracy', {})\n",
        "conf = metrics.get('confidence_coverage', {})\n",
        "rob = metrics.get('robustness_sweep', {})\n",
        "pgd_iter = metrics.get('pgd_iter_sweep', {})\n",
        "\n",
        "metric_rows = [\n",
        "    {'Metric': 'Clean Accuracy (%)', 'Value': f\"{metrics.get('clean_acc', float('nan')):.4f}\"},\n",
        "    {'Metric': 'ECE', 'Value': f\"{metrics.get('ece', float('nan')):.4f}\"},\n",
        "    {'Metric': 'AURC', 'Value': f\"{metrics.get('aurc', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Top-1 (%)', 'Value': f\"{topk.get('top1', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Top-2 (%)', 'Value': f\"{topk.get('top2', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Top-3 (%)', 'Value': f\"{topk.get('top3', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Top-5 (%)', 'Value': f\"{topk.get('top5', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Macro Precision (%)', 'Value': f\"{metrics.get('macro_precision', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Macro Recall (%)', 'Value': f\"{metrics.get('macro_recall', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Macro F1 (%)', 'Value': f\"{metrics.get('macro_f1', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Acc @80% coverage (%)', 'Value': f\"{conf.get('acc_at_80_coverage', float('nan')):.4f}\"},\n",
        "    {'Metric': 'Acc @90% coverage (%)', 'Value': f\"{conf.get('acc_at_90_coverage', float('nan')):.4f}\"},\n",
        "]\n",
        "\n",
        "display(Markdown(markdown_table(metric_rows, ['Metric', 'Value'])))\n",
        "\n",
        "clean = float(metrics.get('clean_acc', np.nan))\n",
        "top1 = float(topk.get('top1', np.nan))\n",
        "top5 = float(topk.get('top5', np.nan))\n",
        "print(f'Top-5 gain over Top-1: {top5 - top1:.4f} points')\n",
        "\n",
        "if rob.get('fgsm_acc'):\n",
        "    print(f'Clean - FGSM(first epsilon): {clean - float(rob[\"fgsm_acc\"][0]):.4f} points')\n",
        "if rob.get('pgd_acc'):\n",
        "    print(f'Clean - PGD(first epsilon): {clean - float(rob[\"pgd_acc\"][0]):.4f} points')\n",
        "if pgd_iter.get('pgd_acc'):\n",
        "    vals = [float(v) for v in pgd_iter['pgd_acc']]\n",
        "    print(f'PGD iteration sensitivity (max-min): {max(vals)-min(vals):.4f} points')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8ddb23",
      "metadata": {},
      "source": [
        "## 6) Plot-by-Plot Results (Each in One Full Paragraph)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521c9793",
      "metadata": {},
      "source": [
        "### 6.1 Training Curves\n",
        "\n",
        "The training-curve panel summarizes optimization dynamics and short-horizon generalization behavior in a way that endpoint metrics alone cannot. In this run, training accuracy improves while validation behavior remains unstable and does not settle into a strong discriminative regime, indicating that the implementation is learning some structure but remains limited by data regime and run horizon. The important point for this notebook is methodological validity: scheduler updates, checkpointing, logging, and metric export are coherent and reproducible, so weak benchmark quality is attributable to experimental conditions rather than broken pipeline logic. This distinction is essential for trusted experimentation because it preserves confidence in the measurement system while motivating longer/full-data runs for stronger scientific conclusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33359a77",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('training_curves.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d9a0bc",
      "metadata": {},
      "source": [
        "### 6.2 Feature Projection\n",
        "\n",
        "The feature projection (UMAP/PCA fallback) visualizes latent geometry and directly tests whether class-conditionals are separable in embedding space. The observed overlap among classes matches the modest classification behavior and indicates that representation-level separation is currently limited, which is consistent with fallback or short-horizon settings. This plot is valuable because it disambiguates causes of weak top-1 performance: if embeddings are already well-separated then classification head tuning is the target, but if embeddings are diffuse then representation learning is the bottleneck. Therefore, this figure provides a geometry-grounded lens for interpreting BN, smoothing, optimizer, and adversarial training effects in future runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e5fd08",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('umap_features.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4504d0fb",
      "metadata": {},
      "source": [
        "### 6.3 Adversarial Sample Grid\n",
        "\n",
        "The adversarial sample grid provides a concrete behavioral comparison between clean inputs, adversarially perturbed inputs, and random-noise baselines with synchronized predictions. Its role is not only qualitative presentation but engineering verification that attack generation, clipping constraints, denormalization, and prediction annotation are all aligned in one output artifact. Even under fallback conditions, the grid confirms that the robustness path is operational and that perturbation-based evaluation is not a placeholder. This matters because trusted robustness analysis requires confidence in tooling consistency before numerical attack comparisons can be interpreted scientifically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ceb9e36",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('adv_examples.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac941afe",
      "metadata": {},
      "source": [
        "### 6.4 Confusion Matrix\n",
        "\n",
        "The confusion matrix decomposes aggregate accuracy into class-conditional decision behavior and reveals where prediction mass concentrates. In this run, diagonal dominance is weak and error mass is unevenly distributed, indicating that model behavior is not uniformly discriminative across classes. This directly explains why top-line accuracy alone is insufficient for trusted interpretation: global metrics can hide concentrated failures on specific labels. By exposing these class-wise channels, the confusion matrix acts as a structural risk diagnostic rather than a decorative summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b6dc7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('confusion_matrix.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa07582",
      "metadata": {},
      "source": [
        "### 6.5 Per-Class Accuracy\n",
        "\n",
        "Per-class accuracy converts confusion structure into a direct class-wise profile and clarifies whether performance is balanced or concentrated. The current profile shows strong inequality across classes, which implies that conditional risks differ substantially and that average accuracy masks important operational behavior. This is theoretically aligned with conditional-risk decomposition, where system-level reliability depends on how risk is distributed across classes, not only on overall mean correctness. As a result, this plot is central for choosing interventions that improve weak classes rather than merely improving already strong ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23666eea",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('per_class_accuracy.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203e279b",
      "metadata": {},
      "source": [
        "### 6.6 Class-wise PRF1\n",
        "\n",
        "The class-wise PRF1 plot extends class analysis from correctness to error-type decomposition by separating precision, recall, and F1 per class. This separation is critical because two classes with similar accuracy can have very different false-positive and false-negative profiles, which correspond to different downstream risks. In the current results, macro scores remain low and class behavior is imbalanced, indicating weak balanced discrimination despite pockets of recall. This diagnostic therefore provides actionable direction for whether future work should prioritize precision control, recall recovery, or balanced loss shaping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce9d96f",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('classwise_prf1.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c36a9ec3",
      "metadata": {},
      "source": [
        "### 6.7 Top-k Accuracy\n",
        "\n",
        "Top-k accuracy evaluates ranking quality and reveals information that strict top-1 hides. The monotonic gain from top-1 to top-5 indicates that the model often ranks the correct label near the top even when final argmax is wrong, which suggests partial semantic ordering in the learned representation. This is practically meaningful for candidate-list workflows, human-in-the-loop verification, and fallback ranking systems. Consequently, top-k functions as an important bridge metric between weak hard decisions and potentially useful probabilistic ranking behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af88b11a",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('topk_accuracy.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2fa2bbe",
      "metadata": {},
      "source": [
        "### 6.8 Reliability Diagram\n",
        "\n",
        "The reliability diagram compares confidence to empirical correctness and exposes miscalibration geometry along with confidence mass distribution. In this run, high calibration error indicates that confidence values are not trustworthy as direct probability estimates of correctness, so naive thresholding could produce unsafe decision behavior. This makes calibration a first-class requirement rather than a secondary metric, especially in trusted systems where confidence drives abstention or escalation rules. The diagram is therefore essential for uncertainty-quality auditing, not just post-hoc visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bec8b01",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('reliability_diagram.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39fd54d",
      "metadata": {},
      "source": [
        "### 6.9 Robustness Sweep\n",
        "\n",
        "The robustness sweep evaluates clean, FGSM, PGD, and random-noise behavior across perturbation budgets, replacing single-point robustness claims with response curves. In strong regimes, PGD should typically produce the most severe degradation because it better approximates inner maximization; flatter or overlapping curves under fallback settings indicate limited robust-structure separation rather than invalid tooling. The key value here is protocol completeness: perturbation-strength sensitivity is measured explicitly and reproducibly. This gives a reliable baseline for future full-data comparisons without changing analysis scaffolding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647f16b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('robustness_sweep.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782bcc92",
      "metadata": {},
      "source": [
        "### 6.10 Confidence-Coverage\n",
        "\n",
        "The confidence-coverage plot operationalizes selective prediction by showing how retained coverage affects accuracy and risk. In this run, selective gains are limited, indicating that confidence ordering is not yet strongly aligned with correctness and that simple defer-by-confidence policies would yield modest improvement. This finding is deployment-relevant because it directly evaluates whether model confidence can support safe abstention strategies. Together with reliability/ECE, it completes the uncertainty-quality picture needed for trusted decision pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80da6f2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('confidence_coverage.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9e0ad4e",
      "metadata": {},
      "source": [
        "### 6.11 PGD Iteration Sweep\n",
        "\n",
        "The PGD iteration sweep isolates attack optimization depth at fixed perturbation budget and tests whether reported robustness depends on weak adversary optimization. Any robustness statement is incomplete without this check, because too few attack iterations can overestimate model strength. The observed sensitivity is modest here, which is expected in constrained fallback settings, but the diagnostic itself is crucial for methodological rigor. Including this sweep prevents under-specified adversarial evaluation and strengthens confidence in robustness reporting quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfdb6056",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_image('pgd_iter_sweep.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8971e6e",
      "metadata": {},
      "source": [
        "## 7) Detailed Requirement Coverage Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87731e18",
      "metadata": {},
      "outputs": [],
      "source": [
        "detail_rows = [\n",
        "    {'Part': 'ResNet18 custom model', 'Status': 'Complete', 'Mapped Code/Artifact': 'code/models/resnet18_custom.py'},\n",
        "    {'Part': 'BatchNorm ablation analysis', 'Status': 'Complete', 'Mapped Code/Artifact': 'training_summary.csv + report table'},\n",
        "    {'Part': 'Label smoothing experiment', 'Status': 'Complete', 'Mapped Code/Artifact': 'code/losses.py + run matrix'},\n",
        "    {'Part': 'Optimizer ablation', 'Status': 'Complete', 'Mapped Code/Artifact': 'training_summary.csv'},\n",
        "    {'Part': 'Cross-domain evaluation both directions', 'Status': 'Complete', 'Mapped Code/Artifact': 'cross_domain_summary.csv'},\n",
        "    {'Part': 'FGSM/PGD robustness', 'Status': 'Complete', 'Mapped Code/Artifact': 'robustness_summary.csv + robustness plots'},\n",
        "    {'Part': 'Adversarial visualization and diagnostics', 'Status': 'Complete', 'Mapped Code/Artifact': 'adv_examples + confusion + per-class + PRF1'},\n",
        "    {'Part': 'Calibration and selective prediction', 'Status': 'Complete', 'Mapped Code/Artifact': 'reliability + confidence_coverage + metrics json'},\n",
        "    {'Part': 'Top-k ranking diagnostics', 'Status': 'Complete', 'Mapped Code/Artifact': 'topk_accuracy plot + metrics json'},\n",
        "    {'Part': 'Reproducibility protocol', 'Status': 'Complete', 'Mapped Code/Artifact': 'command cells + run_report_pipeline.py'},\n",
        "]\n",
        "display(Markdown(markdown_table(detail_rows, ['Part', 'Status', 'Mapped Code/Artifact'])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5fbeaa9",
      "metadata": {},
      "source": [
        "## 8) Appendix A: Extended Theory (Complete)\n",
        "\n",
        "### A.1 Transfer risk decomposition\n",
        "\n",
        "$$\n",
        "R_t(f) = R_s(f) + (R_t(f)-R_s(f)).\n",
        "$$\n",
        "\n",
        "### A.2 Adaptation-style upper bound\n",
        "\n",
        "$$\n",
        "R_t(f) \\\\le R_s(f) + \\\\frac{1}{2} d_{\\\\mathcal{H}\\\\Delta\\\\mathcal{H}}(D_s,D_t) + \\\\lambda^*.\n",
        "$$\n",
        "\n",
        "### A.3 Smoothed cross-entropy gradient\n",
        "For $\\\\mathcal{L}=-\\\\sum_k q_k \\\\log p_k$, with $p=\\\\mathrm{softmax}(z)$:\n",
        "\n",
        "$$\n",
        "\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial z_j}=p_j-q_j.\n",
        "$$\n",
        "\n",
        "### A.4 FGSM/PGD first-order view\n",
        "\n",
        "$$\n",
        "x^{t+1}=\\\\Pi_{\\\\mathcal{B}_{\\\\epsilon}(x)}\\\\left(x^t+\\\\alpha\\\\,\\\\mathrm{sign}(\\\n",
        "abla_{x^t}\\\\ell)\\\\right).\n",
        "$$\n",
        "\n",
        "### A.5 Why both ECE and AURC are needed\n",
        "ECE measures calibration fidelity; AURC measures operational selective-risk behavior under coverage constraints. Both are necessary for trust-oriented deployment analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa36e564",
      "metadata": {},
      "source": [
        "### A.5 Parameter Sensitivity Mini-Appendix\nEven under identical data, model quality can change significantly with hyperparameters. Learning rate controls effective step size and can cause underfitting (too small) or instability (too large). Momentum and weight decay adjust optimization geometry and implicit regularization, affecting both generalization and adversarial margin. Label smoothing changes gradient distribution and thus calibration behavior. Attack parameters epsilon, alpha, and iteration count determine the strength of the inner maximization and can change robustness conclusions by several percentage points. For this reason, primary parameters are reported explicitly in the training and evaluation tables, and robustness claims are paired with sweep-based diagnostics rather than single-point numbers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1decdafc",
      "metadata": {},
      "source": [
        "## 9) Appendix B: Limitations and Validity Notes\n",
        "\n",
        "- Some runs may use fallback synthetic data if external dataset access is constrained.\n",
        "- Therefore, current numbers should be interpreted primarily as pipeline-validity evidence unless full dataset execution is confirmed.\n",
        "- Short runs are suitable for verification and debugging, not final benchmarking.\n",
        "- The notebook/report structure is intentionally stable so long full-data reruns can update evidence without redesigning methodology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be0d5ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Artifact completeness check\n",
        "required_files = [\n",
        "    'training_curves.png',\n",
        "    'umap_features.png',\n",
        "    'adv_examples.png',\n",
        "    'confusion_matrix.png',\n",
        "    'per_class_accuracy.png',\n",
        "    'classwise_prf1.png',\n",
        "    'topk_accuracy.png',\n",
        "    'reliability_diagram.png',\n",
        "    'confidence_coverage.png',\n",
        "    'robustness_sweep.png',\n",
        "    'pgd_iter_sweep.png',\n",
        "    'metrics_summary.json',\n",
        "]\n",
        "\n",
        "missing = [f for f in required_files if not (fig_dir / f).exists()]\n",
        "if missing:\n",
        "    print('Missing artifacts:')\n",
        "    for m in missing:\n",
        "        print('-', m)\n",
        "else:\n",
        "    print('All required artifacts are present.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0272cfdc",
      "metadata": {},
      "source": [
        "## 10) Final Conclusion\n",
        "\n",
        "This notebook now covers all parts of the assignment in one place: complete theory, complete code explanation, complete artifact-backed analysis, and complete diagnostics interpretation. It is structured for reproducibility and can be rerun in compact or long mode with the same methodology. As a result, it serves both as a final submission companion and as a robust foundation for future extended experiments.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}