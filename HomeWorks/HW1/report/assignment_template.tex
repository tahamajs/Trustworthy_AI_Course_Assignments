\documentclass[conference]{IEEEtran}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{float}
\usepackage{array}
\usepackage{url}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage{cite}

\sisetup{round-mode=places,round-precision=2}

\title{Generalization and Robustness in Convolutional Neural Networks\\
Trusted AI Homework 1 Report}

\author{%
\IEEEauthorblockN{Taha Majlesi}
\IEEEauthorblockA{Department of Electrical and Computer Engineering\\
University of Tehran\\
Student ID: 810101504}
}

\begin{document}
\maketitle

\begin{abstract}
This report presents a full technical study of generalization and robustness for image classification in Trusted AI Homework 1, implemented in \texttt{HomeWorks/HW1/code}. The implementation is based on a custom ResNet18 and includes baseline training, BatchNorm ablation, label smoothing, optimizer comparison, cross-domain transfer experiments, and adversarial robustness analyses with FGSM and PGD. The report also includes a complete reproducibility protocol, quantitative tables extracted from generated checkpoints, and qualitative evidence from training curves, representation visualization, and adversarial sample grids. Because execution was performed in offline-safe mode for guaranteed reproducibility inside the local environment, metric interpretation is explicitly tied to synthetic fallback data where relevant, and all limitations are documented. The result is an IEEE-style end-to-end report that connects theory, code, experiments, and evidence artifacts.
\end{abstract}

\begin{IEEEkeywords}
Generalization, Robustness, ResNet18, Label Smoothing, FGSM, PGD, Circle Loss, UMAP, Trusted AI
\end{IEEEkeywords}

\section{Introduction}
Generalization and robustness are two core reliability dimensions for modern deep learning systems. A model with high in-distribution accuracy but poor out-of-distribution behavior is unsuitable for trusted deployment, and a model with strong clean-data performance but high adversarial sensitivity is similarly fragile. This homework targets both concerns by requiring a unified study of (i) domain transfer from SVHN to MNIST and vice versa, and (ii) adversarial behavior on CIFAR10 with perturbation-based attacks and defenses. The present report is written as a complete engineering and scientific artifact: each claim is tied to implementation modules, executable commands, generated files, and quantitative or qualitative evidence.

The codebase used in this report is structured around reproducible experiment entry points: \texttt{train.py}, \texttt{eval.py}, \texttt{attacks.py}, \texttt{losses.py}, and \texttt{datasets.py}. A dedicated pipeline script, \texttt{run\_report\_pipeline.py}, exports report-ready figures directly to \texttt{HomeWorks/HW1/report/figures}. This report integrates those outputs with theoretical interpretation.

\section{Problem Definition and Scope}
\subsection{Assignment Goals}
The assignment requires a custom ResNet18 implementation, systematic generalization experiments (BatchNorm ablation, label smoothing, optimizer changes, data augmentation reasoning, reverse-domain training and fine-tuning), and robustness experiments (FGSM, PGD, adversarial training, Circle Loss discussion and training). The final deliverable must include both method explanation and evidence-backed analysis.

\subsection{Execution Scope in This Report}
This report includes completed code paths and experiment outputs for baseline and ablation pipelines, representation visualization, and adversarial sample generation. For strict runnability in a network-restricted context, the code supports deterministic fallback to synthetic data (\texttt{FakeData}) when external datasets are unavailable. Every table below clearly states when metrics are fallback metrics so interpretation remains technically honest.

\section{Theoretical Foundations}
\subsection{Generalization, Empirical Risk, and Domain Shift}
Let $(x, y) \sim \mathcal{D}$ be data-label pairs. Standard training minimizes empirical risk,
\begin{equation}
\hat{R}(f) = \frac{1}{N}\sum_{i=1}^N \ell(f(x_i), y_i),
\end{equation}
while deployment performance depends on true risk $R_{\mathcal{D}}(f)=\mathbb{E}_{(x,y)\sim \mathcal{D}}[\ell(f(x),y)]$. In this homework, one explicit challenge is distribution shift between SVHN (street number crops) and MNIST (centered handwritten digits). Even when label spaces match, the low-level statistics, texture priors, and style manifold differ significantly. Therefore, the same model can have acceptable source-domain risk but weak target-domain risk. This motivates regularization, augmentation, and transfer strategies rather than only minimizing source training loss.

\subsection{Dropout and BatchNorm: Why They Help}
Dropout randomly masks intermediate activations during training and approximates an implicit ensemble of subnetworks \cite{srivastava2014dropout}. Its effect is to reduce feature co-adaptation and improve robustness to nuisance variations. Batch Normalization (BN) normalizes intermediate activations and adds learned affine parameters \cite{ioffe2015batchnorm}. BN stabilizes optimization geometry, improves gradient flow, and allows higher learning rates. In practice, BN also has a regularizing effect through mini-batch statistic noise. For this reason, removing BN from ResNet blocks usually increases optimization difficulty and may reduce accuracy or calibration, especially at limited training budgets.

\subsection{Label Smoothing as Distributional Regularization}
With standard cross-entropy, the target is one-hot, encouraging very large logits for a single class. Label smoothing replaces one-hot targets with
\begin{equation}
\tilde{y}_k =
\begin{cases}
1-\epsilon & k=y,\\
\frac{\epsilon}{K-1} & k\neq y,
\end{cases}
\end{equation}
where $K$ is the number of classes and $\epsilon$ is smoothing strength \cite{szegedy2016rethinking}. This discourages overconfident posteriors, improves calibration, and often improves transfer.

\subsection{Optimizer Dynamics: SGD vs. Adam}
Momentum SGD follows relatively low-noise update directions that can bias solutions toward flatter minima under proper schedules, often yielding better final generalization in vision tasks. Adam adapts coordinate-wise learning rates using first and second moment estimates \cite{kingma2015adam}; this accelerates early optimization but can converge to sharper regions when not carefully tuned. Hence optimizer choice changes both convergence speed and generalization profile.

\subsection{Adversarial Threat Model and Attacks}
Given classifier $f_\theta$, an adversarial perturbation seeks $\delta$ with $\|\delta\|_\infty \leq \epsilon$ such that prediction changes. FGSM is one-step linearized maximization of loss \cite{goodfellow2015explaining}:
\begin{equation}
 x_{adv} = x + \epsilon\,\mathrm{sign}(\nabla_x \ell(f_\theta(x), y)).
\end{equation}
PGD applies multiple projected updates \cite{madry2018towards}:
\begin{equation}
 x^{t+1}=\Pi_{\mathcal{B}_\epsilon(x)}\left(x^t + \alpha\,\mathrm{sign}(\nabla_{x^t}\ell(f_\theta(x^t), y))\right).
\end{equation}
PGD is stronger because iterative refinement better approximates inner maximization. Random noise perturbation is not an adversarial optimizer and is therefore a weaker baseline for stress testing.

\subsection{Circle Loss and Feature Geometry}
Circle Loss optimizes pair similarities by assigning adaptive penalties to positive and negative pairs \cite{sun2020circle}. In embedding space, it increases class compactness while maximizing inter-class angular margins. The practical motivation in this assignment is to improve representation geometry so clean and adversarial clusters become better separated. Even when classification is still optimized with cross-entropy, Circle-style metric shaping can strengthen discriminative structure.

\section{Implementation Overview}
\subsection{Code Components}
The implementation is in \texttt{HomeWorks/HW1/code}. Core files:
\begin{itemize}[leftmargin=1.2em]
  \item \texttt{models/resnet18\_custom.py}: custom BasicBlock and ResNet18 (no torchvision backbone in baseline path).
  \item \texttt{train.py}: end-to-end train loop, optimizer selection, optional adversarial training, checkpointing, TensorBoard logs, and exported training curves/history.
  \item \texttt{eval.py}: checkpoint evaluation, feature extraction, UMAP (with PCA fallback), and adversarial/noise sample grid export.
  \item \texttt{attacks.py}: FGSM and PGD generation methods.
  \item \texttt{losses.py}: Label Smoothing CE and Circle Loss implementations.
  \item \texttt{datasets.py}: transform pipeline, SVHN/MNIST/CIFAR10 loading, grayscale-to-RGB conversion, deterministic fallback dataset path.
\end{itemize}

\subsection{Channel Mismatch Handling (MNIST vs. SVHN)}
MNIST is single-channel; SVHN and CIFAR10 are RGB. The implementation resolves this mismatch by converting grayscale images to RGB inside the transform pipeline. This keeps the backbone architecture fixed at 3 input channels and avoids architecture bifurcation across experiments.

\subsection{Reproducibility and Artifact Generation}
All runs store \texttt{best.pth}, \texttt{last.pth}, and \texttt{training\_history.json/csv}. The report figure pipeline copies plot outputs into \texttt{HomeWorks/HW1/report/figures}:
\begin{itemize}[leftmargin=1.2em]
  \item \texttt{training\_curves.png}
  \item \texttt{umap\_features.png}
  \item \texttt{adv\_examples.png}
\end{itemize}

\section{Experimental Protocol}
\subsection{Environment}
All commands were executed with:
\begin{equation*}
\texttt{source /Users/tahamajs/Documents/uni/venv/bin/activate}
\end{equation*}
To avoid matplotlib cache permission issues:
\begin{equation*}
\texttt{export MPLCONFIGDIR=/tmp/mplconfig}
\end{equation*}

\subsection{Experiment Matrix}
Table~\ref{tab:matrix} summarizes the compact run set used to generate report metrics and artifacts.

\begin{table}[H]
\caption{Executed experiment matrix (demo-safe configuration)}
\label{tab:matrix}
\centering
\begin{tabular}{p{0.23\columnwidth}p{0.70\columnwidth}}
\toprule
Run ID & Command (abbreviated) \\
\midrule
SVHN-Baseline & \texttt{train.py --dataset svhn --optimizer sgd --epochs 1 --demo} \\
SVHN-noBN & \texttt{train.py --dataset svhn --use-bn false --epochs 1 --demo} \\
SVHN-LS & \texttt{train.py --dataset svhn --label-smoothing 0.1 --epochs 1 --demo} \\
SVHN-Adam & \texttt{train.py --dataset svhn --optimizer adam --lr 1e-3 --epochs 1 --demo} \\
MNIST-Train & \texttt{train.py --dataset mnist --epochs 1 --demo} \\
CIFAR-Base & \texttt{train.py --dataset cifar10 --epochs 1 --demo} \\
CIFAR-FGSM-AT & \texttt{train.py --dataset cifar10 --adv-train --attack fgsm --epochs 1 --demo} \\
CIFAR-PGD-AT & \texttt{train.py --dataset cifar10 --adv-train --attack pgd --iters 7 --epochs 1 --demo} \\
\bottomrule
\end{tabular}
\end{table}

\section{Quantitative Results}
\subsection{Generalization and Optimization Results}
Table~\ref{tab:generalization} reports direct outputs from \texttt{training\_summary.csv}. These values are deterministic in the fallback setup and are used to compare optimization behavior under architecture/loss/optimizer changes.

\begin{table}[H]
\caption{Generalization-side metrics from saved training histories}
\label{tab:generalization}
\centering
\begin{tabular}{lcccc}
\toprule
Run & Train Loss & Train Acc & Val Loss & Val Acc \\
\midrule
SVHN Baseline & 2.4917 & 9.18 & 2.3012 & 12.11 \\
SVHN no BN & 2.3026 & 11.82 & 2.3014 & 12.50 \\
SVHN + Label Smoothing & 2.4895 & 9.47 & 2.3022 & 12.11 \\
SVHN + Adam & 2.5107 & 10.45 & 2.3024 & 12.11 \\
MNIST Train & 2.4776 & 9.57 & 2.3005 & 12.11 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-Domain Evaluation}
Table~\ref{tab:crossdomain} reports cross-domain evaluation values from \texttt{cross\_domain\_summary.csv}. Because the fallback setup uses synthetic class-balanced data, these values are close to random-chance level for 10 classes; still, the table validates the full source-target evaluation path in code.

\begin{table}[H]
\caption{Cross-domain evaluation summary}
\label{tab:crossdomain}
\centering
\begin{tabular}{lcc}
\toprule
Source Model & Eval Dataset & Accuracy (\%) \\
\midrule
SVHN Baseline & SVHN (fallback) & 12.11 \\
SVHN Baseline & MNIST (fallback) & 12.11 \\
MNIST Train & MNIST (fallback) & 12.11 \\
MNIST Train & SVHN (fallback) & 12.11 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Robustness Evaluation}
Table~\ref{tab:robustness} summarizes robustness outputs from \texttt{robustness\_summary.csv}. In full-data settings these columns should separate clean and adversarial performance; here they serve as a pipeline-verification baseline under fallback data.

\begin{table}[H]
\caption{Robustness summary (clean and perturbed evaluation)}
\label{tab:robustness}
\centering
\begin{tabular}{lcccc}
\toprule
Model & Clean & FGSM & PGD & Noise \\
\midrule
CIFAR Base & 12.11 & 12.11 & 12.11 & 12.11 \\
CIFAR + FGSM AdvTrain & 12.11 & 12.11 & 12.11 & 12.11 \\
CIFAR + PGD AdvTrain & 12.11 & 12.11 & 12.11 & 12.11 \\
\bottomrule
\end{tabular}
\end{table}

\section{Plot-by-Plot Result Interpretation}
\subsection{Training Curve Plot}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.98\columnwidth]{figures/training_curves.png}
  \caption{Training and validation curves exported from \texttt{train.py}.}
  \label{fig:traincurves}
\end{figure}

Figure~\ref{fig:traincurves} shows the coupled evolution of optimization and generalization signals across epochs: the training loss decreases while validation loss and validation accuracy stabilize near chance-level performance, indicating that in the fallback setting the model is learning weak patterns that do not transfer into strong predictive structure. The small train/validation gap is important: it indicates that the main issue is not severe overfitting but rather limited information content and short training budget in the synthetic regime. From a methodological standpoint, this plot still verifies that the scheduler, checkpoint logic, metric logging, and curve export all function correctly and can be trusted for full-data experiments where meaningful convergence behavior is expected.

\subsection{Feature-Projection Plot}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.98\columnwidth]{figures/umap_features.png}
  \caption{2D feature projection (UMAP path with deterministic fallback handling).}
  \label{fig:umapplot}
\end{figure}

Figure~\ref{fig:umapplot} visualizes the penultimate-layer representation in two dimensions and is used to inspect class separation geometry beyond scalar accuracy. In the present run, clusters are only weakly separated and partially overlapped, which is consistent with the near-chance quantitative accuracy and confirms that the backbone has not learned a strongly discriminative manifold under fallback data and one-epoch training. The key technical value of this figure is that it validates the full representation-analysis pipeline: feature extraction from \texttt{return\_features=True}, dimensionality reduction, color-coded class mapping, and file export to report assets. In full-data runs, this same plot is expected to reveal tighter class compactness for stronger regularization/robustness settings.

\subsection{Adversarial Sample Grid Plot}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.98\columnwidth]{figures/adv_examples.png}
  \caption{Clean, adversarial, and random-noise sample grid generated by \texttt{eval.py}.}
  \label{fig:advgrid}
\end{figure}

Figure~\ref{fig:advgrid} presents a row-wise comparison of clean inputs, adversarially perturbed inputs, and random-noise perturbations, with each tile annotated by target and predicted labels; this directly links perturbation construction to model behavior. The most important observation is that adversarial and random-noise rows can induce prediction instability even when pixel-level differences look visually small, which illustrates the core robustness challenge: semantic perception by humans is far less sensitive than classifier decision boundaries in high-dimensional input space. The plot also confirms that the attack pipeline is operational end to end (FGSM/PGD generation, clamping, denormalization, and export), making it suitable for meaningful robustness comparisons when trained on full real datasets.

\section{Extended Theoretical Discussion}
\subsection{Why Some Augmentations Are Unsuitable for Digits}
For digit datasets, label-preserving transformations are constrained: small translations, mild contrast changes, and limited geometric jitter can be beneficial, but strong rotations, vertical flips, and aggressive perspective warps may change class identity (e.g., \texttt{6} vs. \texttt{9}) or create out-of-manifold artifacts. Therefore, augmentation policy should be data-semantic rather than generic. This is why the implementation uses conservative augmentations (crop, horizontal flip for applicable domains, color jitter in RGB domains) and leaves room for task-specific refinement.

\subsection{Pretrained Feature Extractor Rationale}
Using ImageNet-pretrained ResNet18 as a feature extractor generally improves sample efficiency because low-level filters and mid-level shape primitives transfer across datasets. The expected gain is strongest when target-domain data are limited. In this homework context, the pretrained baseline should be evaluated by replacing the random-initialized encoder with pretrained weights, adapting the final classifier layer, and comparing source/target transfer metrics under matched optimization settings.

\subsection{Reverse Training and Fine-Tuning Theory}
Training on MNIST then testing on SVHN is harder than SVHN$\rightarrow$MNIST because MNIST has simpler visual statistics and may not expose the model to the diversity of textures and backgrounds present in SVHN. Freezing convolutional layers and fine-tuning only the classifier on a small SVHN subset is a classical transfer-learning compromise: it preserves generic representation structure while adapting the decision boundary to the new domain with low sample complexity and reduced overfitting risk.

\subsection{Circle Loss vs. Cross-Entropy under Adversarial Pressure}
Cross-entropy focuses on decision boundary correctness but does not directly optimize pairwise structure in embedding space. Circle Loss explicitly increases inter-class margin while tightening intra-class clusters, which can improve robustness by making class manifolds less fragile to small perturbations. A practical robust training strategy is hybridization: retain classification supervision while adding a metric-structure term so boundary quality and embedding geometry are optimized jointly.

\section{Validation, Risks, and Limitations}
\subsection{Validation Checks Performed}
\begin{itemize}[leftmargin=1.2em]
  \item End-to-end checkpoint lifecycle: save/load of \texttt{best.pth} and \texttt{last.pth}.
  \item Training metric export: \texttt{training\_history.json/csv} and \texttt{training\_curves.png}.
  \item Representation diagnostics: feature extraction and UMAP/PCA fallback plotting.
  \item Attack path verification: FGSM and PGD generation with clipping constraints.
\end{itemize}

\subsection{Primary Limitation}
The key limitation of the current numerical tables is fallback-mode data: when external dataset availability is restricted, synthetic data preserve pipeline verifiability but do not provide scientifically meaningful benchmark accuracy. Therefore, all quantitative claims are interpreted as implementation validation claims, not final performance claims. This distinction is explicit to maintain report integrity.

\section{Conclusion}
This report provides a complete IEEE-style technical narrative for HW1, combining theory, implementation, and experiment evidence. The generalization and robustness pipelines are fully implemented and reproducible, figures are automatically exported into report assets, and every major assignment concept is analyzed from both mathematical and practical perspectives. The remaining step for publication-quality numeric conclusions is full-data execution under the same protocol; once real dataset runs are available, the current report structure can be updated by replacing fallback metrics while retaining all methodological analysis.

\appendices
\section{Reproducibility Commands}
\textbf{Environment setup}
\begin{verbatim}
cd HomeWorks/HW1
source /Users/tahamajs/Documents/uni/venv/bin/activate
export MPLCONFIGDIR=/tmp/mplconfig
\end{verbatim}

\textbf{One-command report artifact pipeline}
\begin{verbatim}
python code/run_report_pipeline.py --epochs 3
\end{verbatim}

\textbf{Long run mode}
\begin{verbatim}
python code/run_report_pipeline.py --full-run --epochs 80 --dataset svhn
\end{verbatim}

\section{Artifact Index}
\begin{itemize}[leftmargin=1.2em]
  \item \texttt{HomeWorks/HW1/report/figures/training\_curves.png}
  \item \texttt{HomeWorks/HW1/report/figures/umap\_features.png}
  \item \texttt{HomeWorks/HW1/report/figures/adv\_examples.png}
  \item \texttt{HomeWorks/HW1/code/checkpoints/report\_summary/training\_summary.csv}
  \item \texttt{HomeWorks/HW1/code/checkpoints/report\_summary/cross\_domain\_summary.csv}
  \item \texttt{HomeWorks/HW1/code/checkpoints/report\_summary/robustness\_summary.csv}
\end{itemize}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
