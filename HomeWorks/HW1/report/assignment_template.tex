% Beautiful assignment LaTeX template — polished layout for reports
% Compile: make pdf  (or pdflatex + bibtex + pdflatex x2)
\documentclass[11pt,a4paper]{article}

% --- Typography & layout ---------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathpazo}          % nicer serif font (Palatino)
\usepackage{microtype}
\usepackage[a4paper,margin=1in]{geometry}
\setlength{\headheight}{15pt} % avoids fancyhdr warning

% --- Useful packages -----------------------------------------------------
\usepackage{amsmath,amssymb,mathtools}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{float}
\usepackage{tikz}              % drawing placeholders / decorations
\usepackage{enumitem}         % compact lists
\usepackage{xcolor}
\definecolor{accent}{HTML}{2A9D8F}
\definecolor{heading}{HTML}{264653}

% --- Section heading style ------------------------------------------------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries\color{heading}}{\thesection}{1em}{}
\titleformat{\subsection}{\bfseries\color{heading}}{\thesubsection}{0.5em}{}
\titlespacing*{\section}{0pt}{12pt}{6pt}

% --- Code listing style ---------------------------------------------------
\usepackage{listings}
\lstdefinestyle{py}{
  language=Python,
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{gray!6},
  frame=single,
  framesep=4pt,
  rulecolor=\color{gray!40},
  keywordstyle=\color{blue!65!black},
  commentstyle=\color{gray!55!black}\itshape,
  stringstyle=\color{red!65!black},
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny\color{gray},
  breaklines=true,
  captionpos=b,
}
\lstset{style=py}

% --- Pretty abstract box --------------------------------------------------
\usepackage{tcolorbox}
\tcbset{colback=gray!7, colframe=accent, left=6pt, right=6pt, boxrule=0.8pt}

% --- Header / footer -----------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.6pt}
\renewcommand{\footrulewidth}{0.0pt}
\fancyhead[L]{\small\textbf{\course{}}}
\fancyhead[C]{\small Assignment \assignment{}}
\fancyhead[R]{\small \authorname{}}
\fancyfoot[C]{\thepage}

% --- Metadata (edit these) ------------------------------------------------
\newcommand{\course}{Trusted Artificial Intelligence}
\newcommand{\instructor}{Dr. Mostafa Tavasolipour}
\newcommand{\semester}{Spring 2024}
\newcommand{\assignment}{1}
\newcommand{\authorname}{Taha Majlesi}
\newcommand{\studentid}{810101504}
\newcommand{\affiliation}{Department of Electrical and Computer Engineering, University of Tehran} 

% --- Helpers --------------------------------------------------------------
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\codefile}[1]{\lstinputlisting[style=py]{#1}}
\usepackage[hidelinks]{hyperref}
\usepackage{natbib}
\bibliographystyle{plainnat}

% --- Document -------------------------------------------------------------
\begin{document}

% ---------- Title page ----------------------------------------------------
\begin{titlepage}
  \centering
  {\LARGE\bfseries \course{} \par}
  \vspace{1.2cm}
  {\Huge\bfseries Homework \assignment{}\par}
  \vspace{0.6cm}
  {\large \semester{}\par}
  \vspace{1.2cm}
  {\Large\bfseries \authorname{}\par}
  \vspace{0.2cm}
  {\small ID: \studentid{} \quad | \quad \affiliation{}\par}
  \vspace{1.5cm}
  \begin{tikzpicture}
    \draw[accent,line width=2pt] (0,0) -- (8,0);
  \end{tikzpicture}
  \vfill
  {\large Instructor: \instructor{}\par}
  {\small Submitted: \today\par}
\end{titlepage}

% ---------- Abstract ------------------------------------------------------
\begin{tcolorbox}
\textbf{Abstract.} This report documents experiments on generalization and robustness for image classifiers. We implement a custom ResNet18 and evaluate cross-dataset generalization by training on SVHN and testing on SVHN and MNIST. We study regularizers (label smoothing, dropout/BatchNorm ablation), data augmentations, optimizer effects, and transfer learning; for robustness we evaluate FGSM/PGD attacks and explore Circle Loss and adversarial training. Results and figures in this document are structured for direct replacement with your measured outputs (tables currently contain example values to be replaced by your runs). Key methods cited include FGSM, PGD and Circle Loss.\end{tcolorbox}

\vspace{6pt}
\tableofcontents
\clearpage

% ---------- Main sections -------------------------------------------------
\section{Introduction}
This report documents experiments for HW1 (Generalization & Robustness). We train a ResNet18-style classifier on the SVHN dataset and evaluate generalization to SVHN and MNIST. The goal is to (1) measure baseline cross-dataset generalization, (2) evaluate regularizers and augmentations, and (3) analyse robustness under simple adversarial attacks.

\section{Methods}
This section gives precise reproducible details so reviewers can reproduce results.

\subsection{Datasets and preprocessing}
- SVHN (train / test): color images, resize to 32×32 where needed.
- MNIST (test only for cross-domain evaluation): convert to 3-channels by repeating the single channel, then normalize using SVHN mean/std for fair evaluation.
- Standard preprocessing: resize/crop → to-tensor → normalize(mean, std).

\subsection{Model (ResNet18, custom)}
We use a ResNet18 implemented from scratch (no `torchvision.models` for the baseline). The final classifier outputs 10 logits. When using transfer learning, the convolutional backbone is initialized from ImageNet-pretrained weights and the final linear layer is replaced.

\subsection{Training setup}
All experiments use a fixed random seed and the following default hyperparameters unless stated otherwise.

\begin{table}[H]
  \centering
  \caption{Default training hyperparameters}
  \begin{tabular}{ll}
    \toprule
    Hyperparameter & Value \\
    \midrule
    Optimizer & SGD with momentum (0.9) \\
    Initial learning rate & 0.1 (step decay / cosine where noted) \\
    Batch size & 128 \\
    Weight decay & 5e-4 \\
    Epochs & 80 \\
    Label smoothing & 0.0 (change to 0.1 when used) \\
    Augmentations & random crop, horizontal flip, color jitter (see text) \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Losses and regularization}
We report results for: (a) standard Cross-Entropy, (b) Label-Smoothing Cross-Entropy (smoothing=0.1), and (c) Circle Loss (when used for robustness experiments). Implementation notes: label smoothing is applied on the logits level before cross-entropy. Circle Loss and its use for metric-style supervision are implemented following Sun et al. \citep{sun2020circle}.

\section{Experiments}
We run these experiments with fixed seeds and report mean ± std over 3 runs for final numbers.

\subsection{Experiment list}
1. Baseline: train on SVHN (no BN removal, standard augmentations) and evaluate on SVHN/MNIST.
2. No-BN ablation: remove BatchNorm layers and retrain.
3. Label smoothing: train with smoothing=0.1.
4. Augmentation search: evaluate combinations and select the best generalizing augmentation.
5. Transfer learning: use ImageNet-pretrained backbone and fine-tune classifier.
6. Reverse training & fine-tuning (MNIST→SVHN small subset).
7. Robustness experiments (FGSM / PGD) on separate CIFAR-10 setup (Part 2). Key adversarial methods used are FGSM and PGD \citep{goodfellow2015explaining, madry2018towards}.

\subsection{Evaluation protocol}
- Report top-1 accuracy on SVHN test and MNIST test.
- For robustness, report accuracy on adversarially perturbed test sets and visualize features with UMAP.
- Include train/val loss and accuracy curves for each run.

\section{Results}
Below are example result tables and recommended figure placeholders. Replace numbers with your experiment outputs.

\begin{table}[H]
  \centering
  \caption{Representative results (example values — replace with your measured outputs)}
  \begin{tabular}{lccc}
    \toprule
    Setting & SVHN test acc & MNIST test acc & Notes \\
    \midrule
    Baseline ResNet18 & 0.843 & 0.512 & trained from scratch on SVHN \\
    No BN & 0.792 & 0.335 & BN layers removed \\
    +Label Smoothing (0.1) & 0.852 & 0.548 & softer targets improved cross-domain \\
    +Augmentations (best) & 0.861 & 0.602 & augmentation selected by val MNIST proxy \\
    Pretrained backbone (ImageNet) & 0.889 & 0.721 & feature extractor improved transfer \\
    Adam optimizer & 0.855 & 0.530 & same lr schedule as SGD \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \IfFileExists{figures/umap_features.png}{\includegraphics[width=\textwidth]{figures/umap_features.png}}{%
      \fbox{\parbox[c][4.3cm][c]{\textwidth}{\centering\small\itshape \texttt{umap\_features.png} (missing)}} }
    \caption{UMAP of 512-d features (baseline)}
    \label{fig:umap}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \IfFileExists{figures/adv_examples.png}{\includegraphics[width=\textwidth]{figures/adv_examples.png}}{%
      \fbox{\parbox[c][4.3cm][c]{\textwidth}{\centering\small\itshape \texttt{adv\_examples.png} (missing)}} }
    \caption{Adversarial examples (FGSM)}
    \label{fig:adv}
  \end{subfigure}
  \caption{Feature visualizations and adversarial examples.}
\end{figure}

\section{Discussion}
- The pretrained backbone strongly improves cross-dataset transfer — suggests feature reuse is effective when domain gap is moderate.
- Label smoothing and targeted augmentations improved generalization to MNIST, but removing BN hurt performance consistently.
- Adversarial augmentation and Circle Loss (Part 2) yield improved robustness at some cost in clean accuracy.

\section{Conclusion}
Summarize the successful interventions (pretraining, careful augmentations, label smoothing) and list concrete next experiments (e.g., stronger adversarial training, more augmentation search budget).

% ---------- Appendix ------------------------------------------------------

% ---------- Appendix ------------------------------------------------------
\appendix
\section{Hyperparameters and Implementation Details}
Full hyperparameters and experimental settings.

\section{Selected Code}
Important snippets or reference to script files.
\begin{lstlisting}[style=py,caption={Training loop (example)}]
for epoch in range(epochs):
    model.train()
    for x,y in train_loader:
        # training step
        pass
\end{lstlisting}

\section{Additional Figures}
Extra visualizations, UMAP plots, adversarial examples, etc.

\clearpage
\bibliography{references}
\end{document}
