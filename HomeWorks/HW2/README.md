# HW2 — Interpretability (Tabular & Vision)

Complete implementations and notebooks for interpretability experiments on tabular and image models (LIME/SHAP, Grad-CAM variants, activation maximization).

---

## Overview
- Tabular: MLP and Neural Additive Model (NAM) on the Pima diabetes dataset.
- Vision: Grad-CAM, Guided Backprop, SmoothGrad, activation maximization on pretrained CNNs.
- Includes notebook `HW2_solution.ipynb` with step-by-step experiments and report figures.

---

## Folder layout
- `code/` — scripts and modules (`tabular.py`, `vision.py`, `interpretability.py`, `models.py`).
- `notebooks/` — executed homework notebook and visual analysis.
- `report/` — LaTeX report and `figures/` used by the assignment.
- `description/` — assignment prompt and requirements.

---

## Quick setup & run
```bash
cd HomeWorks/HW2/code
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
# Tabular baseline + plots
python tabular.py
python generate_report_plots.py
# Open the notebook for the full workflow
jupyter-notebook ../notebooks/HW2_solution.ipynb
```

---

## What to run for common tasks
- Tabular training & evaluation (script `tabular.py`) — loads `diabetes.csv` if present or downloads/falls back to a synthetic dataset.
- Interpretability (script `interpretability.py`) — functions for LIME and SHAP explanations on individual samples.
- Vision explanations (`vision.py`) — Grad-CAM, Guided Backprop, SmoothGrad. Notebook contains visualization examples.

---

## Reproducibility & notes
- The tabular loader saves a local copy `diabetes.csv` after download.
- If you have no internet, `tabular.py` uses a deterministic synthetic fallback so results are reproducible.
- Vision utilities will fall back to non‑pretrained weights if network access is unavailable (notebooks indicate that behavior).

---

## Expected outputs
- Tabular: printed test metrics and `report/figures/` plots generated by `generate_report_plots.py`.
- Vision: saliency maps, Grad-CAM overlays, and activation-maximization images exported by the notebook.

---

## Further work (ideas)
- Compare LIME vs SHAP explanations for model bias on subpopulations.
- Use Guided Grad-CAM for class-specific visualization and compare to activation maximization.

If you want, I can expand `code/README.md` with a full CLI reference and example notebooks for each interpretability method.
