{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de86249",
   "metadata": {},
   "source": [
    "# HW2 Interpretability Notebook (Complete)\n",
    "\n",
    "This notebook is a complete, runnable companion to the IEEE report. It covers all required parts of Homework 2:\n",
    "\n",
    "1. Tabular EDA, preprocessing, MLP training, deterministic evaluation.\n",
    "2. LIME and SHAP local explanations for three samples, SHAP force plots, and cross-method comparison.\n",
    "3. NAM modeling and feature-function interpretability.\n",
    "4. Bonus GRACE-style contrastive perturbation analysis.\n",
    "5. Vision interpretability with VGG16: Grad-CAM, Guided Backprop, Guided Grad-CAM, SmoothGrad, adversarial FGSM, and feature visualization with TV + random shifts.\n",
    "\n",
    "All generated artifacts are saved to `report/figures/` and summarized in `report/figures/metrics_summary.json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266b190",
   "metadata": {},
   "source": [
    "## Reproducibility Notes\n",
    "\n",
    "- Use the same Python environment as the report build.\n",
    "- This notebook reuses the project modules in `code/`.\n",
    "- Seed control is enforced to keep outputs stable.\n",
    "\n",
    "Activate your project virtual environment before running (e.g. `source .venv/bin/activate` from `HomeWorks/HW2/code`, or your preferred venv path).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "# If run from repo root, point to HomeWorks/HW2\n",
    "CODE_DIR = ROOT / \"code\"\n",
    "if not (CODE_DIR / \"generate_report_plots.py\").exists():\n",
    "    hw2 = ROOT / \"HomeWorks\" / \"HW2\"\n",
    "    if (hw2 / \"code\" / \"generate_report_plots.py\").exists():\n",
    "        ROOT = hw2\n",
    "        CODE_DIR = ROOT / \"code\"\n",
    "\n",
    "REPORT_DIR = ROOT / \"report\"\n",
    "FIG_DIR = REPORT_DIR / \"figures\"\n",
    "METRICS_JSON = FIG_DIR / \"metrics_summary.json\"\n",
    "\n",
    "if str(CODE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(CODE_DIR))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Root:\", ROOT)\n",
    "print(\"Code dir:\", CODE_DIR)\n",
    "print(\"Figures dir:\", FIG_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfd43d",
   "metadata": {},
   "source": [
    "## Theoretical Foundation (Tabular)\n",
    "\n",
    "For binary classification with logit score $f_\\theta(x)$, probability is\n",
    "\n",
    "$$\n",
    "P(Y=1\\mid x)=\\sigma(f_\\theta(x)),\\quad \\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "Training minimizes BCE risk\n",
    "\n",
    "$$\n",
    "\\hat\\theta=\\arg\\min_\\theta\\frac{1}{N}\\sum_{i=1}^N\\ell\\big(y_i,f_\\theta(x_i)\\big).\n",
    "$$\n",
    "\n",
    "LIME fits a local weighted surrogate around a point $x$:\n",
    "\n",
    "$$\n",
    "\\xi(x)=\\arg\\min_{g\\in\\mathcal{G}}\\mathcal{L}(f,g,\\pi_x)+\\Omega(g).\n",
    "$$\n",
    "\n",
    "SHAP allocates additive contributions:\n",
    "\n",
    "$$\n",
    "f(x)\\approx \\phi_0+\\sum_j\\phi_j,\n",
    "$$\n",
    "\n",
    "with Shapley axioms (efficiency, symmetry, dummy, additivity), giving a principled attribution decomposition.\n",
    "\n",
    "NAM enforces structural interpretability through additive subnetworks:\n",
    "\n",
    "$$\n",
    "f(x)=\\sum_{j=1}^d g_j(x_j),\\quad \\hat y=\\sigma(f(x)).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c680cd",
   "metadata": {},
   "source": [
    "## Generate All Artifacts (Tabular + Vision)\n",
    "\n",
    "This single run produces all report figures and updates `metrics_summary.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_report_plots import (\n",
    "    METRICS_JSON as PIPELINE_METRICS_JSON,\n",
    "    _ensure_dirs,\n",
    "    _set_seed,\n",
    "    generate_tabular_figures,\n",
    "    generate_vision_figures,\n",
    ")\n",
    "\n",
    "_set_seed(SEED)\n",
    "_ensure_dirs()\n",
    "\n",
    "tabular_summary = generate_tabular_figures()\n",
    "vision_summary = generate_vision_figures()\n",
    "combined_summary = {\"tabular\": tabular_summary, \"vision\": vision_summary}\n",
    "PIPELINE_METRICS_JSON.write_text(json.dumps(combined_summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\", PIPELINE_METRICS_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae25ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "summary = json.loads(METRICS_JSON.read_text(encoding=\"utf-8\"))\n",
    "print(\"Top-level keys:\", list(summary.keys()))\n",
    "print(\"\\nTabular metrics:\")\n",
    "pprint(summary[\"tabular\"][\"mlp\"])\n",
    "pprint(summary[\"tabular\"][\"nam\"])\n",
    "print(\"\\nVision keys:\", list(summary[\"vision\"].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c836400",
   "metadata": {},
   "source": [
    "## Tabular EDA Outputs\n",
    "\n",
    "The following figures cover the assignment EDA requirements: class balance, correlation matrix, pairplot, and outlier dispersion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as DisplayImage, display\n",
    "\n",
    "def show_fig(name, width=900):\n",
    "    path = FIG_DIR / name\n",
    "    print(path)\n",
    "    display(DisplayImage(filename=str(path), width=width))\n",
    "\n",
    "show_fig(\"class_distribution.png\", width=760)\n",
    "show_fig(\"eda_correlation_matrix.png\", width=760)\n",
    "show_fig(\"eda_pairplot.png\", width=900)\n",
    "show_fig(\"eda_outlier_boxplots.png\", width=900)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a135c",
   "metadata": {},
   "source": [
    "### EDA Interpretation\n",
    "\n",
    "The class distribution shows moderate imbalance (about 34.8% positives), so threshold-aware metrics such as recall and F1 are mandatory next to accuracy. The correlation matrix and pairplot indicate no single dominant collinearity pattern in this dataset, which means model decisions are not trivially reducible to one redundant feature pair; instead, multiple weak-to-moderate signals combine nonlinearly. Outlier plots show limited but nonzero tails (especially for insulin-like dimensions), motivating robust preprocessing and caution when interpreting local linear surrogate coefficients near atypical points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d77a45",
   "metadata": {},
   "source": [
    "## Core Predictive Diagnostics\n",
    "\n",
    "This block covers optimization behavior and thresholded/threshold-free evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71038d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fig(\"training_loss_curves.png\", width=900)\n",
    "show_fig(\"confusion_matrix_comparison.png\", width=760)\n",
    "show_fig(\"roc_pr_comparison.png\", width=900)\n",
    "show_fig(\"calibration_comparison.png\", width=760)\n",
    "show_fig(\"threshold_sensitivity.png\", width=900)\n",
    "show_fig(\"permutation_importance_comparison.png\", width=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9ba69",
   "metadata": {},
   "source": [
    "### Diagnostic Interpretation\n",
    "\n",
    "Training curves show stable convergence with MLP reaching a lower validation-loss basin than NAM, consistent with its stronger test metrics. Confusion matrices reveal similar true-negative behavior but slightly better minority-class recovery for MLP, which drives higher recall/F1. ROC/PR dominance for MLP confirms better ranking quality, while reliability curves and Brier scores indicate slightly better probability calibration. Threshold sensitivity shows both models improve substantially around $t\\approx 0.2$, proving decision-threshold choice is part of the operational model. Permutation importance shows glucose as dominant global signal, with smaller marginal reliance for other features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb400421",
   "metadata": {},
   "source": [
    "## Local Explainability: LIME, SHAP, Force Plots, and Correlation Linkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fig(\"lime_shap_agreement.png\", width=760)\n",
    "show_fig(\"lime_shap_compare_sample_0.png\", width=900)\n",
    "show_fig(\"lime_shap_compare_sample_1.png\", width=900)\n",
    "show_fig(\"lime_shap_compare_sample_2.png\", width=900)\n",
    "show_fig(\"shap_force_sample_0.png\", width=900)\n",
    "show_fig(\"shap_force_sample_1.png\", width=900)\n",
    "show_fig(\"shap_force_sample_2.png\", width=900)\n",
    "show_fig(\"correlation_vs_shap_importance.png\", width=760)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64baf18",
   "metadata": {},
   "source": [
    "### Local Explanation Interpretation\n",
    "\n",
    "Agreement diagnostics show strong rank-level consistency between LIME and SHAP across the three analyzed samples, while per-sample bar charts show expected differences in exact magnitudes due to different estimators (local weighted surrogate fit versus Shapley-based additive credit). Force plots validate additive contribution flow from baseline to final prediction for each sample and provide intuitive sign-consistent evidence about why borderline points remain near decision boundaries. The correlation-versus-SHAP comparison confirms that raw correlation with target and learned attribution are related but not equivalent: SHAP reflects model-conditional contribution, not just pairwise data statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b6573",
   "metadata": {},
   "source": [
    "## NAM and Bonus GRACE-Style Contrastive Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4828b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fig(\"nam_feature_functions.png\", width=900)\n",
    "show_fig(\"grace_counterfactual_shap_shift.png\", width=760)\n",
    "\n",
    "grace = summary[\"tabular\"].get(\"grace_counterfactual\", {})\n",
    "print(\"GRACE summary:\")\n",
    "print(grace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da8652",
   "metadata": {},
   "source": [
    "### NAM + GRACE Interpretation\n",
    "\n",
    "NAM feature-function plots provide intrinsic interpretability because each subplot is a direct estimate of one additive component $g_j(x_j)$, making local slopes and regime changes directly inspectable without post-hoc approximation. The GRACE-style contrastive plot shows that controlled perturbation of a selected feature produces coherent movement in both predicted probability and SHAP attribution mass, which is exactly the expected behavior for contrastive reasoning: meaningful feature edits should create aligned changes in outputs and explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9539dc",
   "metadata": {},
   "source": [
    "## Vision Theory Summary\n",
    "\n",
    "Grad-CAM for class $c$ uses\n",
    "\n",
    "$$\n",
    "\\alpha_k^c = \\frac{1}{Z}\\sum_i\\sum_j\\frac{\\partial y^c}{\\partial A_{ij}^k},\\qquad\n",
    "L_{\\text{Grad-CAM}}^c=\\mathrm{ReLU}\\left(\\sum_k\\alpha_k^cA^k\\right).\n",
    "$$\n",
    "\n",
    "Guided Backprop keeps positive gradient flow through ReLU gates. Guided Grad-CAM fuses localization and fine details. SmoothGrad estimates\n",
    "\n",
    "$$\n",
    "\\hat S(x)=\\frac{1}{K}\\sum_{k=1}^K \\nabla_x y^c(x+\\epsilon_k),\\quad \\epsilon_k\\sim\\mathcal N(0,\\sigma^2I),\n",
    "$$\n",
    "\n",
    "which reduces high-frequency gradient variance.\n",
    "\n",
    "FGSM adversarial perturbation:\n",
    "\n",
    "$$\n",
    "x_{adv}=x+\\epsilon\\,\\mathrm{sign}(\\nabla_x \\mathcal{L}(f(x),y)).\n",
    "$$\n",
    "\n",
    "Activation maximization for class visualization:\n",
    "\n",
    "$$\n",
    "x^*=\\arg\\max_x y^c(x)-\\lambda_{TV}TV(x),\n",
    "$$\n",
    "\n",
    "and random shifts improve translation-consistent structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57efb29",
   "metadata": {},
   "source": [
    "## Vision Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fig(\"vgg16_six_image_predictions.png\", width=900)\n",
    "show_fig(\"gradcam_demo.png\", width=600)\n",
    "show_fig(\"gradcam_overlay_demo.png\", width=900)\n",
    "show_fig(\"guided_gradcam_example.png\", width=900)\n",
    "show_fig(\"smoothgrad_guided_comparison.png\", width=900)\n",
    "show_fig(\"smoothgrad_guided_backprop.png\", width=900)\n",
    "show_fig(\"smoothgrad_guided_gradcam.png\", width=900)\n",
    "show_fig(\"smoothgrad_sample_sweep.png\", width=900)\n",
    "show_fig(\"smoothgrad_convergence_metrics.png\", width=900)\n",
    "show_fig(\"adversarial_fgsm_comparison.png\", width=980)\n",
    "show_fig(\"feature_visualization_hen.png\", width=900)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac7baf",
   "metadata": {},
   "source": [
    "### Vision Interpretation\n",
    "\n",
    "The six-image panel provides class-diverse probes for explainability stress tests. Grad-CAM and overlay plots confirm coherent class-localized activation concentration. Guided Grad-CAM sharpens details while preserving localization, and SmoothGrad demonstrates the expected variance-reduction behavior as $K$ grows (supported by cosine, entropy, and total variation trends in the summary metrics). FGSM comparison shows prediction and saliency can shift under bounded adversarial perturbation, highlighting robustness concerns in explanation reliability. Feature-visualization results show that TV regularization plus random shifts transform noisy maximization artifacts into more stable, structured class-relevant patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54cdc8",
   "metadata": {},
   "source": [
    "## Requirement Coverage Check (Programmatic)\n",
    "\n",
    "This quick check verifies that all expected report artifact files exist after notebook execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ac84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_figures = [\n",
    "    \"class_distribution.png\",\n",
    "    \"eda_correlation_matrix.png\",\n",
    "    \"eda_pairplot.png\",\n",
    "    \"eda_outlier_boxplots.png\",\n",
    "    \"training_loss_curves.png\",\n",
    "    \"confusion_matrix_comparison.png\",\n",
    "    \"roc_pr_comparison.png\",\n",
    "    \"calibration_comparison.png\",\n",
    "    \"threshold_sensitivity.png\",\n",
    "    \"permutation_importance_comparison.png\",\n",
    "    \"lime_shap_agreement.png\",\n",
    "    \"lime_shap_compare_sample_0.png\",\n",
    "    \"lime_shap_compare_sample_1.png\",\n",
    "    \"lime_shap_compare_sample_2.png\",\n",
    "    \"shap_force_sample_0.png\",\n",
    "    \"shap_force_sample_1.png\",\n",
    "    \"shap_force_sample_2.png\",\n",
    "    \"correlation_vs_shap_importance.png\",\n",
    "    \"grace_counterfactual_shap_shift.png\",\n",
    "    \"nam_feature_functions.png\",\n",
    "    \"vgg16_six_image_predictions.png\",\n",
    "    \"gradcam_demo.png\",\n",
    "    \"gradcam_overlay_demo.png\",\n",
    "    \"guided_gradcam_example.png\",\n",
    "    \"smoothgrad_guided_comparison.png\",\n",
    "    \"smoothgrad_guided_backprop.png\",\n",
    "    \"smoothgrad_guided_gradcam.png\",\n",
    "    \"smoothgrad_sample_sweep.png\",\n",
    "    \"smoothgrad_convergence_metrics.png\",\n",
    "    \"adversarial_fgsm_comparison.png\",\n",
    "    \"feature_visualization_hen.png\",\n",
    "]\n",
    "\n",
    "missing = [f for f in required_figures if not (FIG_DIR / f).exists()]\n",
    "print(\"Total required figures:\", len(required_figures))\n",
    "print(\"Missing:\", len(missing))\n",
    "if missing:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"All required figures are present.\")\n",
    "print(\"Metrics summary exists:\", METRICS_JSON.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3585590",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "- This notebook is intentionally aligned with the report pipeline for strict reproducibility.\n",
    "- For PDF regeneration run:\n",
    "\n",
    "```bash\n",
    "cd ../report\n",
    "make pdf\n",
    "```\n",
    "\n",
    "- The appendix-level theory in the report expands the mathematical assumptions for each method (LIME, SHAP, NAM, calibration, FGSM, activation maximization, SmoothGrad convergence).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
