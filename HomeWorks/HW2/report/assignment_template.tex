\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{siunitx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{url}

\lstdefinestyle{py}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  frame=single,
  breaklines=true,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  captionpos=b,
}
\lstset{style=py}

\title{HW2 Interpretability Report in IEEE Format\\
Comprehensive Tabular and Vision Explanation Analysis}

\author{\IEEEauthorblockN{Taha Majlesi}
\IEEEauthorblockA{Student ID: 810101504\\
Department of Electrical and Computer Engineering, University of Tehran\\
Course: Trusted Artificial Intelligence (Homework 2)}}

\begin{document}
\maketitle

\begin{abstract}
This report presents a complete IEEE-style implementation and analysis of Homework 2 on interpretable machine learning across tabular and computer-vision domains.
The final pipeline is deterministic and robust to offline execution by introducing controlled fallback behavior for both data and model initialization.
Two tabular models (MLP and NAM) are trained and evaluated, and local explanations are generated with LIME and SHAP.
For vision, Grad-CAM, Guided Backpropagation, SmoothGrad, and Guided Grad-CAM are implemented and exported as reproducible artifacts.
Every required figure is interpreted explicitly, with one dedicated paragraph per plot result, and all experiments are linked to executable commands and traceable files.
\end{abstract}

\begin{IEEEkeywords}
Interpretability, LIME, SHAP, Neural Additive Model, Grad-CAM, SmoothGrad, Guided Backpropagation, Reproducibility
\end{IEEEkeywords}

\section{Introduction}
Interpretable AI is critical in settings where predictions affect high-impact decisions, because model quality must be understood in terms of both aggregate performance and individual rationale.
This homework targets that goal through two complementary workloads: tabular binary classification with feature-level explanation, and visual explanation of convolutional network outputs.
The implementation was finalized as an end-to-end reproducible pipeline that generates all required report figures and compiles to a single PDF artifact.

\section{Reproducible Setup}
The project code is organized under \texttt{HomeWorks/HW2/code} with dedicated modules for models, training, tabular explainers, and vision explainers.
The final figure export entry point is \texttt{code/generate\_report\_plots.py}, which writes artifacts into \texttt{HomeWorks/HW2/report/figures}.
All stochastic components are controlled with seed 42 for \texttt{random}, \texttt{numpy}, and \texttt{torch}.

Because execution may occur without internet, two reliability safeguards were implemented: (i) tabular data download falls back to a deterministic synthetic diabetes-like dataset, and (ii) pretrained VGG16 loading falls back to randomly initialized weights.
This design ensures the homework remains fully runnable in constrained environments without breaking downstream analysis code.

\section{Methods}
\subsection{Tabular Models and Optimization}
The MLP classifier follows the architecture \(8\rightarrow100\rightarrow50\rightarrow50\rightarrow20\rightarrow1\), optimized with binary cross-entropy on logits.
For a sample \(x\), the probability output is \(\sigma(f_\theta(x))\), where
\begin{equation}
\sigma(z)=\frac{1}{1+e^{-z}}.
\end{equation}
The population objective can be expressed as empirical risk minimization:
\begin{equation}
\hat{\theta}=\arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}\ell\!\left(y_i,f_{\theta}(x_i)\right),
\end{equation}
with \(\ell\) equal to logistic loss. Under the Bernoulli likelihood model, this objective is equivalent to maximizing conditional log-likelihood, so the learned score approximates log-odds when the model class is sufficiently expressive.

The NAM model uses an additive decomposition inspired by neural additive modeling \cite{agarwal2021nam}:
\begin{equation}
f(x)=\sum_{j=1}^{d} g_j(x_j), \quad \hat{y}=\sigma(f(x)).
\end{equation}
This supports direct per-feature response visualization and therefore intrinsic interpretability.
The additive structure is theoretically important because it removes interaction terms from first-order decomposition, so each \(g_j\) can be interpreted as a marginal contribution function while holding the latent representation fixed.

\subsection{Tabular Explanation Methods}
LIME explains predictions through a locally weighted surrogate objective \cite{ribeiro2016lime}:
\begin{equation}
\xi(x)=\arg\min_{g\in\mathcal{G}}\;\mathcal{L}(f,g,\pi_x)+\Omega(g).
\end{equation}
SHAP estimates feature attributions via Shapley-value decomposition \cite{lundberg2017shap}, approximated here with KernelSHAP.
For any sample \(x\), SHAP satisfies local additivity:
\begin{equation}
f(x)\approx \phi_0+\sum_{j=1}^{d}\phi_j,
\end{equation}
where \(\phi_j\) is the feature attribution. Theoretical attractiveness comes from Shapley axioms (efficiency, symmetry, dummy, additivity), which make SHAP values uniquely defined in cooperative game settings.
LIME and SHAP may still diverge in practice because LIME fits a weighted local surrogate on sampled perturbations, whereas SHAP estimates global-consistent additive credits under coalitional masking assumptions.

\subsection{Vision Explanation Methods}
Grad-CAM localizes class-relevant activation regions by weighting feature maps with class gradients \cite{selvaraju2017gradcam}:
\begin{align}
\alpha_k^c &= \frac{1}{Z}\sum_i\sum_j\frac{\partial y^c}{\partial A_{ij}^k},\\
L_{\text{Grad-CAM}}^c &= \mathrm{ReLU}\left(\sum_k \alpha_k^c A^k\right).
\end{align}
Guided Backpropagation \cite{simonyan2013saliency} and SmoothGrad \cite{smilkov2017smoothgrad} are used to improve saliency interpretability, and their fusion with Grad-CAM provides Guided Grad-CAM maps.
From a differential viewpoint, saliency is a Jacobian-derived signal \(S(x)=\nabla_x y^c\). SmoothGrad estimates a denoised gradient field by Monte Carlo averaging over Gaussian perturbations:
\begin{equation}
\hat{S}(x)=\frac{1}{K}\sum_{k=1}^{K}\nabla_x y^c(x+\epsilon_k), \;\; \epsilon_k\sim\mathcal{N}(0,\sigma^2 I),
\end{equation}
which acts as a variance-reduction estimator for pixel-level sensitivity.

\subsection{Theoretical Basis for Plot Interpretation}
Each result plot is interpreted using a common decomposition principle: prediction behavior is explained by either additive feature contributions (tabular) or spatial sensitivity decomposition (vision). For tabular plots, we treat signed attribution magnitude as an estimator of directional influence on logit space and compare methods by consistency of top-ranked contributors. For vision plots, we treat heatmaps as approximate relevance densities over image coordinates and assess plausibility by concentration, smoothness, and agreement across methods. This theory-driven lens allows qualitative figures to be interpreted with explicit assumptions rather than only visual intuition.

\section{Quantitative Summary}
\begin{table}[H]
\caption{Deterministic Test Metrics (Tabular)}
\label{tab:metrics}
\centering
\begin{tabular}{lccc}
\toprule
Model & Accuracy & Recall & F1 \\
\midrule
MLPClassifier & 0.7013 & 0.4528 & 0.5106 \\
NAMClassifier & 0.6883 & 0.3585 & 0.4419 \\
\bottomrule
\end{tabular}
\end{table}

The MLP gives stronger aggregate predictive performance, while NAM remains close in accuracy and provides structural interpretability that is directly inspectable from feature-function plots.
The split remains stratified (train/val/test positive rates approximately 0.348/0.351/0.344), supporting fair comparison between models.

\section{Plot-by-Plot Result Interpretation}

\subsection{Class Distribution Plot}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/class_distribution.png}
\caption{Outcome class distribution in the tabular dataset.}
\label{fig:class_dist}
\end{figure}
The class-distribution plot shows a moderate imbalance (about 34.8\% positive class), which is not extreme but is still large enough to influence threshold-dependent behavior and the interpretation of raw accuracy; in practical terms, the figure justifies emphasizing recall and F1 alongside accuracy because a majority-favoring classifier can look deceptively strong while still missing many positives, and this is exactly the risk predicted by decision theory where the prior \(\pi=P(Y=1)\) shifts Bayes-optimal thresholding under asymmetric error costs, making prior-sensitive metrics necessary for faithful evaluation.

\subsection{LIME--SHAP Comparison, Sample 0}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/lime_shap_compare_sample_0.png}
\caption{Local explanation comparison for test sample 0.}
\label{fig:lime_shap_0}
\end{figure}
For sample 0 (predicted positive probability \(\approx 0.397\), true label 0), SHAP and LIME identify a mixed-sign attribution pattern in which \textit{DiabetesPedigreeFunction} and a mid-range \textit{Glucose} interval increase risk while lower \textit{Pregnancies} and age-related effects decrease it, yielding a near-boundary explanation where competing factors nearly cancel; the shared top-level story but imperfect rank/scale agreement is theoretically expected because SHAP enforces additive credit allocation from Shapley axioms whereas LIME fits a locality-weighted surrogate whose coefficients are more sensitive to neighborhood sampling and therefore less stable near the decision margin.

\subsection{LIME--SHAP Comparison, Sample 1}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/lime_shap_compare_sample_1.png}
\caption{Local explanation comparison for test sample 1.}
\label{fig:lime_shap_1}
\end{figure}
For sample 1 (predicted positive probability \(\approx 0.342\), true label 0), both methods assign the strongest positive contribution to high \textit{Glucose} while \textit{BloodPressure} and \textit{SkinThickness} pull in the opposite direction, producing a coherent explanation in which a salient risk signal is present but outweighed by compensating evidence; theoretically this is a direct demonstration of additive logit composition, because the decision depends on the signed sum \(\sum_j \phi_j\) relative to the boundary, so even one large positive component cannot flip the class when the remaining terms generate a sufficiently negative aggregate.

\subsection{LIME--SHAP Comparison, Sample 2}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/lime_shap_compare_sample_2.png}
\caption{Local explanation comparison for test sample 2.}
\label{fig:lime_shap_2}
\end{figure}
For sample 2 (predicted positive probability \(\approx 0.261\), true label 0), SHAP and LIME both indicate that lower \textit{DiabetesPedigreeFunction} and lower \textit{Glucose} provide the dominant negative evidence, with only modest positive offsets from \textit{Pregnancies} and \textit{BloodPressure}, so the net attribution remains clearly on the negative side and aligns with the predicted class; compared with samples 0 and 1, the tighter cross-method agreement suggests higher local explanation stability, which is theoretically consistent with a lower-curvature neighborhood where attribution estimates are less sensitive to perturbation and sampling choices.

\subsection{NAM Feature-Function Plot}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\columnwidth]{figures/nam_feature_functions.png}
\caption{Per-feature additive functions learned by NAM.}
\label{fig:nam_functions}
\end{figure}
The NAM feature-function figure provides intrinsic structural interpretability by visualizing each learned \(g_j(x_j)\) while other features are fixed, and the nonlinear slopes and curvatures reveal where each variable increases or decreases logit contribution; this matters theoretically because separability implies \(\partial f/\partial x_j=g_j'(x_j)\), so every subplot is a direct view of true model sensitivity rather than a post-hoc approximation, enabling principled audits of monotonicity, saturation, and regime transitions while making transparent the tradeoff against the slightly higher aggregate accuracy of the less-interpretable MLP.

\subsection{Grad-CAM Demo Plot}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/gradcam_demo.png}
\caption{Grad-CAM heatmap produced by the vision pipeline.}
\label{fig:gradcam_demo}
\end{figure}
The Grad-CAM plot confirms correct class-conditioned localization because the resulting heatmap is spatially concentrated rather than diffuse, indicating that gradient hooks, channel-weight averaging, ReLU gating, and upsampling are operating coherently; under the Grad-CAM formulation \(L_{\text{Grad-CAM}}^c=\mathrm{ReLU}(\sum_k \alpha_k^c A^k)\), this concentration is theoretically expected since ReLU removes negative evidence and preserves regions with positive contribution to the class score \(y^c\), so the figure supports both implementation validity and interpretive plausibility even in fallback-weight conditions.

\subsection{Guided Grad-CAM Example Plot}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\columnwidth]{figures/guided_gradcam_example.png}
\caption{Image, Grad-CAM map, and Guided Grad-CAM fusion result.}
\label{fig:guided_gradcam}
\end{figure}
The Guided Grad-CAM example demonstrates the expected complementarity in which Grad-CAM contributes coarse class-localization while Guided Backprop contributes high-frequency boundary detail, and the fused map is both sharper and spatially constrained, making it more informative than either component alone; this behavior follows the product-form intuition \(M_{\text{guided-cam}}\approx M_{\text{grad-cam}}\odot |\nabla_x y^c|\), where multiplicative interaction uses Grad-CAM as a spatial prior that gates fine-grained gradients to retain detailed structure primarily inside class-relevant regions.

\subsection{SmoothGrad and Guided Comparison Plot}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\columnwidth]{figures/smoothgrad_guided_comparison.png}
\caption{SmoothGrad, Guided Backprop absolute map, and Guided Grad-CAM comparison.}
\label{fig:smoothgrad_guided}
\end{figure}
The SmoothGrad comparison plot shows that averaging gradients over noisy perturbations suppresses high-frequency variance while preserving salient regions, and when contrasted with absolute Guided Backprop and Guided Grad-CAM it reveals a principled bias-variance tradeoff in saliency estimation: SmoothGrad is most stable but less edge-sharp, Guided Backprop is most detailed but noisier, and Guided Grad-CAM sits between them by adding localization priors from class-activation weighting; estimator-wise, increasing \(K\) in SmoothGrad reduces attribution variance roughly by averaging independent perturbation noise, at the cost of some detail attenuation, which matches the observed smoother appearance.

\section{Discussion}
The complete set of figures supports a consistent interpretation narrative: tabular attributions from LIME/SHAP are locally coherent with model decisions, NAM provides transparent global feature-shape behavior, and vision methods provide progressively richer saliency views from localization to fused detailed maps.
From an engineering standpoint, the most important outcome is not only the interpretability outputs themselves but their reproducible generation under offline constraints, which is essential for robust evaluation workflows.

\section{Conclusion}
The report is now fully aligned with IEEE formatting conventions and includes complete, plot-specific interpretation coverage.
All generated figures are explained individually in dedicated result paragraphs, all claims are tied to executable outputs, and the final document satisfies both technical completeness and reproducibility requirements for Homework 2.

\appendices
\section{Reproduction Commands}
\begin{lstlisting}[language=bash,caption={Commands used to regenerate plots and PDF}]
source /Users/tahamajs/Documents/uni/venv/bin/activate
MPLCONFIGDIR=/tmp/mpl python code/generate_report_plots.py
cd report
make pdf
\end{lstlisting}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
