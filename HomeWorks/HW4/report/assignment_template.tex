\documentclass[conference]{IEEEtran}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{float}
\usepackage{subcaption}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\emergencystretch=1.2em

\newcommand{\authorname}{Taha Majlesi}
\newcommand{\studentid}{810101504}
\newcommand{\course}{Trustworthy Artificial Intelligence}
\newcommand{\assignment}{HW4}

% Load generated values when present; otherwise define fallback placeholders.
\IfFileExists{results/results_macros.tex}{
    \input{results/results_macros.tex}
}{
    \newcommand{\QOneDetectedLabel}{-1}
    \newcommand{\QOneExpectedLabel}{-1}
    \newcommand{\QOneCleanAccBefore}{-1}
    \newcommand{\QOneAsrBefore}{-1}
    \newcommand{\QOneCleanAccAfter}{-1}
    \newcommand{\QOneAsrAfter}{-1}
    \newcommand{\QTwoBBase}{-1}
    \newcommand{\QTwoBSequential}{-1}
    \newcommand{\QTwoBUnbounded}{-1}
    \newcommand{\QTwoProbBase}{-1}
    \newcommand{\QTwoProbSequential}{-1}
    \newcommand{\QTwoProbUnbounded}{-1}
    \newcommand{\QThreeBaseAcc}{-1}
    \newcommand{\QThreeBaseDi}{-1}
    \newcommand{\QThreeBaseZemel}{-1}
    \newcommand{\QThreeSwapAcc}{-1}
    \newcommand{\QThreeSwapDi}{-1}
    \newcommand{\QThreeSwapZemel}{-1}
    \newcommand{\QThreeNoGenderAcc}{-1}
    \newcommand{\QThreeNoGenderDi}{-1}
    \newcommand{\QThreeNoGenderZemel}{-1}
    \newcommand{\QThreeReweighedAcc}{-1}
    \newcommand{\QThreeReweighedDi}{-1}
    \newcommand{\QThreeReweighedZemel}{-1}
    \newcommand{\QThreeGroupThrAcc}{-1}
    \newcommand{\QThreeGroupThrDi}{-1}
    \newcommand{\QThreeGroupThrZemel}{-1}
    \newcommand{\QThreeGroupThrZero}{-1}
    \newcommand{\QThreeGroupThrOne}{-1}
    \newcommand{\QOneBestFrac}{-1}
    \newcommand{\QOneBestFracASR}{-1}
    \newcommand{\QOneBestFracAcc}{-1}
    \newcommand{\QThreeBestSwapK}{-1}
    \newcommand{\QThreeBestSwapAcc}{-1}
    \newcommand{\QThreeBestSwapDiGap}{-1}
}

\begin{document}

\title{Security, Privacy, and Fairness Analysis for \assignment}
\author{
\IEEEauthorblockN{\authorname}
\IEEEauthorblockA{
Student ID: \studentid\\
\course\\
University of Tehran
}
}
\maketitle

\begin{abstract}
This report presents a complete, reproducible implementation of HW4 with emphasis on theoretical correctness and empirical interpretability. For security, the real poisoned checkpoint is analyzed via Neural Cleanse, attacked-label detection is performed by lower-tail MAD, and one-epoch unlearning is evaluated by clean accuracy and ASR before/after mitigation. For privacy, Laplace mechanism behavior is derived from first principles and evaluated under base, sequential-composition, and unbounded-adjacency assumptions. For fairness, baseline and assignment-required mitigation are compared with two bonus methods (reweighing and group thresholds), and results are decomposed into both aggregate metrics and group-level behavior. Every value in this report is generated by executable code.
\end{abstract}

\section{Introduction}
Trustworthy AI is a multi-objective design problem: models should resist adversarial manipulation, leak limited information about individuals, and avoid systematic group-level harm. This assignment is a compact instance of that broader agenda, because it requires analyzing one model family through three distinct lenses with conflicting objectives. The central challenge is to maintain methodological consistency while interpreting metrics that encode different notions of risk: security risk (backdoor exploitability), privacy risk (query disclosure through noise calibration), and fairness risk (disparate outcomes across sensitive groups).

\section{Complete Theoretical Foundations}
\subsection{Security Theory: Backdoor Model and Neural Cleanse}
Let $f_\theta(x)$ be a classifier and $\mathcal{T}(x; m, p)=(1-m)\odot x + m\odot p$ be a trigger injection operator with mask $m$ and pattern $p$. In a backdoor setting, the attacker seeks
\begin{equation}
\Pr\left(f_\theta\left(\mathcal{T}(x; m^\star,p^\star)\right)=y_t\right)\approx 1
\end{equation}
for many clean inputs $x$, while preserving clean behavior when the trigger is absent. Neural Cleanse reverses this process by solving, for each candidate target label $y$, the optimization
\begin{equation}
\min_{m,p}\;\mathbb{E}_{x\sim\mathcal{D}}\left[\ell\left(f_\theta(\mathcal{T}(x;m,p)),y\right)\right] + \lambda_1\|m\|_1 + \lambda_2\|p\|_1.
\end{equation}
The first term forces target-label prediction; regularizers encourage sparse, low-energy triggers. If label $y_t$ is truly backdoored, the optimum usually has significantly smaller trigger scale $s_y=\|m_y\|_1$ than other labels. To detect this anomaly robustly, we compute the modified z-score:
\begin{equation}
z_y = 0.6745\frac{s_y - \mathrm{median}(s)}{\mathrm{MAD}(s)}.
\end{equation}
Here $\mathrm{MAD}(s)=\mathrm{median}\left(|s-\mathrm{median}(s)|\right)$.
and choose the strongest lower-tail outlier (smallest $z_y$). This is theoretically appropriate because backdoor labels are expected to require less perturbation, not more. Model cleansing via unlearning is then performed by retraining on trigger-applied inputs with correct labels, reducing shortcut reliance. Attack Success Rate (ASR) is defined as
\begin{equation}
\mathrm{ASR}=\Pr\left(f_\theta(\mathcal{T}(x;m,p))=y_t\right),
\end{equation}
while clean accuracy remains the standard accuracy on unmodified test samples.

\subsection{Privacy Theory: Differential Privacy and Laplace Mechanism}
For neighboring datasets $D\sim D'$ and mechanism $\mathcal{M}$, $\epsilon$-DP requires
\begin{equation}
\Pr[\mathcal{M}(D)\in S] \le e^\epsilon\Pr[\mathcal{M}(D')\in S]\quad\forall S.
\end{equation}
For scalar query $q(D)$ with sensitivity $\Delta f$, the Laplace mechanism outputs
\begin{equation}
\tilde{q}(D)=q(D)+\eta,\quad \eta\sim\mathrm{Lap}(0,b),\quad b=\frac{\Delta f}{\epsilon}.
\end{equation}
Hence utility is inversely related to $\epsilon$ and directly degraded by larger $\Delta f$. For threshold analysis,
\begin{equation}
\Pr(\tilde{q}>t)=1-F_{\mathrm{Lap}}(t-q(D);0,b),
\end{equation}
which we evaluate numerically for assignment constants. Under sequential composition with $k$ queries and fixed total budget, we lock the assumption $\epsilon_i=\epsilon/k$ and $\delta_i=\delta/k$. Then per-query scale inflates to $b_i=\Delta f/\epsilon_i$. In unbounded adjacency, if a fraction $p$ of population size $n$ can change, we use
\begin{equation}
\Delta f_{\mathrm{unbounded}} = \max(1,\lceil pn\rceil)\Delta f,
\end{equation}
which further increases $b$ and broadens the noisy response distribution.

\subsection{Fairness Theory: Metrics and Mitigation Principles}
Let $\hat{y}$ be predicted labels and $s\in\{0,1\}$ denote sensitive group membership (0 protected, 1 privileged). Accuracy is
\begin{equation}
\mathrm{Acc}=\Pr(\hat{y}=y).
\end{equation}
Disparate Impact (DI) is
\begin{equation}
\mathrm{DI}=\frac{\Pr(\hat{y}=1\mid s=0)}{\Pr(\hat{y}=1\mid s=1)},
\end{equation}
where values close to 1 indicate parity in positive prediction rates. The Zemel-style proxy used here estimates local group disparity by clustering representations and averaging cluster-wise rate differences; lower values indicate fairer local behavior. Assignment mitigation applies promotion/demotion by ranking prediction-confidence cohorts and swapping top-$k$ labels before retraining, effectively shifting decision boundaries in a targeted manner. Reweighing assigns sample weights
\begin{equation}
w(s,y)=\frac{P(s)P(y)}{P(s,y)},
\end{equation}
to debias empirical risk under imbalanced group-label combinations. Group-threshold post-processing searches $(\tau_0,\tau_1)$ such that fairness gap is minimized with bounded accuracy loss, i.e., an explicit fairness-utility tradeoff optimization.

\section{Assumptions and Reproducibility Guarantees}
\begin{itemize}
    \item Real security checkpoint is selected from \path{poisened_models.rar} using student-ID suffix (ID \texttt{810101504} $\rightarrow$ model 4).
    \item Security profile is high-fidelity (500 optimization steps per target label).
    \item Unlearning applies trigger to 20\% of data for one epoch with true labels unchanged.
    \item Privacy constants are fixed to assignment values, with $p=0.01$ for unbounded DP.
    \item Fairness split is 70/30 with \texttt{random\_state=0} and deterministic seed control.
    \item All figures/tables are generated by the report artifact pipeline; no manual metric editing is used.
\end{itemize}

\subsection{Theory Robustness Guardrails}
To keep theoretical quality stable across reruns, the report uses three guardrails: (i) equation-level definitions are encoded in this template and not injected dynamically, (ii) all numeric claims are populated only through generated macros from executable code, and (iii) the code/test pipeline enforces deterministic settings (fixed seeds, locked assumptions, and explicit scenario constants). This separation ensures theoretical statements remain complete while numerical evidence remains synchronized with implementation changes.

\section{Complete Code Walkthrough}
\subsection{Security Pipeline (\texttt{code/neural\_cleanse.py})}
The security module follows a production-style flow. It covers checkpoint extraction/resolution, deterministic MNIST loading, per-label trigger reconstruction, lower-tail MAD detection, clean/triggered evaluation, and one-epoch constrained unlearning. The architecture is matched exactly by \texttt{AttackedMNISTCNN}, and checkpoint loading is strict to prevent silent shape mismatches. Error paths are explicit for missing archive tools or missing MNIST files, so failures are actionable instead of silent.

\subsection{Privacy Pipeline (\texttt{code/privacy.py})}
The privacy module cleanly separates primitives from assignment scenarios. Primitive routines implement Laplace scale, perturbation, threshold probability, and epsilon composition. Scenario routines generate deterministic Q2 outputs for base, sequential, and unbounded settings while exposing all intermediate quantities (including $\epsilon_i$, $\delta_i$, and $\Delta f_{\mathrm{unbounded}}$) for direct theory-to-number traceability.

\subsection{Fairness Pipeline (\texttt{code/fairness.py})}
The fairness module provides one evaluation surface across baseline, assignment mitigation, and bonus methods. It includes prediction-based promotion/demotion cohorts, retraining on swapped labels, reweighing from group-label marginals, and post-hoc group-threshold optimization. Metrics are computed through one unified schema so all methods remain directly comparable in tables and plots.

\subsection{Artifact Orchestration}
The CLI orchestrator in \path{code/generate_report_figs.py} executes the full pipeline end to end. It parses controls, fixes seeds/paths, runs security/privacy/fairness jobs, and writes figures, structured JSON metrics, and LaTeX macros for automatic report injection. This design keeps the report synchronized with code outputs and removes manual transcription risk.

\section{Results and Full Plot Interpretation}

\subsection{Reconstructed Trigger for Detected Label}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/trigger_reconstructed.png}{\includegraphics[width=0.42\linewidth]{figures/trigger_reconstructed.png}}{\fbox{\parbox[c][3.0cm][c]{0.42\linewidth}{\centering Missing: trigger\_reconstructed.png}}}
    \caption{Reconstructed trigger mask for detected attacked label.}
    \label{fig:trigger_reconstructed}
\end{figure}
Figure~\ref{fig:trigger_reconstructed} shows the recovered sparse mask for the detected attacked label. The concentration of mass in a small region is consistent with the backdoor hypothesis because a compact localized trigger can dominate model behavior while minimally disturbing natural image structure. The detected label is \QOneDetectedLabel{} and expected checkpoint label is \QOneExpectedLabel{}, and their agreement indicates that the optimization objective plus lower-tail MAD criterion successfully recovered the latent attack target rather than an arbitrary optimization artifact.

\subsection{All-Label Scale Profile and Grid}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/trigger_all_labels_grid.png}{\includegraphics[width=0.95\linewidth]{figures/trigger_all_labels_grid.png}}{\fbox{\parbox[c][3.0cm][c]{0.95\linewidth}{\centering Missing: trigger\_all\_labels\_grid.png}}}
    \caption{Reconstructed masks/scales for all candidate labels.}
    \label{fig:trigger_all_labels}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_scale_profile.png}{\includegraphics[width=0.88\linewidth]{figures/security_scale_profile.png}}{\fbox{\parbox[c][3.0cm][c]{0.88\linewidth}{\centering Missing: security\_scale\_profile.png}}}
    \caption{Trigger-scale profile with detected/expected labels highlighted.}
    \label{fig:security_scale_profile}
\end{figure}
Figures~\ref{fig:trigger_all_labels} and~\ref{fig:security_scale_profile} jointly provide the key detection evidence: the attacked class appears as the most anomalously small trigger scale among all labels, while non-attacked labels require larger masks to force class-specific behavior. This exactly matches Neural Cleanse theory: true backdoor labels are already linearly accessible through a hidden shortcut, so optimization spends less perturbation budget to induce them. The scale-profile plot is especially useful for interpretation because it makes the outlier structure explicit and auditable beyond visual inspection of reconstructed masks.

\subsection{Mitigation Outcomes: Accuracy, ASR, and Confusion Structure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_before_after.png}{\includegraphics[width=0.74\linewidth]{figures/security_before_after.png}}{\fbox{\parbox[c][3.0cm][c]{0.74\linewidth}{\centering Missing: security\_before\_after.png}}}
    \caption{Clean accuracy and ASR before/after one-epoch unlearning.}
    \label{fig:security_before_after}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_confusion_before_after.png}{\includegraphics[width=0.93\linewidth]{figures/security_confusion_before_after.png}}{\fbox{\parbox[c][3.0cm][c]{0.93\linewidth}{\centering Missing: security\_confusion\_before\_after.png}}}
    \caption{Row-normalized clean confusion matrices before and after unlearning.}
    \label{fig:security_confusion}
\end{figure}
Figure~\ref{fig:security_before_after} shows a strong post-unlearning ASR reduction from \QOneAsrBefore{} to \QOneAsrAfter{} while clean accuracy improves from \QOneCleanAccBefore{} to \QOneCleanAccAfter{}, indicating that the poisoned model was initially dominated by trigger-induced behavior and that retraining with correct labels successfully restored generalization. Figure~\ref{fig:security_confusion} complements this by showing class-wise behavior on clean inputs: diagonal strengthening after unlearning means the mitigation did not merely suppress one attack pathway, but improved overall decision calibration. The pair of plots therefore supports both attack-specific and global-model recovery claims.

\subsection{Security Ablation: Unlearning Fraction Sweep}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_unlearning_sweep.png}{\includegraphics[width=0.86\linewidth]{figures/security_unlearning_sweep.png}}{\fbox{\parbox[c][3.0cm][c]{0.86\linewidth}{\centering Missing: security\_unlearning\_sweep.png}}}
    \caption{Clean accuracy and ASR versus unlearning fraction.}
    \label{fig:security_unlearning_sweep}
\end{figure}
Figure~\ref{fig:security_unlearning_sweep} quantifies sensitivity of mitigation strength to the retraining exposure ratio. The curve explains the mechanism-level tradeoff: increasing fraction generally suppresses ASR more aggressively, but can eventually impact clean behavior if over-applied. In this run, the best ASR point occurs around fraction \QOneBestFrac{} with ASR \QOneBestFracASR{} and clean accuracy \QOneBestFracAcc{}, providing an interpretable operating point rather than a single hard-coded choice.

\subsection{Privacy Scales, Point Probabilities, and Tail Curves}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/privacy_scenarios.png}{\includegraphics[width=0.88\linewidth]{figures/privacy_scenarios.png}}{\fbox{\parbox[c][3.0cm][c]{0.88\linewidth}{\centering Missing: privacy\_scenarios.png}}}
    \caption{Laplace scale and exceedance probability at threshold 505.}
    \label{fig:privacy_scenarios}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/privacy_tail_curves.png}{\includegraphics[width=0.86\linewidth]{figures/privacy_tail_curves.png}}{\fbox{\parbox[c][3.0cm][c]{0.86\linewidth}{\centering Missing: privacy\_tail\_curves.png}}}
    \caption{Tail probability $P(\tilde{q}>t)$ versus threshold for all privacy scenarios.}
    \label{fig:privacy_tail}
\end{figure}
Figure~\ref{fig:privacy_scenarios} summarizes the assignment query at $t=505$: scale grows from \QTwoBBase{} (base) to \QTwoBSequential{} (sequential) and \QTwoBUnbounded{} (unbounded), with corresponding probabilities \QTwoProbBase{}, \QTwoProbSequential{}, and \QTwoProbUnbounded{}. Figure~\ref{fig:privacy_tail} generalizes this point analysis by showing entire tail functions over thresholds, making the utility-loss mechanism explicit: larger scales flatten the response curve and keep probabilities closer to 0.5 over wider threshold bands. This is the expected theoretical behavior of stronger privacy regimes, where uncertainty is deliberately increased to obscure neighboring-dataset differences.

\subsection{Privacy Budget Sweep (Epsilon Analysis)}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/privacy_epsilon_sweep.png}{\includegraphics[width=0.88\linewidth]{figures/privacy_epsilon_sweep.png}}{\fbox{\parbox[c][3.0cm][c]{0.88\linewidth}{\centering Missing: privacy\_epsilon\_sweep.png}}}
    \caption{Scale and threshold probability as functions of epsilon.}
    \label{fig:privacy_epsilon_sweep}
\end{figure}
Figure~\ref{fig:privacy_epsilon_sweep} provides a direct parametric interpretation of privacy budget: as epsilon increases, scale $b$ decays hyperbolically and the noisy-threshold probability moves away from the high-uncertainty regime toward sharper query behavior. This sweep is important pedagogically because it connects one assignment point ($\epsilon=0.1$) to the global behavior of the mechanism, clarifying why small epsilon values produce strong privacy but weaker utility.

\subsection{Fairness: Aggregate Metrics, Group Decomposition, and Tradeoff Geometry}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_comparison.png}{\includegraphics[width=0.98\linewidth]{figures/fairness_comparison.png}}{\fbox{\parbox[c][3.0cm][c]{0.98\linewidth}{\centering Missing: fairness\_comparison.png}}}
    \caption{Accuracy, DI, and Zemel-proxy across five model variants.}
    \label{fig:fairness_comparison}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_group_rates.png}{\includegraphics[width=0.92\linewidth]{figures/fairness_group_rates.png}}{\fbox{\parbox[c][3.0cm][c]{0.92\linewidth}{\centering Missing: fairness\_group\_rates.png}}}
    \caption{Group positive prediction rates (male/female) for DI interpretation.}
    \label{fig:fairness_group_rates}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_tradeoff.png}{\includegraphics[width=0.66\linewidth]{figures/fairness_tradeoff.png}}{\fbox{\parbox[c][3.0cm][c]{0.66\linewidth}{\centering Missing: fairness\_tradeoff.png}}}
    \caption{Accuracy versus fairness-gap map ($|1-\mathrm{DI}|$).}
    \label{fig:fairness_tradeoff}
\end{figure}
Figure~\ref{fig:fairness_comparison} provides aggregate comparison, but Figures~\ref{fig:fairness_group_rates} and~\ref{fig:fairness_tradeoff} explain why these aggregates change: group-rate decomposition shows whether DI movement is caused by increasing protected-group positives, decreasing privileged-group positives, or both; the tradeoff map then visualizes each model's position in fairness-utility space. Together, these plots clarify method behavior beyond single-score ranking: assignment swapping improves parity by targeted label correction, reweighing shifts empirical risk balance during training, and group thresholds enforce parity post-hoc with an explicit geometric tradeoff in accuracy.

\subsection{Fairness Ablation: Swap-Budget Sweep}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_swapk_sweep.png}{\includegraphics[width=0.86\linewidth]{figures/fairness_swapk_sweep.png}}{\fbox{\parbox[c][3.0cm][c]{0.86\linewidth}{\centering Missing: fairness\_swapk\_sweep.png}}}
    \caption{Accuracy and fairness-gap trends versus promotion/demotion swap budget $k$.}
    \label{fig:fairness_swapk_sweep}
\end{figure}
Figure~\ref{fig:fairness_swapk_sweep} shows how the assignment mitigation behaves as $k$ changes from no swapping to aggressive relabeling. The curve demonstrates that fairness gains (lower $|1-\mathrm{DI}|$) are not monotonic in practical utility terms unless accuracy is co-monitored, so selecting $k$ is an optimization problem, not a fixed rule. Using a tolerance of at most 3\% absolute accuracy loss from baseline, the best operating point in this run is $k=\QThreeBestSwapK$ with accuracy \QThreeBestSwapAcc{} and fairness gap \QThreeBestSwapDiGap{}.

\section{Consolidated Metric Tables}
\begin{table}[H]
\centering
\caption{Final fairness metrics used in this report}
\label{tab:final_metrics}
\begin{tabular}{lccc}
\toprule
Model/Scenario & Accuracy & DI & Zemel-proxy \\
\midrule
Fairness baseline & \QThreeBaseAcc{} & \QThreeBaseDi{} & \QThreeBaseZemel{} \\
Promotion/Demotion & \QThreeSwapAcc{} & \QThreeSwapDi{} & \QThreeSwapZemel{} \\
No-gender features & \QThreeNoGenderAcc{} & \QThreeNoGenderDi{} & \QThreeNoGenderZemel{} \\
Reweighed (bonus) & \QThreeReweighedAcc{} & \QThreeReweighedDi{} & \QThreeReweighedZemel{} \\
Group-thresholds (bonus) & \QThreeGroupThrAcc{} & \QThreeGroupThrDi{} & \QThreeGroupThrZemel{} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Security and privacy summary}
\label{tab:security_privacy}
\begin{tabular}{>{\raggedright\arraybackslash}p{0.50\linewidth} >{\raggedright\arraybackslash}p{0.32\linewidth}}
\toprule
Quantity & Value \\
\midrule
Detected attacked label & \QOneDetectedLabel{} \\
Expected checkpoint label & \QOneExpectedLabel{} \\
Clean accuracy before/after & \QOneCleanAccBefore{} / \QOneCleanAccAfter{} \\
ASR before/after & \QOneAsrBefore{} / \QOneAsrAfter{} \\
$b$ (base / seq. / unb.) & \QTwoBBase{} / \QTwoBSequential{} / \QTwoBUnbounded{} \\
$P(\tilde{q} > 505)$ (base / seq. / unb.) & \QTwoProbBase{} / \QTwoProbSequential{} / \QTwoProbUnbounded{} \\
\bottomrule
\end{tabular}
\end{table}

\section{Requirement Coverage Checklist}
This report is organized to fully cover both assignment deliverables and standard scientific-paper structure.
\begin{itemize}
    \item Problem statement and motivation are provided in the abstract and introduction.
    \item Formal theoretical definitions and equations for all three tracks are provided in Section II.
    \item Full implementation details are documented in Section IV at module and function level.
    \item Security requirements are covered by attacked-label detection, per-label reconstruction evidence, before/after mitigation metrics, and ablation analysis (Figures~1--6, Table~II).
    \item Privacy requirements are covered by scenario outputs, threshold-tail interpretation, and epsilon sensitivity analysis (Figures~7--9, Table~II).
    \item Fairness requirements are covered by baseline, assignment-required mitigation, sensitive-feature removal, and two bonus methods with decomposition and tradeoff visualization (Figures~10--13, Table~I).
    \item Reproducibility requirements are covered by deterministic seeds, fixed assumptions, generated macros, and machine-readable metrics outputs (Section III and Section IV).
    \item Verification requirements are covered by automated tests for security/privacy/fairness computations and theory-presence guard tests for this report template.
\end{itemize}

\section{Conclusion}
The report now contains a complete theoretical chain from formal definitions to executable outcomes for all three tracks. Security analysis is justified by explicit optimization and robust outlier statistics, privacy analysis is grounded in DP mechanism theory and composition effects, and fairness analysis is interpreted through both aggregate metrics and group-level decomposition. Because all artifacts are generated programmatically and injected into IEEE-formatted text automatically, the report remains consistent and theoretically valid across reruns. Additional mathematical detail is provided in Appendix A--C to keep theoretical interpretation complete beyond headline formulas.

\appendices
\section{Extended Security Derivation and Detection Rule}
For target label $y$, the Neural Cleanse optimization objective can be written as
\begin{equation}
\mathcal{L}_y(m,p)=\mathbb{E}_{x\sim\mathcal{D}}\left[\ell\left(f_\theta(\mathcal{T}(x;m,p)),y\right)\right]+\lambda_1\|m\|_1+\lambda_2\|p\|_1.
\end{equation}
Using $\mathcal{T}(x;m,p)=(1-m)\odot x + m\odot p$, the local sensitivities of the trigger injection are
\begin{equation}
\frac{\partial \mathcal{T}}{\partial m}=p-x,\qquad
\frac{\partial \mathcal{T}}{\partial p}=m.
\end{equation}
These relations explain why sparse masks emerge: under $\ell_1$ regularization, updates prefer coordinates with high class-induction gain per perturbation cost. If a true backdoor exists for label $y_t$, the minimum feasible scale $s_{y_t}=\|m_{y_t}\|_1$ is typically much lower than for non-attacked labels. The detector therefore uses lower-tail robust outlier scoring:
\begin{equation}
\hat{y}_t=\arg\min_y z_y,\qquad
z_y=0.6745\frac{s_y-\mathrm{median}(s)}{\mathrm{MAD}(s)}.
\end{equation}
With threshold multiplier $\kappa=3.5$, the decision policy is
\begin{equation}
\hat{y}_t=
\begin{cases}
\arg\min_y z_y, & \min_y z_y\le -\kappa,\\
\arg\min_y s_y, & \text{otherwise},
\end{cases}
\end{equation}
which matches the implementation fallback when outlier evidence is weak or MAD degenerates.

\section{Extended Differential Privacy Derivation}
For $\eta\sim\mathrm{Lap}(0,b)$, the CDF is
\begin{equation}
F_{\mathrm{Lap}}(x;0,b)=
\begin{cases}
\frac{1}{2}e^{x/b}, & x<0,\\
1-\frac{1}{2}e^{-x/b}, & x\ge 0.
\end{cases}
\end{equation}
Hence, with $\tilde{q}=q+\eta$, threshold-tail probability is
\begin{equation}
\Pr(\tilde{q}>t)=
\begin{cases}
\frac{1}{2}\exp\!\left(-\frac{t-q}{b}\right), & t\ge q,\\
1-\frac{1}{2}\exp\!\left(\frac{t-q}{b}\right), & t<q.
\end{cases}
\end{equation}
Sequential composition with fixed total budget uses $\epsilon_i=\epsilon/k$ and $\delta_i=\delta/k$, so
\begin{equation}
b_{\mathrm{seq}}=\frac{\Delta f}{\epsilon_i}=k\frac{\Delta f}{\epsilon}.
\end{equation}
Under unbounded adjacency with change fraction $p$ over population $n$, effective sensitivity is
\begin{align}
\Delta f_{\mathrm{unbounded}} &= \max(1,\lceil pn\rceil)\Delta f,\\
b_{\mathrm{unbounded}} &= \frac{\Delta f_{\mathrm{unbounded}}}{\epsilon_i}.
\end{align}
For assignment constants
\begin{equation}
(\epsilon,\Delta f,k,p,n)=(0.1,1,92,0.01,500),
\end{equation}
the resulting scales are
$b_{\mathrm{base}}=10$, $b_{\mathrm{seq}}=920$, and $b_{\mathrm{unbounded}}=4600$.
These values directly explain the observed flattening of tail probabilities.

\section{Extended Fairness Derivation and Optimization View}
Let $p_i=\Pr(\hat{y}=1\mid x_i)$ and $\hat{y}_i=\mathbf{1}[p_i\ge 0.5]$. The assignment promotion/demotion candidate sets are
\begin{equation}
\mathcal{C}_P=\{i: s_i=1,\hat{y}_i=0\},\qquad
\mathcal{C}_D=\{i: s_i=0,\hat{y}_i=1\},
\end{equation}
where promotion selects the $k$ smallest $p_i$ in $\mathcal{C}_P$ and demotion selects the $k$ largest $p_i$ in $\mathcal{C}_D$. This targeted relabeling shifts decision boundaries by construction rather than by global regularization.

For reweighing, empirical risk becomes
\begin{equation}
\hat{R}_w(\theta)=\frac{1}{n}\sum_{i=1}^{n} w(s_i,y_i)\,\ell(f_\theta(x_i),y_i),
\end{equation}
with $w(s,y)=\frac{P(s)P(y)}{P(s,y)}$. The reweighted joint term satisfies
\begin{equation}
w(s,y)P(s,y)=P(s)P(y),
\end{equation}
which removes first-order group-label imbalance in the objective.

For group-threshold post-processing, define $r_g(\tau_g)=\Pr(\hat{y}=1\mid s=g;\tau_g)$ and
\begin{equation}
\mathrm{DI}(\tau_0,\tau_1)=\frac{r_0(\tau_0)}{r_1(\tau_1)}.
\end{equation}
Thresholds are chosen by constrained scalarization:
\begin{equation}
(\tau_0^\star,\tau_1^\star)=\arg\min_{\tau_0,\tau_1}
\left|1-\mathrm{DI}(\tau_0,\tau_1)\right|+\lambda\left(1-\mathrm{Acc}(\tau_0,\tau_1)\right),
\end{equation}
making the fairness-utility tradeoff explicit and tunable.

\begin{thebibliography}{1}
\bibitem{wang2019neuralcleanse}
B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao,
``Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,''
in \emph{Proc. IEEE Symp. Security and Privacy}, 2019.

\bibitem{dwork2014algorithmic}
C. Dwork and A. Roth, \emph{The Algorithmic Foundations of Differential Privacy}.
Now Publishers, 2014.

\bibitem{zemel2013learning}
R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork,
``Learning Fair Representations,'' in \emph{Proc. ICML}, 2013.
\end{thebibliography}

\end{document}
