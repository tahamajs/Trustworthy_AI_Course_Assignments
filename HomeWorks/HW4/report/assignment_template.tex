\documentclass[conference]{IEEEtran}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}

\newcommand{\authorname}{Taha Majlesi}
\newcommand{\studentid}{810101504}
\newcommand{\course}{Trustworthy Artificial Intelligence}
\newcommand{\assignment}{HW4}

% Load generated values when present; otherwise define fallback placeholders.
\IfFileExists{results/results_macros.tex}{
    \input{results/results_macros.tex}
}{
    \newcommand{\QOneDetectedLabel}{-1}
    \newcommand{\QOneExpectedLabel}{-1}
    \newcommand{\QOneCleanAccBefore}{-1}
    \newcommand{\QOneAsrBefore}{-1}
    \newcommand{\QOneCleanAccAfter}{-1}
    \newcommand{\QOneAsrAfter}{-1}
    \newcommand{\QTwoBBase}{-1}
    \newcommand{\QTwoBSequential}{-1}
    \newcommand{\QTwoBUnbounded}{-1}
    \newcommand{\QTwoProbBase}{-1}
    \newcommand{\QTwoProbSequential}{-1}
    \newcommand{\QTwoProbUnbounded}{-1}
    \newcommand{\QThreeBaseAcc}{-1}
    \newcommand{\QThreeBaseDi}{-1}
    \newcommand{\QThreeBaseZemel}{-1}
    \newcommand{\QThreeSwapAcc}{-1}
    \newcommand{\QThreeSwapDi}{-1}
    \newcommand{\QThreeSwapZemel}{-1}
    \newcommand{\QThreeNoGenderAcc}{-1}
    \newcommand{\QThreeNoGenderDi}{-1}
    \newcommand{\QThreeNoGenderZemel}{-1}
    \newcommand{\QThreeReweighedAcc}{-1}
    \newcommand{\QThreeReweighedDi}{-1}
    \newcommand{\QThreeReweighedZemel}{-1}
    \newcommand{\QThreeGroupThrAcc}{-1}
    \newcommand{\QThreeGroupThrDi}{-1}
    \newcommand{\QThreeGroupThrZemel}{-1}
    \newcommand{\QThreeGroupThrZero}{-1}
    \newcommand{\QThreeGroupThrOne}{-1}
    \newcommand{\QOneBestFrac}{-1}
    \newcommand{\QOneBestFracASR}{-1}
    \newcommand{\QOneBestFracAcc}{-1}
    \newcommand{\QThreeBestSwapK}{-1}
    \newcommand{\QThreeBestSwapAcc}{-1}
    \newcommand{\QThreeBestSwapDiGap}{-1}
}

\begin{document}

\title{Security, Privacy, and Fairness Analysis for \assignment}
\author{
\IEEEauthorblockN{\authorname}
\IEEEauthorblockA{
Student ID: \studentid\\
\course\\
University of Tehran
}
}
\maketitle

\begin{abstract}
This report presents a complete, reproducible implementation of HW4 with emphasis on theoretical correctness and empirical interpretability. For security, the real poisoned checkpoint is analyzed via Neural Cleanse, attacked-label detection is performed by lower-tail MAD, and one-epoch unlearning is evaluated by clean accuracy and ASR before/after mitigation. For privacy, Laplace mechanism behavior is derived from first principles and evaluated under base, sequential-composition, and unbounded-adjacency assumptions. For fairness, baseline and assignment-required mitigation are compared with two bonus methods (reweighing and group thresholds), and results are decomposed into both aggregate metrics and group-level behavior. Every value in this report is generated by executable code.
\end{abstract}

\section{Introduction}
Trustworthy AI is a multi-objective design problem: models should resist adversarial manipulation, leak limited information about individuals, and avoid systematic group-level harm. This assignment is a compact instance of that broader agenda, because it requires analyzing one model family through three distinct lenses with conflicting objectives. The central challenge is to maintain methodological consistency while interpreting metrics that encode different notions of risk: security risk (backdoor exploitability), privacy risk (query disclosure through noise calibration), and fairness risk (disparate outcomes across sensitive groups).

\section{Complete Theoretical Foundations}
\subsection{Security Theory: Backdoor Model and Neural Cleanse}
Let $f_\theta(x)$ be a classifier and $\mathcal{T}(x; m, p)=(1-m)\odot x + m\odot p$ be a trigger injection operator with mask $m$ and pattern $p$. In a backdoor setting, the attacker seeks
\begin{equation}
\Pr\left(f_\theta\left(\mathcal{T}(x; m^\star,p^\star)\right)=y_t\right)\approx 1
\end{equation}
for many clean inputs $x$, while preserving clean behavior when the trigger is absent. Neural Cleanse reverses this process by solving, for each candidate target label $y$, the optimization
\begin{equation}
\min_{m,p}\;\mathbb{E}_{x\sim\mathcal{D}}\left[\ell\left(f_\theta(\mathcal{T}(x;m,p)),y\right)\right] + \lambda_1\|m\|_1 + \lambda_2\|p\|_1.
\end{equation}
The first term forces target-label prediction; regularizers encourage sparse, low-energy triggers. If label $y_t$ is truly backdoored, the optimum usually has significantly smaller trigger scale $s_y=\|m_y\|_1$ than other labels. To detect this anomaly robustly, we compute the modified z-score with MAD:
\begin{equation}
z_y = 0.6745\frac{s_y - \mathrm{median}(s)}{\mathrm{MAD}(s)},\quad \mathrm{MAD}(s)=\mathrm{median}\left(|s-\mathrm{median}(s)|\right),
\end{equation}
and choose the strongest lower-tail outlier (smallest $z_y$). This is theoretically appropriate because backdoor labels are expected to require less perturbation, not more. Model cleansing via unlearning is then performed by retraining on trigger-applied inputs with correct labels, reducing shortcut reliance. Attack Success Rate (ASR) is defined as
\begin{equation}
\mathrm{ASR}=\Pr\left(f_\theta(\mathcal{T}(x;m,p))=y_t\right),
\end{equation}
while clean accuracy remains the standard accuracy on unmodified test samples.

\subsection{Privacy Theory: Differential Privacy and Laplace Mechanism}
For neighboring datasets $D\sim D'$ and mechanism $\mathcal{M}$, $\epsilon$-DP requires
\begin{equation}
\Pr[\mathcal{M}(D)\in S] \le e^\epsilon\Pr[\mathcal{M}(D')\in S]\quad\forall S.
\end{equation}
For scalar query $q(D)$ with sensitivity $\Delta f$, the Laplace mechanism outputs
\begin{equation}
\tilde{q}(D)=q(D)+\eta,\quad \eta\sim\mathrm{Lap}(0,b),\quad b=\frac{\Delta f}{\epsilon}.
\end{equation}
Hence utility is inversely related to $\epsilon$ and directly degraded by larger $\Delta f$. For threshold analysis,
\begin{equation}
\Pr(\tilde{q}>t)=1-F_{\mathrm{Lap}}(t-q(D);0,b),
\end{equation}
which we evaluate numerically for assignment constants. Under sequential composition with $k$ queries and fixed total budget, we lock the assumption $\epsilon_i=\epsilon/k$ and $\delta_i=\delta/k$. Then per-query scale inflates to $b_i=\Delta f/\epsilon_i$. In unbounded adjacency, if a fraction $p$ of population size $n$ can change, we use
\begin{equation}
\Delta f_{\mathrm{unbounded}} = \max(1,\lceil pn\rceil)\Delta f,
\end{equation}
which further increases $b$ and broadens the noisy response distribution.

\subsection{Fairness Theory: Metrics and Mitigation Principles}
Let $\hat{y}$ be predicted labels and $s\in\{0,1\}$ denote sensitive group membership (0 protected, 1 privileged). Accuracy is
\begin{equation}
\mathrm{Acc}=\Pr(\hat{y}=y).
\end{equation}
Disparate Impact (DI) is
\begin{equation}
\mathrm{DI}=\frac{\Pr(\hat{y}=1\mid s=0)}{\Pr(\hat{y}=1\mid s=1)},
\end{equation}
where values close to 1 indicate parity in positive prediction rates. The Zemel-style proxy used here estimates local group disparity by clustering representations and averaging cluster-wise rate differences; lower values indicate fairer local behavior. Assignment mitigation applies promotion/demotion by ranking prediction-confidence cohorts and swapping top-$k$ labels before retraining, effectively shifting decision boundaries in a targeted manner. Reweighing assigns sample weights
\begin{equation}
w(s,y)=\frac{P(s)P(y)}{P(s,y)},
\end{equation}
to debias empirical risk under imbalanced group-label combinations. Group-threshold post-processing searches $(\tau_0,\tau_1)$ such that fairness gap is minimized with bounded accuracy loss, i.e., an explicit fairness-utility tradeoff optimization.

\section{Assumptions and Reproducibility Guarantees}
\begin{itemize}
    \item Real security checkpoint is selected from \texttt{poisened\_models.rar} using student-ID suffix (ID \texttt{810101504} $\rightarrow$ model 4).
    \item Security profile is high-fidelity (500 optimization steps per target label).
    \item Unlearning applies trigger to 20\% of data for one epoch with true labels unchanged.
    \item Privacy constants are fixed to assignment values, with $p=0.01$ for unbounded DP.
    \item Fairness split is 70/30 with \texttt{random\_state=0} and deterministic seed control.
    \item All figures/tables come from \texttt{code/generate\_report\_figs.py}; no manual metric editing is used.
\end{itemize}

\subsection{Theory Robustness Guardrails}
To keep theoretical quality stable across reruns, the report uses three guardrails: (i) equation-level definitions are encoded in this template and not injected dynamically, (ii) all numeric claims are populated only through generated macros from executable code, and (iii) the code/test pipeline enforces deterministic settings (fixed seeds, locked assumptions, and explicit scenario constants). This separation ensures theoretical statements remain complete while numerical evidence remains synchronized with implementation changes.

\section{Complete Code Walkthrough}
\subsection{Security Pipeline (\texttt{code/neural\_cleanse.py})}
The security module is structured as a robust production-style pipeline. \texttt{AttackedMNISTCNN} matches the exact checkpoint architecture, allowing strict state-dict verification in \texttt{load\_model}. Archive extraction and checkpoint resolution are automated by \texttt{extract\_poisoned\_models\_if\_needed} and \texttt{resolve\_checkpoint\_path}, preventing manual mismatch errors. \texttt{load\_mnist\_test} handles deterministic subsampling and explicit offline failure messages. \texttt{reconstruct\_trigger} optimizes mask and pattern logits, maps them to valid ranges via sigmoid, and minimizes class-induction plus regularization terms. \texttt{reconstruct\_all\_labels} executes this optimization for labels 0--9 and returns structured results. \texttt{detect\_outlier\_scales} applies lower-tail MAD to identify the attacked label. \texttt{evaluate\_clean\_accuracy} and \texttt{evaluate\_asr} separate clean and triggered behaviors. Finally, \texttt{unlearn\_by\_retraining} performs assignment-constrained cleansing with controlled trigger exposure.

\subsection{Privacy Pipeline (\texttt{code/privacy.py})}
The privacy module separates primitive mechanisms from assignment scenarios. Primitive functions implement Laplace scale, perturbation, threshold probability, and epsilon composition. \texttt{income\_query\_results} computes Q2-Part1 values deterministically and includes split-budget effects. \texttt{counting\_query\_results} computes Q2-Part2 base, sequential, and unbounded cases with locked assumptions; it returns all intermediate quantities (e.g., $\epsilon_i$, $\delta_i$, $\Delta f_{\mathrm{unbounded}}$) to ensure transparent traceability from theory to final probabilities.

\subsection{Fairness Pipeline (\texttt{code/fairness.py})}
The fairness module provides a unified evaluation surface across baseline, assignment mitigation, and bonus methods. \texttt{train\_baseline\_model} standardizes features and preserves scaler state for reproducible inference. \texttt{apply\_promotion\_demotion} now uses prediction-based cohorts, consistent with assignment-style procedural fairness correction. \texttt{retrain\_with\_swapped\_labels} realizes the new training targets. Bonus method 1 (\texttt{train\_reweighed\_model}) injects sample-importance correction from group-label marginals. Bonus method 2 (\texttt{optimize\_group\_thresholds} + \texttt{apply\_group\_thresholds}) performs post-hoc parity adjustment at decision time. \texttt{compute\_fairness\_metrics} enforces a consistent metric schema for comparison tables and plots.

\subsection{Artifact Orchestration (\texttt{code/generate\_report\_figs.py})}
The orchestrator converts all analysis into one CLI-driven run. It parses experiment controls, initializes deterministic paths/seeds, and executes security/privacy/fairness runners. The script now includes additional educational artifacts: security scale profile, before/after confusion matrices, privacy tail-probability curves, and fairness decomposition/tradeoff plots. Outputs are persisted as: figures under \texttt{report/figures}, structured metrics in \texttt{report/results/metrics\_summary.json}, and LaTeX macros in \texttt{report/results/results\_macros.tex}. This design guarantees report freshness and removes manual transcription risk.

\section{Results and Full Plot Interpretation}

\subsection{Reconstructed Trigger for Detected Label}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/trigger_reconstructed.png}{\includegraphics[width=0.42\linewidth]{figures/trigger_reconstructed.png}}{\fbox{\parbox[c][3.0cm][c]{0.42\linewidth}{\centering Missing: trigger\_reconstructed.png}}}
    \caption{Reconstructed trigger mask for detected attacked label.}
    \label{fig:trigger_reconstructed}
\end{figure}
Figure~\ref{fig:trigger_reconstructed} shows the recovered sparse mask for the detected attacked label. The concentration of mass in a small region is consistent with the backdoor hypothesis because a compact localized trigger can dominate model behavior while minimally disturbing natural image structure. The detected label is \QOneDetectedLabel{} and expected checkpoint label is \QOneExpectedLabel{}, and their agreement indicates that the optimization objective plus lower-tail MAD criterion successfully recovered the latent attack target rather than an arbitrary optimization artifact.

\subsection{All-Label Scale Profile and Grid}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/trigger_all_labels_grid.png}{\includegraphics[width=0.95\linewidth]{figures/trigger_all_labels_grid.png}}{\fbox{\parbox[c][3.0cm][c]{0.95\linewidth}{\centering Missing: trigger\_all\_labels\_grid.png}}}
    \caption{Reconstructed masks/scales for all candidate labels.}
    \label{fig:trigger_all_labels}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_scale_profile.png}{\includegraphics[width=0.88\linewidth]{figures/security_scale_profile.png}}{\fbox{\parbox[c][3.0cm][c]{0.88\linewidth}{\centering Missing: security\_scale\_profile.png}}}
    \caption{Trigger-scale profile with detected/expected labels highlighted.}
    \label{fig:security_scale_profile}
\end{figure}
Figures~\ref{fig:trigger_all_labels} and~\ref{fig:security_scale_profile} jointly provide the key detection evidence: the attacked class appears as the most anomalously small trigger scale among all labels, while non-attacked labels require larger masks to force class-specific behavior. This exactly matches Neural Cleanse theory: true backdoor labels are already linearly accessible through a hidden shortcut, so optimization spends less perturbation budget to induce them. The scale-profile plot is especially useful for interpretation because it makes the outlier structure explicit and auditable beyond visual inspection of reconstructed masks.

\subsection{Mitigation Outcomes: Accuracy, ASR, and Confusion Structure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_before_after.png}{\includegraphics[width=0.74\linewidth]{figures/security_before_after.png}}{\fbox{\parbox[c][3.0cm][c]{0.74\linewidth}{\centering Missing: security\_before\_after.png}}}
    \caption{Clean accuracy and ASR before/after one-epoch unlearning.}
    \label{fig:security_before_after}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_confusion_before_after.png}{\includegraphics[width=0.93\linewidth]{figures/security_confusion_before_after.png}}{\fbox{\parbox[c][3.0cm][c]{0.93\linewidth}{\centering Missing: security\_confusion\_before\_after.png}}}
    \caption{Row-normalized clean confusion matrices before and after unlearning.}
    \label{fig:security_confusion}
\end{figure}
Figure~\ref{fig:security_before_after} shows a strong post-unlearning ASR reduction from \QOneAsrBefore{} to \QOneAsrAfter{} while clean accuracy improves from \QOneCleanAccBefore{} to \QOneCleanAccAfter{}, indicating that the poisoned model was initially dominated by trigger-induced behavior and that retraining with correct labels successfully restored generalization. Figure~\ref{fig:security_confusion} complements this by showing class-wise behavior on clean inputs: diagonal strengthening after unlearning means the mitigation did not merely suppress one attack pathway, but improved overall decision calibration. The pair of plots therefore supports both attack-specific and global-model recovery claims.

\subsection{Security Ablation: Unlearning Fraction Sweep}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/security_unlearning_sweep.png}{\includegraphics[width=0.86\linewidth]{figures/security_unlearning_sweep.png}}{\fbox{\parbox[c][3.0cm][c]{0.86\linewidth}{\centering Missing: security\_unlearning\_sweep.png}}}
    \caption{Clean accuracy and ASR versus unlearning fraction.}
    \label{fig:security_unlearning_sweep}
\end{figure}
Figure~\ref{fig:security_unlearning_sweep} quantifies sensitivity of mitigation strength to the retraining exposure ratio. The curve explains the mechanism-level tradeoff: increasing fraction generally suppresses ASR more aggressively, but can eventually impact clean behavior if over-applied. In this run, the best ASR point occurs around fraction \QOneBestFrac{} with ASR \QOneBestFracASR{} and clean accuracy \QOneBestFracAcc{}, providing an interpretable operating point rather than a single hard-coded choice.

\subsection{Privacy Scales, Point Probabilities, and Tail Curves}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/privacy_scenarios.png}{\includegraphics[width=0.88\linewidth]{figures/privacy_scenarios.png}}{\fbox{\parbox[c][3.0cm][c]{0.88\linewidth}{\centering Missing: privacy\_scenarios.png}}}
    \caption{Laplace scale and exceedance probability at threshold 505.}
    \label{fig:privacy_scenarios}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/privacy_tail_curves.png}{\includegraphics[width=0.86\linewidth]{figures/privacy_tail_curves.png}}{\fbox{\parbox[c][3.0cm][c]{0.86\linewidth}{\centering Missing: privacy\_tail\_curves.png}}}
    \caption{Tail probability $P(\tilde{q}>t)$ versus threshold for all privacy scenarios.}
    \label{fig:privacy_tail}
\end{figure}
Figure~\ref{fig:privacy_scenarios} summarizes the assignment query at $t=505$: scale grows from \QTwoBBase{} (base) to \QTwoBSequential{} (sequential) and \QTwoBUnbounded{} (unbounded), with corresponding probabilities \QTwoProbBase{}, \QTwoProbSequential{}, and \QTwoProbUnbounded{}. Figure~\ref{fig:privacy_tail} generalizes this point analysis by showing entire tail functions over thresholds, making the utility-loss mechanism explicit: larger scales flatten the response curve and keep probabilities closer to 0.5 over wider threshold bands. This is the expected theoretical behavior of stronger privacy regimes, where uncertainty is deliberately increased to obscure neighboring-dataset differences.

\subsection{Privacy Budget Sweep (Epsilon Analysis)}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/privacy_epsilon_sweep.png}{\includegraphics[width=0.88\linewidth]{figures/privacy_epsilon_sweep.png}}{\fbox{\parbox[c][3.0cm][c]{0.88\linewidth}{\centering Missing: privacy\_epsilon\_sweep.png}}}
    \caption{Scale and threshold probability as functions of epsilon.}
    \label{fig:privacy_epsilon_sweep}
\end{figure}
Figure~\ref{fig:privacy_epsilon_sweep} provides a direct parametric interpretation of privacy budget: as epsilon increases, scale $b$ decays hyperbolically and the noisy-threshold probability moves away from the high-uncertainty regime toward sharper query behavior. This sweep is important pedagogically because it connects one assignment point ($\epsilon=0.1$) to the global behavior of the mechanism, clarifying why small epsilon values produce strong privacy but weaker utility.

\subsection{Fairness: Aggregate Metrics, Group Decomposition, and Tradeoff Geometry}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_comparison.png}{\includegraphics[width=0.98\linewidth]{figures/fairness_comparison.png}}{\fbox{\parbox[c][3.0cm][c]{0.98\linewidth}{\centering Missing: fairness\_comparison.png}}}
    \caption{Accuracy, DI, and Zemel-proxy across five model variants.}
    \label{fig:fairness_comparison}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_group_rates.png}{\includegraphics[width=0.92\linewidth]{figures/fairness_group_rates.png}}{\fbox{\parbox[c][3.0cm][c]{0.92\linewidth}{\centering Missing: fairness\_group\_rates.png}}}
    \caption{Group positive prediction rates (male/female) for DI interpretation.}
    \label{fig:fairness_group_rates}
\end{figure}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_tradeoff.png}{\includegraphics[width=0.66\linewidth]{figures/fairness_tradeoff.png}}{\fbox{\parbox[c][3.0cm][c]{0.66\linewidth}{\centering Missing: fairness\_tradeoff.png}}}
    \caption{Accuracy versus fairness-gap map ($|1-\mathrm{DI}|$).}
    \label{fig:fairness_tradeoff}
\end{figure}
Figure~\ref{fig:fairness_comparison} provides aggregate comparison, but Figures~\ref{fig:fairness_group_rates} and~\ref{fig:fairness_tradeoff} explain why these aggregates change: group-rate decomposition shows whether DI movement is caused by increasing protected-group positives, decreasing privileged-group positives, or both; the tradeoff map then visualizes each model's position in fairness-utility space. Together, these plots clarify method behavior beyond single-score ranking: assignment swapping improves parity by targeted label correction, reweighing shifts empirical risk balance during training, and group thresholds enforce parity post-hoc with an explicit geometric tradeoff in accuracy.

\subsection{Fairness Ablation: Swap-Budget Sweep}
\begin{figure}[H]
    \centering
    \IfFileExists{figures/fairness_swapk_sweep.png}{\includegraphics[width=0.86\linewidth]{figures/fairness_swapk_sweep.png}}{\fbox{\parbox[c][3.0cm][c]{0.86\linewidth}{\centering Missing: fairness\_swapk\_sweep.png}}}
    \caption{Accuracy and fairness-gap trends versus promotion/demotion swap budget $k$.}
    \label{fig:fairness_swapk_sweep}
\end{figure}
Figure~\ref{fig:fairness_swapk_sweep} shows how the assignment mitigation behaves as $k$ changes from no swapping to aggressive relabeling. The curve demonstrates that fairness gains (lower $|1-\mathrm{DI}|$) are not monotonic in practical utility terms unless accuracy is co-monitored, so selecting $k$ is an optimization problem, not a fixed rule. Using a tolerance of at most 3\% absolute accuracy loss from baseline, the best operating point in this run is $k=\\QThreeBestSwapK$ with accuracy \QThreeBestSwapAcc{} and fairness gap \QThreeBestSwapDiGap{}.

\section{Consolidated Metric Tables}
\begin{table}[H]
\centering
\caption{Final fairness metrics used in this report}
\label{tab:final_metrics}
\begin{tabular}{lccc}
\toprule
Model/Scenario & Accuracy & DI & Zemel-proxy \\
\midrule
Fairness baseline & \QThreeBaseAcc{} & \QThreeBaseDi{} & \QThreeBaseZemel{} \\
Promotion/Demotion & \QThreeSwapAcc{} & \QThreeSwapDi{} & \QThreeSwapZemel{} \\
No-gender features & \QThreeNoGenderAcc{} & \QThreeNoGenderDi{} & \QThreeNoGenderZemel{} \\
Reweighed (bonus) & \QThreeReweighedAcc{} & \QThreeReweighedDi{} & \QThreeReweighedZemel{} \\
Group-thresholds (bonus) & \QThreeGroupThrAcc{} & \QThreeGroupThrDi{} & \QThreeGroupThrZemel{} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Security and privacy summary}
\label{tab:security_privacy}
\begin{tabular}{lc}
\toprule
Quantity & Value \\
\midrule
Detected attacked label & \QOneDetectedLabel{} \\
Expected checkpoint label & \QOneExpectedLabel{} \\
Clean accuracy before/after & \QOneCleanAccBefore{} / \QOneCleanAccAfter{} \\
ASR before/after & \QOneAsrBefore{} / \QOneAsrAfter{} \\
$b$ (base / sequential / unbounded) & \QTwoBBase{} / \QTwoBSequential{} / \QTwoBUnbounded{} \\
$P(\tilde{q} > 505)$ (base / sequential / unbounded) & \QTwoProbBase{} / \QTwoProbSequential{} / \QTwoProbUnbounded{} \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
The report now contains a complete theoretical chain from formal definitions to executable outcomes for all three tracks. Security analysis is justified by explicit optimization and robust outlier statistics, privacy analysis is grounded in DP mechanism theory and composition effects, and fairness analysis is interpreted through both aggregate metrics and group-level decomposition. Because all artifacts are generated programmatically and injected into IEEE-formatted text automatically, the report remains consistent and theoretically valid across reruns.

\begin{thebibliography}{1}
\bibitem{wang2019neuralcleanse}
B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao,
``Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,''
in \emph{Proc. IEEE Symp. Security and Privacy}, 2019.

\bibitem{dwork2014algorithmic}
C. Dwork and A. Roth, \emph{The Algorithmic Foundations of Differential Privacy}.
Now Publishers, 2014.

\bibitem{zemel2013learning}
R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork,
``Learning Fair Representations,'' in \emph{Proc. ICML}, 2013.
\end{thebibliography}

\end{document}
