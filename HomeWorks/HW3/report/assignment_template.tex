\documentclass[10pt,journal]{IEEEtran}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{array}
\usepackage{url}
\usepackage{cite}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}

\definecolor{codebg}{RGB}{246,246,246}
\lstdefinestyle{plainstyle}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{codebg},
  frame=single,
  breaklines=true,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny
}
\lstset{style=plainstyle}

\title{Complete Causal Recourse Implementation on Health Data\\
(IEEE-Style Report for Trusted AI HW3, Question 5)}

\author{Taha~Majlesi,
        Student~ID~810101504\\
        Department of Electrical and Computer Engineering, University of Tehran}

\begin{document}
\maketitle

\begin{abstract}
This report presents a fully completed implementation and analysis of the causal recourse pipeline for Homework~3 Question~5 on the health dataset. The work includes completion of data actionability constraints, classifier training, structural causal model implementation, Jacobian derivation, robust recourse evaluation, and direct comparison between Nearest Counterfactual Explanation and Causal Algorithmic Recourse. The report is written in IEEE format and provides both empirical and theoretical interpretation. We evaluate linear and neural classifiers, report validity--cost tradeoffs across robustness radii, and show that causally informed interventions can reduce required intervention cost under matched conditions. All experiments are reproducible with explicit commands and generated artifacts.
\end{abstract}

\begin{IEEEkeywords}
Causal inference, structural causal model, algorithmic recourse, counterfactual explanation, robustness, trustworthy AI.
\end{IEEEkeywords}

\section{Introduction}
Algorithmic recourse asks: given an unfavorable model decision, what minimal actionable change should be recommended so the decision flips? In high-stakes settings, recourse quality is not only about decision flip rate but also about intervention realism and cost. If feature dependencies are ignored, recommended actions can be unrealistic or unnecessarily expensive. This is why causal recourse, which explicitly models how interventions propagate through a structural causal model (SCM), is central to trustworthy decision support.

This report focuses on complete implementation and verification of Question~5 in HW3. The practical objective is to classify healthy vs unhealthy individuals and generate efficient interventions that transform unhealthy predictions into healthy ones. Beyond a simple pipeline run, this submission completes missing SCM components, evaluates robustness across uncertainty radii, and explains each generated plot in a dedicated, theory-grounded paragraph.

\section{Theoretical Background}
\subsection{Counterfactual and Causal Recourse}
For a binary classifier with score function $g_\theta(x)$ and threshold $\tau$, prediction is
\begin{equation}
\hat{y}=\mathbb{I}[\sigma(g_\theta(x)) \ge \tau].
\end{equation}
Nearest counterfactual recourse typically solves a constrained optimization that minimizes intervention magnitude while satisfying the decision constraint. In the linear case, this corresponds to an L1-minimization under feasibility constraints \cite{ustun2019actionable}. Causal recourse extends this by evaluating intervention effects through an SCM, using abduction-action-prediction logic \cite{pearl2009causality,karimi2021algorithmic}.

\subsection{Robust Linear Recourse Geometry}
Under uncertainty radius $\epsilon$, robust linear recourse shifts the effective decision boundary by a dual-norm margin term. If $w$ is the classifier normal and $J$ is the intervention Jacobian under SCM, robust feasibility depends on
\begin{equation}
\langle w, x+J a \rangle \ge b + \|J^\top w\|_2\,\epsilon.
\end{equation}
As $\epsilon$ increases, feasible interventions generally require larger norm. Therefore, monotonic recourse cost increase with $\epsilon$ is theoretically expected for fixed actionability and model class.

\subsection{Differentiable Recourse for Nonlinear Models}
For MLP classifiers, recourse is obtained via iterative optimization over intervention variables. The objective combines classification loss toward favorable outcome and intervention sparsity/magnitude penalties. Because this is non-convex, validity and cost can be sensitive to initialization, learning rate, and regularization schedule \cite{mothilal2020explaining,karimi2020algorithmic}. This theoretical sensitivity motivates reporting both validity and cost, not just one metric.

\section{Implementation Completion for Q5}
\subsection{Q5.1 Data Processing and Actionability}
In \texttt{code/Q5\_codes/data\_utils.py}, health preprocessing is configured so only \texttt{insulin} and \texttt{blood\_glucose} are actionable. Feature bounds are enforced using observed dataset limits, preventing interventions from leaving realistic ranges. Non-actionable features \texttt{age} and \texttt{blood\_pressure} remain fixed under direct intervention.

\subsection{Q5.2 Running on 10 Unhealthy Individuals}
The evaluation pipeline is executed with $N_{\text{explain}}=10$, sampling negatively classified test instances and computing valid recourse/cost arrays. For linear ERM with SCM enabled, seed-0 cost at $\epsilon=0$ is approximately 0.909, and the multi-seed mean is 0.889.

\subsection{Q5.3 and Q5.4 Completing \texttt{Health\_SCM} and Jacobian}
The \texttt{Health\_SCM} class was completed with structural equations $f$, inverse equations \texttt{inv\_f}, actionability mask, and linear coefficients:
\begin{align}
X_1 &= U_1, \\
X_2 &= \tfrac{1}{18}X_1 + U_2, \\
X_3 &= 2.0 X_1 + 1.05 X_2 + U_3, \\
X_4 &= 0.4 X_2 + 0.3 X_3 + U_4.
\end{align}
The corresponding Jacobian is implemented in \texttt{get\_Jacobian} and used by linear causal recourse.

\subsection{Q5.5 and Q5.6 SCM-On Rerun and Method Comparison}
With SCM enabled, the pipeline computes causal recourse recommendations and saves validity/cost arrays. Matched comparison between SCM-off (Nearest Counterfactual) and SCM-on (Causal Recourse) is generated by \texttt{generate\_report\_artifacts.py}, yielding a direct numerical comparison under identical seed/model/sample settings.

\section{Experimental Protocol}
\subsection{Environment and Reproducibility}
All runs use:
\begin{itemize}
  \item Python environment: \texttt{/Users/tahamajs/Documents/uni/venv/bin/activate}
  \item Code root: \texttt{HomeWorks/HW3/code/Q5\_codes}
  \item Report root: \texttt{HomeWorks/HW3/report}
\end{itemize}

\subsection{Evaluated Configurations}
\begin{table}[H]
\centering
\caption{Model and recourse settings used in this report}
\begin{tabular}{lccc}
\toprule
Configuration & Seeds & $\epsilon$ set & $N_{\text{explain}}$ \\
\midrule
lin-ERM & 0,1,2 & \{0.0, 0.1, 0.2\} & 10 \\
lin-AF  & 0,1,2 & \{0.0, 0.1, 0.2\} & 10 \\
mlp-ERM & 0,1   & \{0.0, 0.1, 0.2\} & 10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Generated Analysis Artifacts}
The script \texttt{generate\_report\_artifacts.py} produces:
\begin{itemize}
  \item \texttt{results/health\_report\_summary.csv}
  \item \texttt{results/health\_report\_aggregate.csv}
  \item \texttt{results/nearest\_vs\_causal\_lin\_seed0.csv}
  \item Plot files under \texttt{report/figures/}
\end{itemize}

\section{Results and Complete Plot Explanations}
\subsection{Classifier Performance Summary}
\begin{table}[H]
\centering
\caption{Classifier quality (mean $\pm$ std across available seeds)}
\label{tab:clf}
\begin{tabular}{lcc}
\toprule
Configuration & Accuracy & MCC \\
\midrule
lin-ERM & $0.903 \pm 0.002$ & $0.803 \pm 0.004$ \\
lin-AF  & $0.903 \pm 0.002$ & $0.803 \pm 0.003$ \\
mlp-ERM & $1.000 \pm 0.000$ & $1.000 \pm 0.000$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/classifier_metrics.png}
\caption{Classifier metrics by model/trainer.}
\label{fig:clfmetrics}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:clfmetrics}:}
This plot shows that linear ERM and linear AF are nearly identical in predictive discrimination (accuracy and MCC around 0.903 and 0.803), while the MLP reaches perfect scores on the tested split. Theoretically, this indicates that AF masking does not hurt linear predictive utility for this dataset because non-actionable features do not provide dominant unique information beyond actionable correlates in the chosen split. The MLP result suggests high function capacity relative to data complexity; however, in recourse theory high predictive accuracy does not imply low recourse burden, since the optimization landscape for intervention may remain sharp or constraint-limited even with near-perfect classification.

\subsection{Validity--Cost Tradeoff Across Robustness Radius}
\begin{table}[H]
\centering
\caption{Recourse outcomes (mean across seeds)}
\label{tab:recourse}
\begin{tabular}{lccc}
\toprule
Configuration & $\epsilon$ & Valid rate & Mean valid cost \\
\midrule
lin-ERM & 0.0 & 1.00 & 0.889 \\
lin-ERM & 0.1 & 1.00 & 1.004 \\
lin-ERM & 0.2 & 1.00 & 1.120 \\
lin-AF  & 0.0 & 1.00 & 0.701 \\
lin-AF  & 0.1 & 1.00 & 0.823 \\
lin-AF  & 0.2 & 1.00 & 0.946 \\
mlp-ERM & 0.0 & 0.85 & 1.111 \\
mlp-ERM & 0.1 & 0.90 & 1.253 \\
mlp-ERM & 0.2 & 0.90 & 0.885 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/valid_rate_vs_epsilon.png}
\caption{Valid recourse rate vs robustness radius $\epsilon$.}
\label{fig:validrate}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:validrate}:}
The figure indicates that both linear settings preserve 100\% validity across all tested robustness radii, while MLP validity remains below 1.0 and varies with $\epsilon$. The linear stability is theoretically expected because robust linear recourse solves a convex feasibility problem with explicit Jacobian-adjusted boundary shift; as long as feasible action bounds remain wide enough, validity can stay saturated. In contrast, nonlinear recourse uses gradient optimization in a non-convex objective with finite iterations, so the algorithm may fail to find valid interventions for some points even when valid solutions exist, which explains sub-unity validity for MLP despite strong predictive accuracy.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/valid_cost_vs_epsilon.png}
\caption{Mean valid recourse cost vs robustness radius $\epsilon$.}
\label{fig:validcost}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:validcost}:}
For both linear models, intervention cost increases monotonically with $\epsilon$, matching robust recourse theory where larger uncertainty enlarges the required safety margin from the decision boundary. AF consistently has lower cost than ERM, indicating that constraining classifier dependence to actionable dimensions can align decision geometry with feasible intervention directions and reduce required action magnitude. MLP costs are higher and less monotonic because the reported quantity is conditional on successful recourse instances; when validity changes with $\epsilon$, the set of included points also changes, so conditional mean cost can move non-monotonically even if underlying optimization becomes harder.

\subsection{Instance-Level Cost Distribution}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/recourse_costs.png}
\caption{Per-instance recourse costs for explained unhealthy individuals.}
\label{fig:instancecost}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:instancecost}:}
This plot visualizes heterogeneity of intervention effort across individuals: some instances require very small perturbations while others require significantly larger actions. Theoretically, this heterogeneity arises from local geometry of the classifier boundary and individual position relative to actionable feasibility constraints. Points near the boundary and aligned with high-gain actionable directions need small interventions; points deeper in the unfavorable region, or constrained by directional/box bounds, require larger L1 actions. Therefore, average recourse cost should always be interpreted together with distributional spread, not as a single universal burden.

\subsection{Nearest Counterfactual vs Causal Recourse}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\columnwidth]{figures/nearest_vs_causal.png}
\caption{Matched comparison: Nearest Counterfactual (SCM off) vs Causal Recourse (SCM on).}
\label{fig:nvscausal}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:nvscausal}:}
Under matched seed/model/samples, both methods achieve full validity, but causal recourse yields lower mean intervention cost (0.625 vs 0.779). Theoretically, SCM-aware recourse can exploit downstream causal amplification: an intervention on one actionable parent can beneficially move child variables without paying direct action cost on all descendants. Nearest counterfactual methods, by ignoring structural propagation, often optimize in a feature space geometry that treats dependent variables as independent coordinates, which can overestimate required action. This result is consistent with causal recourse arguments that structural knowledge can improve intervention efficiency while retaining decision-flip reliability.

\section{Discussion and Theoretical Implications}
First, robust recourse is not a free lunch: increasing uncertainty tolerance raises intervention cost, especially in linear models where this effect is analytically transparent. Second, classifier architecture alone does not determine recourse practicality. Even with perfect classification, nonlinear recourse can remain optimization-sensitive under causal constraints. Third, actionability-aware training (AF) can reduce practical intervention burden without compromising classifier quality, suggesting a principled route to train recourse-friendly models.

From a causal perspective, this homework confirms a central principle: interventions should be evaluated in a structural model, not only in observational feature space. When feature dependencies are strong, SCM-enabled recommendations can be both more realistic and cheaper.

\section{Extended Theoretical Analysis}
\subsection{Linear Recourse Cost Lower Bound}
For a linear classifier with robust margin shift, any valid intervention must satisfy
\begin{equation}
\langle w, J a \rangle \ge \gamma(\epsilon) \triangleq b + \|J^\top w\|_2\epsilon - \langle w,x\rangle.
\end{equation}
By H\"older duality, a coarse lower bound on L1 action is
\begin{equation}
\|a\|_1 \ge \frac{\gamma(\epsilon)}{\|J^\top w\|_\infty},
\end{equation}
when $\gamma(\epsilon)>0$. This clarifies why increasing $\epsilon$ systematically increases minimal feasible action in linear settings and why slope depends on Jacobian-weight alignment.

\subsection{Why AF Can Reduce Cost Without Hurting Accuracy}
AF constrains model dependence to actionable coordinates. In geometric terms, decision normals are pushed toward directions where interventions are allowed, increasing effective directional derivative of decision score per unit actionable change. If predictive information in non-actionable variables is partially redundant with actionable ones, this rotation can reduce recourse distance while preserving classification quality, which matches the empirical parity of accuracy/MCC and lower AF costs.

\subsection{Causal Amplification Mechanism}
Let an intervention apply on variable set $S$. Under SCM, total feature change is not only direct action but also propagated downstream:
\begin{equation}
\Delta x_{\text{total}} = J_S a_S.
\end{equation}
When downstream links are favorable for class flip, one unit intervention can produce more than one unit aggregate effect on classifier score. Nearest counterfactual methods (without SCM) ignore this propagation term and may therefore over-spend intervention magnitude.

\subsection{Validity-Cost Frontier Interpretation}
Recourse quality can be viewed as a bi-objective frontier: maximize validity and minimize intervention burden. Linear models in this report sit near a high-validity region with predictable cost growth as robustness tightens. MLP settings display frontier instability due optimization non-convexity; therefore, robust deployment should report confidence intervals, not single-point estimates, and include optimization diagnostics.

\section{Conclusion}
This report completes HW3 Question~5 end-to-end in IEEE format with explicit theoretical and empirical analysis. The software pipeline is fully runnable, missing SCM components are completed, robust evaluations are produced, and each plot is interpreted in a dedicated theory-grounded paragraph. Empirically, linear recourse is highly stable on this dataset, AF reduces intervention cost, and causal recourse outperforms nearest counterfactual in matched cost comparison while maintaining full validity.

\appendices
\section{Reproducibility Commands}
\begin{lstlisting}[caption={Exact commands used for the final report build}]
cd /Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3/code/Q5_codes
source /Users/tahamajs/Documents/uni/venv/bin/activate

python main.py --seed 0
python generate_report_artifacts.py

cd /Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3/report
make pdf
\end{lstlisting}

\section{Auto-Generated Aggregate CSV}
\lstinputlisting[caption={Health report aggregate CSV}]{../code/Q5_codes/results/health_report_aggregate.csv}

\section{Auto-Generated Per-Run CSV}
\lstinputlisting[caption={Health report per-run summary CSV}]{../code/Q5_codes/results/health_report_summary.csv}

\section*{Acknowledgment}
This submission was prepared for Trusted Artificial Intelligence coursework under Dr. Mostafa Tavasolipour.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
