\documentclass[10pt,journal]{IEEEtran}

\usepackage{amsmath,amssymb,mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{array}
\usepackage{url}
\usepackage{cite}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codebg}{RGB}{246,246,246}
\lstdefinestyle{plainstyle}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{codebg},
  frame=single,
  breaklines=true,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny
}
\lstset{style=plainstyle}

\title{Complete Causal Recourse Implementation on Health Data\\
(IEEE-Style Report for Trusted AI HW3, Question 5)}

\author{Taha~Majlesi,
        Student~ID~810101504\\
        Department of Electrical and Computer Engineering, University of Tehran}

\begin{document}
\maketitle

\begin{abstract}
This report presents a fully completed implementation and analysis of the causal recourse pipeline for Homework~3 Question~5 on the health dataset. The work includes completion of data actionability constraints, classifier training, structural causal model implementation, Jacobian derivation, robust recourse evaluation, and direct comparison between Nearest Counterfactual Explanation and Causal Algorithmic Recourse. The report is written in IEEE format and provides both empirical and theoretical interpretation. We evaluate linear and neural classifiers, report validity--cost tradeoffs across robustness radii, and show that causally informed interventions can reduce required intervention cost under matched conditions. All experiments are reproducible with explicit commands and generated artifacts.
\end{abstract}

\begin{IEEEkeywords}
Causal inference, structural causal model, algorithmic recourse, counterfactual explanation, robustness, trustworthy AI.
\end{IEEEkeywords}

\section{Introduction}
Algorithmic recourse asks: given an unfavorable model decision, what minimal actionable change should be recommended so the decision flips? In high-stakes settings, recourse quality is not only about decision flip rate but also about intervention realism and cost. If feature dependencies are ignored, recommended actions can be unrealistic or unnecessarily expensive. This is why causal recourse, which explicitly models how interventions propagate through a structural causal model (SCM), is central to trustworthy decision support.

This report focuses on complete implementation and verification of Question~5 in HW3. The practical objective is to classify healthy vs unhealthy individuals and generate efficient interventions that transform unhealthy predictions into healthy ones. Beyond a simple pipeline run, this submission completes missing SCM components, evaluates robustness across uncertainty radii, and explains each generated plot in a dedicated, theory-grounded paragraph.

\section{Theoretical Background}
\subsection{Counterfactual and Causal Recourse}
For a binary classifier with score function $g_\theta(x)$ and threshold $\tau$, prediction is
\begin{equation}
\hat{y}=\mathbb{I}[\sigma(g_\theta(x)) \ge \tau].
\end{equation}
Nearest counterfactual recourse typically solves a constrained optimization that minimizes intervention magnitude while satisfying the decision constraint. In the linear case, this corresponds to an L1-minimization under feasibility constraints \cite{ustun2019actionable}. Causal recourse extends this by evaluating intervention effects through an SCM, using abduction-action-prediction logic \cite{pearl2009causality,karimi2021algorithmic}.

\subsection{Robust Linear Recourse Geometry}
Under uncertainty radius $\epsilon$, robust linear recourse shifts the effective decision boundary by a dual-norm margin term. If $w$ is the classifier normal and $J$ is the intervention Jacobian under SCM, robust feasibility depends on
\begin{equation}
\langle w, x+J a \rangle \ge b + \|J^\top w\|_2\,\epsilon.
\end{equation}
As $\epsilon$ increases, feasible interventions generally require larger norm. Therefore, monotonic recourse cost increase with $\epsilon$ is theoretically expected for fixed actionability and model class.

\subsection{Differentiable Recourse for Nonlinear Models}
For MLP classifiers, recourse is obtained via iterative optimization over intervention variables. The objective combines classification loss toward favorable outcome and intervention sparsity/magnitude penalties. Because this is non-convex, validity and cost can be sensitive to initialization, learning rate, and regularization schedule \cite{mothilal2020explaining,karimi2020algorithmic}. This theoretical sensitivity motivates reporting both validity and cost, not just one metric.

\section{Implementation Completion for Q5}
\subsection{Q5.1 Data Processing and Actionability}
In \texttt{code/q5\_codes/data\_utils.py}, health preprocessing is configured so only \texttt{insulin} and \texttt{blood\_glucose} are actionable. Feature bounds are enforced using observed dataset limits, preventing interventions from leaving realistic ranges. Non-actionable features \texttt{age} and \texttt{blood\_pressure} remain fixed under direct intervention.

\subsection{Q5.2 Running on 10 Unhealthy Individuals}
The evaluation pipeline is executed with $N_{\text{explain}}=10$, sampling negatively classified test instances and computing valid recourse/cost arrays. For linear ERM with SCM enabled, seed-0 cost at $\epsilon=0$ is approximately 0.909, and the multi-seed mean is 0.889.

\subsection{Q5.3 and Q5.4 Completing \texttt{Health\_SCM} and Jacobian}
The \texttt{Health\_SCM} class was completed with structural equations $f$, inverse equations \texttt{inv\_f}, actionability mask, and linear coefficients:
\begin{align}
X_1 &= U_1, \\
X_2 &= \tfrac{1}{18}X_1 + U_2, \\
X_3 &= 2.0 X_1 + 1.05 X_2 + U_3, \\
X_4 &= 0.4 X_2 + 0.3 X_3 + U_4.
\end{align}
The corresponding Jacobian is implemented in \texttt{get\_Jacobian} and used by linear causal recourse.

\subsection{Q5.5 and Q5.6 SCM-On Rerun and Method Comparison}
With SCM enabled, the pipeline computes causal recourse recommendations and saves validity/cost arrays. Matched comparison between SCM-off (Nearest Counterfactual) and SCM-on (Causal Recourse) is generated by \texttt{generate\_report\_artifacts.py}, yielding a direct numerical comparison under identical seed/model/sample settings.

\section{Complete Code Walkthrough}
\subsection{End-to-End Control Flow}
The executable entry point is \texttt{code/q5\_codes/main.py}. It parses \texttt{--seed} and then calls \texttt{run\_benchmark(models, datasets, seed, N\_explain)} in \texttt{runner.py}. Inside \texttt{run\_benchmark}, the pipeline is sequenced as: (i) create output directories, (ii) optionally fit data-driven SCMs for datasets that require them, (iii) train classifiers if their \texttt{.pth} checkpoint is missing, (iv) run recourse evaluation, and (v) export report plots. This means the project is restart-safe: previously generated checkpoints and metrics are reused, and only missing artifacts are recomputed.

\subsection{Data Layer (\texttt{data\_utils.py})}
The data layer exposes two core APIs: \texttt{process\_data(dataset)} and \texttt{train\_test\_split(X, Y)}. The dispatcher \texttt{process\_data} routes to dataset-specific preprocessors. For HW3-Q5, \texttt{process\_health\_data()} loads \texttt{health.csv}, extracts the four modeled variables (\texttt{age}, \texttt{insulin}, \texttt{blood\_glucose}, \texttt{blood\_pressure}), standardizes them using \texttt{StandardScaler}, and returns a constraints dictionary with actionable indices, monotonic direction constraints, and per-feature intervention limits in standardized space. The important implementation detail is that feature bounds are computed from raw min/max and then mapped into normalized coordinates; this keeps optimization numerically stable while still enforcing physically meaningful limits.

\subsection{Model Layer (\texttt{trainers.py} and \texttt{train\_classifiers.py})}
Model construction and optimization are separated. \texttt{train\_classifiers.py} chooses model type (\texttt{LogisticRegression} or \texttt{MLP}), selects trainer class (ERM/AF/ALLR/ROSS), sets seeds, splits data, and launches training. In \texttt{trainers.py}, class \texttt{Classifier} provides threshold-aware inference (\texttt{probs}, \texttt{predict}) and \texttt{set\_max\_mcc\_threshold}, which calibrates decision threshold by maximizing MCC over a grid. \texttt{LogisticRegression.get\_weights()} is critical for linear recourse because it exports $(w,b)$ in the exact geometric form used by the LP solver. AF behavior is implemented by masking model inputs to actionable coordinates only; this is done in the shared \texttt{Classifier.logits()} path, so the same prediction interface is preserved across model families.

\subsection{SCM Layer (\texttt{scm.py})}
The SCM base class implements the full abduction-action-prediction mechanics. \texttt{Xn2X} and \texttt{X2Xn} convert between standardized and original feature scales; \texttt{X2U} infers exogenous noise terms; and \texttt{counterfactual()} applies interventions through structural equations with hard/soft intervention semantics. The completed \texttt{Health\_SCM} defines forward equations \texttt{self.f}, inverse equations \texttt{self.inv\_f}, actionable set \texttt{[1,2]}, and linear Jacobian routines (\texttt{get\_Jacobian}, \texttt{get\_Jacobian\_interv}). In particular, \texttt{get\_Jacobian\_interv} zeros incoming upstream effects for hard-intervened variables, which is the exact mechanism that distinguishes causal from non-causal recourse propagation in the implementation.

\subsection{Recourse Solver Layer (\texttt{recourse.py})}
This file contains both linear and nonlinear recourse engines. \texttt{build\_feasibility\_sets} converts actionability rules into per-instance box bounds over intervention vectors. \texttt{LinearRecourse.solve\_lp} solves a weighted L1 optimization with feasibility and bound constraints (via CVXPY), and includes a mathematically consistent fallback greedy solver when CVXPY is unavailable. \texttt{DifferentiableRecourse.find\_recourse} performs nested optimization: inner robust perturbation approximation (optional PGD refinement) and outer optimization of intervention vector $\delta$ under classification and sparsity penalties. Finally, \texttt{causal\_recourse} enumerates intervention subsets (power set of actionable features when SCM is enabled), solves recourse for each subset, and keeps the minimum-cost valid action per individual.

\subsection{Evaluation Layer (\texttt{evaluate\_recourse.py})}
Evaluation starts by loading the trained model and dataset split, setting the MCC-optimal threshold, and selecting negatively predicted test points to explain. The linear branch computes robust threshold shift using $\|J^\top w\|_2 \epsilon$, then runs LP-based recourse; the MLP branch uses differentiable recourse with hyperparameters from \texttt{utils.get\_recourse\_hyperparams}. Results are saved in a deterministic naming scheme (\texttt{\_ids.npy}, \texttt{\_valid.npy}, \texttt{\_cost.npy}) under \texttt{results/}, and summary statistics (validity rate, valid-only mean cost) are printed for immediate sanity checks.

\subsection{Reporting Layer (\texttt{generate\_report\_artifacts.py} and \texttt{plot\_report\_figures.py})}
The reporting code aggregates all saved runs into publication-ready artifacts. \texttt{generate\_report\_artifacts.py} parses model filenames, reloads models, recomputes classifier metrics consistently, merges them with recourse outputs for each $(\text{model},\text{trainer},\epsilon,\text{seed})$, writes machine-readable CSV summaries, and renders final figures used in the report. The same script also builds the matched Nearest-vs-Causal comparison by evaluating the exact same explained instances with \texttt{scm=None} and \texttt{scm=Health\_SCM}. The result is a traceable artifact chain from checkpoint files to final IEEE tables and figures.

\subsection{Utility and Naming Conventions (\texttt{utils.py})}
\texttt{utils.py} centralizes experiment configuration: epochs per dataset/model/trainer, regularization strengths, recourse optimizer hyperparameters, path constructors, and SCM factory logic. The path helper functions (\texttt{get\_model\_save\_dir}, \texttt{get\_metrics\_save\_dir}) enforce consistent file naming, which is what allows downstream report scripts to automatically discover runs and aggregate them without ad-hoc manual bookkeeping.

\subsection{Implementation Correctness Summary}
From a software engineering perspective, the code now forms a coherent layered system: preprocessing enforces intervention semantics, model training exports decision functions in solver-compatible form, SCM methods provide causally faithful counterfactual mapping, recourse solvers optimize under explicit feasibility sets, and report scripts reproducibly transform experiment outputs into submission artifacts. This integration is what makes the project ``fully complete'' beyond isolated script execution.

\section{Coverage of Original-Paper Requirements}
To ensure theoretical and methodological completeness, this report explicitly covers the core components required by the original recourse literature used in this homework context, including actionable recourse \cite{ustun2019actionable}, causal/interventional recourse \cite{karimi2021algorithmic,pearl2009causality}, and differentiable counterfactual-style optimization \cite{mothilal2020explaining}. Table~\ref{tab:coverage} maps each required component to implementation and report evidence.

\begin{table*}[!t]
\centering
\caption{Coverage matrix linking original-paper components to implementation and report evidence}
\label{tab:coverage}
\begin{tabular}{p{0.24\textwidth}p{0.24\textwidth}p{0.24\textwidth}p{0.24\textwidth}}
\toprule
Original-paper component & Theoretical object in this report & Implementation evidence in code & Evidence in generated report \\
\midrule
Binary thresholded classifier for decision flip & $h(x)=\mathbb{I}[\sigma(g_\theta(x))\ge\tau]$ and MCC-based threshold calibration & \texttt{trainers.Classifier}, \texttt{set\_max\_mcc\_threshold}, \texttt{predict} & Sec. II-A, classifier table/plot in Sec. V-A \\
Actionability-constrained interventions & Feasible action set with actionable indices, monotonic direction constraints, and per-feature bounds & \texttt{data\_utils.process\_health\_data}, \texttt{recourse.build\_feasibility\_sets} & Sec. III-A, diagnostics in Sec. V-E \\
Minimum-cost recourse optimization & Weighted L1 objective with validity constraints (linear LP) and differentiable objective (nonlinear) & \texttt{recourse.LinearRecourse.solve\_lp}, \texttt{DifferentiableRecourse.find\_recourse} & Sec. II, Sec. V-B/C, Appendix A/B/D \\
Robust recourse under uncertainty radius $\epsilon$ & Margin-shifted robust condition and validity-cost frontier analysis & \texttt{evaluate\_recourse.find\_recourse\_lin/mlp}, robust args in causal recourse call & Sec. II-B, Sec. V-B/E, Appendix A \\
Causal abduction-action-prediction mechanism & Counterfactual mapping $X\rightarrow U\rightarrow X^{cf}$ and Jacobian-based propagation & \texttt{scm.SCM.counterfactual}, \texttt{Health\_SCM}, \texttt{get\_Jacobian\_interv} & Sec. III-C, Sec. V-D, Appendix C \\
Intervention-set selection principle & Search over actionable intervention subsets; retain minimum-cost valid action & \texttt{recourse.causal\_recourse} powerset loop and best-cost update & Sec. IV/Evaluation + Sec. V explanations \\
Baseline comparison requirement & Nearest counterfactual (SCM off) versus causal recourse (SCM on) under matched setup & \texttt{generate\_report\_artifacts.nearest\_vs\_causal\_lin} & Fig. 5 and full paragraph in Sec. V-D \\
Reproducibility and artifact completeness & Run commands, aggregate/per-run/instance/action CSV traces, and fixed report figure pipeline & \texttt{generate\_report\_artifacts.py}, saved CSV/PNG artifacts & Sec. IV-C, appendices with listings and commands \\
\bottomrule
\end{tabular}
\end{table*}

\section{Experimental Protocol}
\subsection{Environment and Reproducibility}
All runs use:
\begin{itemize}
  \item Python environment: \texttt{/Users/tahamajs/Documents/uni/venv/bin/activate}
  \item Code root: \texttt{HomeWorks/HW3/code/q5\_codes}
  \item Report root: \texttt{HomeWorks/HW3/report}
\end{itemize}

\subsection{Evaluated Configurations}
\begin{table}[!t]
\centering
\caption{Model and recourse settings used in this report}
\begin{tabular}{lccc}
\toprule
Configuration & Seeds & $\epsilon$ set & $N_{\text{explain}}$ \\
\midrule
lin-ERM & 0,1,2 & \{0.0, 0.1, 0.2\} & 10 \\
lin-AF  & 0,1,2 & \{0.0, 0.1, 0.2\} & 10 \\
mlp-ERM & 0,1,2 & \{0.0, 0.1, 0.2\} & 10 \\
mlp-AF  & 0,1,2 & \{0.0, 0.1, 0.2\} & 10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Generated Analysis Artifacts}
The script \texttt{generate\_report\_artifacts.py} produces:
\begin{itemize}
  \item \texttt{results/health\_report\_summary.csv}
  \item \texttt{results/health\_report\_aggregate.csv}
  \item \texttt{results/nearest\_vs\_causal\_lin\_seed0.csv}
  \item \texttt{results/health\_instance\_costs.csv}
  \item \texttt{results/health\_action\_profiles.csv}
  \item \texttt{results/health\_action\_instance\_stats.csv}
  \item \texttt{results/health\_sparsity\_summary.csv}
  \item \texttt{results/health\_bootstrap\_summary.csv}
  \item Plot files under \texttt{report/figures/}
\end{itemize}

\section{Results and Complete Plot Explanations}
\subsection{Classifier Performance Summary}
\begin{table}[!t]
\centering
\caption{Classifier quality (mean $\pm$ std across available seeds)}
\label{tab:clf}
\begin{tabular}{lcc}
\toprule
Configuration & Accuracy & MCC \\
\midrule
lin-ERM & $0.900 \pm 0.001$ & $0.805 \pm 0.001$ \\
lin-AF  & $0.899 \pm 0.002$ & $0.798 \pm 0.005$ \\
mlp-ERM & $0.997 \pm 0.002$ & $0.995 \pm 0.003$ \\
mlp-AF  & $0.998 \pm 0.001$ & $0.995 \pm 0.002$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/classifier_metrics.png}
\caption{Classifier metrics by model/trainer.}
\label{fig:clfmetrics}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:clfmetrics}:}
This plot shows two clear regimes: linear models (ERM and AF) have similar predictive strength around 0.899--0.900 accuracy and 0.798--0.805 MCC, while MLP models (ERM and AF) are substantially higher near 0.997--0.998 accuracy and about 0.995 MCC. Theoretically, this supports the claim that actionability masking does not impose a major predictive penalty when actionable variables already capture most task-relevant signal. At the same time, the figure emphasizes a key recourse principle: predictive quality and intervention quality are different objectives. Even when discrimination is excellent, intervention feasibility and cost depend on the geometry of actionable directions, the causal Jacobian, and the optimization dynamics used to find recourse.

\subsection{Validity--Cost Tradeoff Across Robustness Radius}
\begin{table}[!t]
\centering
\caption{Recourse outcomes (mean across seeds)}
\label{tab:recourse}
\begin{tabular}{lccc}
\toprule
Configuration & $\epsilon$ & Valid rate & Mean valid cost \\
\midrule
lin-ERM & 0.0 & 1.000 & 0.889 \\
lin-ERM & 0.1 & 1.000 & 1.004 \\
lin-ERM & 0.2 & 1.000 & 1.120 \\
lin-AF  & 0.0 & 1.000 & 0.701 \\
lin-AF  & 0.1 & 1.000 & 0.823 \\
lin-AF  & 0.2 & 1.000 & 0.946 \\
mlp-ERM & 0.0 & 0.867 & 1.177 \\
mlp-ERM & 0.1 & 0.900 & 1.334 \\
mlp-ERM & 0.2 & 0.900 & 1.150 \\
mlp-AF  & 0.0 & 0.967 & 1.793 \\
mlp-AF  & 0.1 & 0.967 & 1.971 \\
mlp-AF  & 0.2 & 0.933 & 1.988 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/valid_rate_vs_epsilon.png}
\caption{Valid recourse rate vs robustness radius $\epsilon$.}
\label{fig:validrate}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:validrate}:}
The figure indicates perfect validity saturation for both linear settings at all tested radii, while nonlinear settings remain below 1.0 with model-dependent behavior (MLP-AF above MLP-ERM but not perfect). This pattern is theoretically consistent with convex versus non-convex recourse search: linear robust recourse has explicit Jacobian-shifted constraints and a stable feasible-set characterization, whereas MLP recourse is obtained by iterative gradient steps over a non-convex objective and can terminate in local basins or near-boundary states that do not cross the threshold. The higher MLP-AF validity here suggests that constraining classifier dependence to actionable coordinates can improve optimization alignment, yet finite-step optimization and heterogeneous instance geometry still prevent guaranteed validity.

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/valid_cost_vs_epsilon.png}
\caption{Mean valid recourse cost vs robustness radius $\epsilon$.}
\label{fig:validcost}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:validcost}:}
For both linear models, intervention cost increases nearly linearly with $\epsilon$, which directly matches robust optimization theory: larger uncertainty requires a larger worst-case margin, hence larger minimum L1 action. AF remains strictly cheaper than ERM in the linear case, supporting the geometric view that actionable masking can rotate effective decision sensitivity toward feasible intervention directions. In nonlinear settings, costs are markedly higher and more variable, and MLP-AF is especially expensive despite higher validity. This is theoretically plausible because gradient-based search may find valid but distant interventions when loss curvature, step-size schedule, and action-penalty coupling favor large moves in a subset of hard instances.

\subsection{Instance-Level Cost Distribution}
\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/recourse_costs.png}
\caption{Per-instance recourse costs for explained unhealthy individuals.}
\label{fig:instancecost}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:instancecost}:}
This plot visualizes heterogeneity of intervention effort across individuals: some instances require very small perturbations while others require significantly larger actions. Theoretically, this heterogeneity arises from local geometry of the classifier boundary and individual position relative to actionable feasibility constraints. Points near the boundary and aligned with high-gain actionable directions need small interventions; points deeper in the unfavorable region, or constrained by directional/box bounds, require larger L1 actions. Therefore, average recourse cost should always be interpreted together with distributional spread, not as a single universal burden.

\subsection{Nearest Counterfactual vs Causal Recourse}
\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/nearest_vs_causal.png}
\caption{Matched comparison: Nearest Counterfactual (SCM off) vs Causal Recourse (SCM on).}
\label{fig:nvscausal}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:nvscausal}:}
Under matched seed/model/samples, both methods achieve full validity, but causal recourse yields lower mean intervention cost (0.589 versus 0.733). Theoretically, SCM-aware optimization can leverage causal amplification: modifying an actionable parent induces beneficial downstream shifts through structural equations, increasing classifier score per unit direct intervention. In contrast, nearest counterfactual search without SCM treats correlated descendants as independent dimensions and may spend action budget redundantly. This cost gap therefore reflects an efficiency benefit from structural knowledge, not merely a random optimization artifact, and aligns with intervention-based recourse theory.

\subsection{Expanded Diagnostic Features for Complete Understanding}
\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/validity_cost_frontier.png}
\caption{Validity-cost frontier across model/trainer/epsilon settings.}
\label{fig:frontier}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:frontier}:}
This frontier plot makes explicit that recourse quality is a multi-objective operating point rather than a single score. Points near the top-left are preferable (high validity, low cost), while downward or rightward shifts indicate weaker practical recourse quality. The linear AF family sits on a favorable region with both perfect validity and lower cost than linear ERM, while nonlinear settings occupy higher-cost regions despite strong classifier accuracy. Theoretically, this figure is useful because it separates predictive performance from intervention burden and visualizes the Pareto-like tradeoff that must be reported for trustworthy deployment.

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/cost_distribution_boxplot.png}
\caption{Per-instance recourse cost distribution by configuration and epsilon.}
\label{fig:costbox}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:costbox}:}
Unlike mean-only summaries, this boxplot reveals distributional behavior and tail risk. Linear configurations show tighter spread and predictable median shifts with $\epsilon$, indicating stable geometry under robust margin increases. Nonlinear configurations exhibit wider dispersion and heavier upper tails, implying that a subset of individuals pays substantially larger intervention cost even when average validity is acceptable. This is theoretically important because fairness and usability concerns are often driven by high-cost tails, not by central tendency alone.

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/feature_mean_abs_action.png}
\caption{Feature-wise mean absolute intervention magnitude (valid recourse only).}
\label{fig:featabs}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:featabs}:}
This diagnostic quantifies where intervention budget is actually spent. Since only insulin and blood glucose are actionable, large action mass should concentrate on those coordinates while non-actionable dimensions remain near zero. The plotted pattern confirms this implementation behavior and also reveals model-dependent preference among actionable features, which reflects how each classifier's local gradient and SCM propagation jointly determine the most efficient direction. This offers direct interpretability: the recommended changes are not only valid but also aligned with declared actionability policy.

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/feature_nonzero_rate.png}
\caption{Feature intervention activation rate among valid recourse actions.}
\label{fig:featnz}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:featnz}:}
Activation rate measures how frequently each feature is used in successful interventions. A high nonzero rate for actionable variables and near-zero rate for non-actionable variables is the expected signature of a policy-consistent recourse system. This frequency view complements magnitude view in Fig.~\ref{fig:featabs}: a feature can have moderate average magnitude but very high activation frequency, indicating it is a reliable ``first-step'' recourse coordinate. Theoretical value comes from separating sparse-but-large actions from frequent-small actions, which correspond to different behavioral recourse strategies.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.92\textwidth]{figures/bootstrap_ci_curves.png}
\caption{Bootstrap confidence intervals (95\%) for validity and valid-cost trends across robustness radius.}
\label{fig:bootci}
\end{figure*}

\textit{Complete interpretation of Fig.~\ref{fig:bootci}:}
This diagnostic adds uncertainty quantification around the mean curves and shows that linear settings are not only high-performing in point estimates but also statistically stable under resampling, with narrow confidence bands for both validity and intervention cost; by contrast, nonlinear settings display wider cost intervals, which indicates sensitivity to sample composition and local optimization outcomes. Theoretically, this is the right reliability lens for deployment because recourse is a stochastic pipeline (instance sampling, initialization effects, solver dynamics), so reporting only means can overstate certainty. Confidence bands operationalize robustness claims by distinguishing true structural trends (persisting under bootstrap resampling) from fragile observations that may shift under slight data perturbations.

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/sparsity_vs_cost.png}
\caption{Sparsity-cost operating points based on valid recourse actions.}
\label{fig:sparsitycost}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:sparsitycost}:}
This plot characterizes how many coordinates are actively changed (L0 sparsity) versus how much total action mass is spent (L1 cost), making explicit that sparse interventions are not automatically cheap and dense interventions are not automatically expensive. In linear robust settings, points move with $\epsilon$ in a relatively smooth way, reflecting predictable margin-induced scaling; when points shift right and upward together, the solver is using both more features and larger amplitudes to maintain validity. Theoretically, this view connects optimization geometry to human burden: L0 approximates behavioral complexity (number of recommendations), while L1 approximates total effort. A trustworthy recourse system should therefore monitor both axes, because similar validity can mask very different user-facing intervention profiles.

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/action_heatmap_lin_erm.png}
\caption{Instance-level signed action heatmap for reference configuration (LIN-ERM, $\epsilon=0.1$, seed 0).}
\label{fig:heatmap}
\end{figure}

\textit{Complete interpretation of Fig.~\ref{fig:heatmap}:}
The heatmap reveals per-instance intervention structure rather than only aggregated averages: rows show individuals and columns show feature-wise signed actions, so one can directly see concentration patterns, sign consistency, and heterogeneity of local solutions. The dominant mass appears on actionable coordinates, while non-actionable coordinates remain near zero, confirming policy-consistent implementation at the individual level, not merely in global statistics. Theoretically, this figure is important because recourse validity is a boundary-crossing event that can be achieved through multiple local paths; visualizing signed action patterns helps detect whether the solver finds coherent directional strategies or unstable oscillatory behavior, and supports qualitative auditing of intervention realism in conjunction with quantitative cost metrics.

\section{Discussion and Theoretical Implications}
First, robust recourse is not a free lunch: increasing uncertainty tolerance raises intervention cost, especially in linear models where this effect is analytically transparent. Second, classifier architecture alone does not determine recourse practicality. The MLP results show that near-ceiling predictive metrics can coexist with high or unstable recourse costs. Third, actionability-aware training (AF) can reduce practical intervention burden in linear settings without sacrificing classifier quality, but this benefit is not guaranteed in nonlinear optimization regimes, where curvature and initialization effects can dominate.

From a causal perspective, this homework confirms a central principle: interventions should be evaluated in a structural model, not only in observational feature space. When feature dependencies are strong, SCM-enabled recommendations can be both more realistic and cheaper.

An additional implication is deployment robustness: operational recourse systems should report uncertainty bands over seeds, initialization, and optimization hyperparameters, especially for nonlinear recourse solvers. A single-point mean can hide heavy-tail intervention costs that are unacceptable in practice. Therefore, trustworthy deployment requires both average-case performance and tail-risk monitoring (e.g., quantiles of valid cost among successful recourse cases).

\section{Extended Theoretical Analysis}
\subsection{Linear Recourse Cost Lower Bound}
For a linear classifier with robust margin shift, any valid intervention must satisfy
\begin{equation}
\langle w, J a \rangle \ge \gamma(\epsilon) \triangleq b + \|J^\top w\|_2\epsilon - \langle w,x\rangle.
\end{equation}
By H\"older duality, a coarse lower bound on L1 action is
\begin{equation}
\|a\|_1 \ge \frac{\gamma(\epsilon)}{\|J^\top w\|_\infty},
\end{equation}
when $\gamma(\epsilon)>0$. This clarifies why increasing $\epsilon$ systematically increases minimal feasible action in linear settings and why slope depends on Jacobian-weight alignment.

\subsection{Why AF Can Reduce Cost Without Hurting Accuracy}
AF constrains model dependence to actionable coordinates. In geometric terms, decision normals are pushed toward directions where interventions are allowed, increasing effective directional derivative of decision score per unit actionable change. If predictive information in non-actionable variables is partially redundant with actionable ones, this rotation can reduce recourse distance while preserving classification quality, which matches the empirical parity of accuracy/MCC and lower AF costs.

\subsection{Causal Amplification Mechanism}
Let an intervention apply on variable set $S$. Under SCM, total feature change is not only direct action but also propagated downstream:
\begin{equation}
\Delta x_{\text{total}} = J_S a_S.
\end{equation}
When downstream links are favorable for class flip, one unit intervention can produce more than one unit aggregate effect on classifier score. Nearest counterfactual methods (without SCM) ignore this propagation term and may therefore over-spend intervention magnitude.

\subsection{Validity-Cost Frontier Interpretation}
Recourse quality can be viewed as a bi-objective frontier: maximize validity and minimize intervention burden. Linear models in this report sit near a high-validity region with predictable cost growth as robustness tightens. MLP settings display frontier instability due optimization non-convexity; therefore, robust deployment should report confidence intervals, not single-point estimates, and include optimization diagnostics.

\subsection{Nonlinear Recourse Curvature Effect}
For differentiable recourse with loss $\mathcal{L}(a)=\ell(g(x+f(a)))+\lambda\|a\|_1$, the local Hessian of the smooth term controls gradient flow stability. In regions of high curvature, a fixed step-size can oscillate or overshoot toward higher-cost valid points. This offers a theoretical explanation for observing high validity but inflated action magnitudes in some MLP-AF runs: optimization reaches feasibility, but not low-cost local minima. In practice, line-search or adaptive trust-region updates can reduce this gap.

\subsection{SCM Misspecification Consideration}
The causal advantage observed here assumes the SCM is approximately correct in sign and relative strength. If structural coefficients are misspecified, propagated effects can be mis-estimated and recommended actions may become suboptimal. Nonetheless, even imperfect SCMs often provide a better inductive bias than no structure at all when domain relations are strong. This motivates future work on recourse under causal uncertainty sets, where interventions are optimized against a family of plausible SCM parameters.

\section{Conclusion}
This report completes HW3 Question~5 end-to-end in IEEE format with explicit theoretical and empirical analysis. The software pipeline is fully runnable, missing SCM components are completed, robust evaluations are produced, and each plot is interpreted in a dedicated theory-grounded paragraph. Empirically, linear recourse is highly stable on this dataset, AF reduces intervention cost, and causal recourse outperforms nearest counterfactual in matched cost comparison while maintaining full validity.

\appendices
\section{Robust Linear Derivation (Complete)}
This appendix provides the full derivation behind the robust linear margin shift used in the implementation. Let the linear decision function be $g(x)=w^\top x-b$ with positive prediction when $g(x)\ge 0$. Under intervention $a$ with causal propagation $x^{cf}=x+Ja$, robust feasibility against perturbation $\delta$ with $\|\delta\|_2\le\epsilon$ requires
\begin{equation}
\min_{\|\delta\|_2\le\epsilon} w^\top(x+Ja+\delta)-b \ge 0.
\end{equation}
Using support-function duality of the Euclidean ball,
\begin{equation}
\min_{\|\delta\|_2\le\epsilon} w^\top \delta = -\epsilon\|w\|_2
\end{equation}
in the IMF case, and
\begin{equation}
\min_{\|\delta\|_2\le\epsilon} w^\top J\delta = -\epsilon\|J^\top w\|_2
\end{equation}
in the causal-coordinate uncertainty view. Therefore robust recourse must satisfy
\begin{equation}
w^\top(x+Ja)-b \ge \epsilon\|J^\top w\|_2,
\end{equation}
which is exactly implemented by shifting the effective bias term by $\epsilon\|J^\top w\|_2$ before solving the linear recourse program. This result establishes monotone cost growth with $\epsilon$ whenever feasible-set geometry is fixed.

\section{Weighted L1 Recourse Primal-Dual View}
For each instance, linear recourse solves
\begin{equation}
\min_a \|C a\|_1 \quad \text{s.t.}\quad w^\top J a \ge \gamma,\;\; l\le a\le u,\;\; a_{\bar{\mathcal{A}}}=0,
\end{equation}
where $C=\text{diag}(c_1,\dots,c_D)$, $\gamma=b-w^\top x+\epsilon\|J^\top w\|_2$, and $\mathcal{A}$ is the actionable set. Introducing sign-split variables $a=a^+-a^-$ with $a^\pm\ge 0$, the objective becomes linear and the problem is an LP. Dual multipliers associated with the margin constraint quantify ``cost per unit margin'' and induce an economically interpretable shadow price: higher multiplier means margin is expensive under current actionability limits. This explains why AF can lower cost even at similar predictive quality: classifier sensitivity aligns with lower shadow-price actionable coordinates.

\section{Causal Counterfactual Algebra for Health SCM}
The completed Health SCM uses
\begin{align}
X_1 &= U_1,\\
X_2 &= w_{21}X_1+U_2,\\
X_3 &= w_{31}X_1+w_{32}X_2+U_3,\\
X_4 &= w_{42}X_2+w_{43}X_3+U_4.
\end{align}
For a factual point $x$, abduction computes exogenous variables:
\begin{align}
u_1 &= x_1,\;\;
u_2 = x_2-w_{21}x_1,\;\;
u_3 = x_3-w_{31}x_1-w_{32}x_2,\\
u_4 &= x_4-w_{42}x_2-w_{43}x_3.
\end{align}
Action sets intervened variables (hard intervention in this report) and prediction propagates downstream through remaining equations. The Jacobian matrix used by robust linear recourse is
\begin{equation}
J=
\begin{bmatrix}
1 & 0 & 0 & 0\\
w_{21} & 1 & 0 & 0\\
w_{31} & w_{32} & 1 & 0\\
0 & w_{42} & w_{43} & 1
\end{bmatrix},
\end{equation}
with row-wise upstream zeroing for hard-intervened coordinates in \texttt{get\_Jacobian\_interv}. This guarantees consistency between optimization geometry and causal semantics.

\section{Nonlinear Recourse Objective and Theoretical Guarantees}
For differentiable recourse, the optimized objective per instance is
\begin{equation}
\mathcal{L}(\delta)=\ell(g_\theta(x^{cf}(\delta)),1)+\lambda\|\delta\|_1,
\end{equation}
and under robust mode the loss is evaluated on adversarially perturbed counterfactuals within an $\epsilon$-ball approximation. Because $x^{cf}(\delta)$ passes through nonlinear classifier and possibly SCM transformations, $\mathcal{L}$ is generally non-convex and non-smooth (L1 term). Consequently, first-order optimization guarantees stationarity of local points rather than global optimality. This theoretical fact explains empirical behavior where validity can improve while mean cost worsens: optimization may reach feasible but non-minimal local basins. Practical mitigation includes multi-start optimization, adaptive step-size schedules, and reporting dispersion statistics (already included via distribution plots and appendix CSV traces).

\section{Reproducibility Commands}
\begin{lstlisting}[caption={Exact commands used for the final report build}]
cd /Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3/code/q5_codes
source /Users/tahamajs/Documents/uni/venv/bin/activate

python main.py --seed 0
python generate_report_artifacts.py

cd /Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3/report
make pdf
\end{lstlisting}

\section{Auto-Generated Aggregate CSV}
\lstinputlisting[caption={Health report aggregate CSV}]{../code/q5_codes/results/health_report_aggregate.csv}

\section{Auto-Generated Per-Run CSV}
\lstinputlisting[caption={Health report per-run summary CSV}]{../code/q5_codes/results/health_report_summary.csv}

\section{Auto-Generated Instance Cost CSV}
\lstinputlisting[caption={Per-instance recourse costs CSV}]{../code/q5_codes/results/health_instance_costs.csv}

\section{Auto-Generated Action Profile CSV}
\lstinputlisting[caption={Feature-wise action diagnostics CSV}]{../code/q5_codes/results/health_action_profiles.csv}

\section{Auto-Generated Action Instance Stats CSV}
\lstinputlisting[caption={Per-instance action vectors and norms CSV}]{../code/q5_codes/results/health_action_instance_stats.csv}

\section{Auto-Generated Sparsity Summary CSV}
\lstinputlisting[caption={Valid recourse sparsity/cost summary CSV}]{../code/q5_codes/results/health_sparsity_summary.csv}

\section{Auto-Generated Bootstrap Summary CSV}
\lstinputlisting[caption={Bootstrap confidence interval summary CSV}]{../code/q5_codes/results/health_bootstrap_summary.csv}

\section*{Acknowledgment}
This submission was prepared for Trusted Artificial Intelligence coursework under Dr. Mostafa Tavasolipour.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
