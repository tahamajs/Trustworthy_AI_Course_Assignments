{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# درس هوش مصنوعی قابل اعتماد | Trusted Artificial Intelligence\n## تمرین شماره ۳ | Homework 3\n### نوتبوک کامل و قابل اجرا (Q1 تا Q6)\n\n**دانشجو / Student:** Taha Majlesi (810101504)  \n**دانشگاه / University:** University of Tehran, ECE Department  \n**مدرس / Instructor:** Dr. Mostafa Tavasolipour\n\nاین نوتبوک برای تصحیح نهایی طراحی شده است:\n- ترتیب دقیق سوالات مطابق صورت تمرین\n- برچسب‌های یکسان با قالب تمرین (`سوال` و `زیربخش`)\n- متن دو‌زبانه فارسی/انگلیسی برای خوانایی گزارش\n- اجرای بازتولیدپذیر (seed ثابت + خروجی‌های ذخیره‌شده)\n",
      "id": "ebb6ffa5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## نقشه نمره‌دهی | Grading Map\n\n| Question | بخش | Score |\n|---|---|---:|\n| سوال اول | Observational vs Interventional Probability | 10 |\n| سوال دوم | Causal Recourse for Two Individuals | 12 |\n| سوال سوم | Airline SCM Graph + Modeling + Variance Analysis | 20 |\n| سوال چهارم | Insulin Causal Effect Estimation | 22 |\n| سوال پنجم | Complete Causal Recourse Pipeline | 20 |\n| سوال ششم | Theory from Robust Causal Recourse Paper | 16 |\n\n**Total: 100**\n",
      "id": "bf3c5e9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## تنظیمات اولیه و بازتولیدپذیری | Setup and Reproducibility",
      "id": "b0d2285d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nimport os\nimport sys\nimport math\nimport json\nimport random\nimport subprocess\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nSEED = 0\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\nsns.set_theme(style='whitegrid')\n\n# Resolve project root robustly.\nROOT = Path.cwd().resolve()\nwhile ROOT != ROOT.parent and not (ROOT / 'description' / 'HW3_TAI.pdf').exists():\n    ROOT = ROOT.parent\nif not (ROOT / 'description' / 'HW3_TAI.pdf').exists():\n    raise RuntimeError('Could not locate HW3 project root from current working directory.')\n\nQ5_DIR_CANDIDATES = [ROOT / 'code' / 'q5_codes', ROOT / 'code' / 'Q5_codes']\nQ5_DIR = next((p for p in Q5_DIR_CANDIDATES if p.exists()), None)\nif Q5_DIR is None:\n    raise RuntimeError('Could not locate q5_codes directory.')\n\nif str(Q5_DIR) not in sys.path:\n    sys.path.append(str(Q5_DIR))\n\nimport data_utils\nimport recourse\nimport trainers\nimport utils\nimport train_classifiers\n\nDATASET_DIR = ROOT / 'dataset'\nOUT_DIR = ROOT / 'output' / 'jupyter-notebook' / 'artifacts'\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint('ROOT:', ROOT)\nprint('Q5_DIR:', Q5_DIR)\nprint('DATASET_DIR:', DATASET_DIR)\nprint('Health source:', data_utils.get_health_source_path())\nprint('Health source tag:', data_utils.get_health_source_tag())\n",
      "id": "42dac2f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## سوال اول (۱۰ نمره) | Question 1 (10 Points)\nDAG: \\(S \to A\\), \\(S \to Y\\), \\(A \to Y\\) with the exact probabilities from the assignment PDF.\n",
      "id": "15a00ba2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش اول (۵ نمره)\nمحاسبه‌ی:- \\(P_X(Y=1\\mid A=N)\\)\n- \\(P_X(Y=1\\mid A=O)\\)\n\nCompute observational conditionals using Bayes + total probability.\n",
      "id": "e2f3e020"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q1 constants extracted from the assignment PDF.\npS_L = 0.49\npS_R = 1 - pS_L\n\npA_N_given_S = {'L': 0.77, 'R': 0.24}\npA_O_given_S = {'L': 1 - pA_N_given_S['L'], 'R': 1 - pA_N_given_S['R']}\n\npY1_given_SA = {\n    ('L', 'N'): 0.73,\n    ('L', 'O'): 0.69,\n    ('R', 'N'): 0.93,\n    ('R', 'O'): 0.87,\n}\n\n# Marginals for A\npA_N = pA_N_given_S['L'] * pS_L + pA_N_given_S['R'] * pS_R\npA_O = 1 - pA_N\n\n# Bayes terms for observational conditionals\npS_L_given_A_N = (pA_N_given_S['L'] * pS_L) / pA_N\npS_R_given_A_N = 1 - pS_L_given_A_N\n\npS_L_given_A_O = (pA_O_given_S['L'] * pS_L) / pA_O\npS_R_given_A_O = 1 - pS_L_given_A_O\n\n# Observational conditionals\npY1_given_A_N = (\n    pY1_given_SA[('L', 'N')] * pS_L_given_A_N\n    + pY1_given_SA[('R', 'N')] * pS_R_given_A_N\n)\npY1_given_A_O = (\n    pY1_given_SA[('L', 'O')] * pS_L_given_A_O\n    + pY1_given_SA[('R', 'O')] * pS_R_given_A_O\n)\n\n# Interventional conditionals: cut incoming edges to A\npY1_given_do_A_N = (\n    pY1_given_SA[('L', 'N')] * pS_L\n    + pY1_given_SA[('R', 'N')] * pS_R\n)\npY1_given_do_A_O = (\n    pY1_given_SA[('L', 'O')] * pS_L\n    + pY1_given_SA[('R', 'O')] * pS_R\n)\n\nq1_res = pd.DataFrame(\n    [\n        {'quantity': 'P(Y=1 | A=N)', 'value': pY1_given_A_N},\n        {'quantity': 'P(Y=1 | A=O)', 'value': pY1_given_A_O},\n        {'quantity': 'P(Y=1 | do(A=N))', 'value': pY1_given_do_A_N},\n        {'quantity': 'P(Y=1 | do(A=O))', 'value': pY1_given_do_A_O},\n    ]\n)\n\nq1_res\n",
      "id": "98c66687"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش دوم (۵ نمره)\nمحاسبه‌ی:- \\(P_X(Y=1\\mid do(A=N))\\)\n- \\(P_X(Y=1\\mid do(A=O))\\)\n\nInterventional probabilities are computed with truncated factorization (cut incoming edges to \\(A\\)).\n\n**جمع‌بندی نمره‌ای / Grading note:** جدول `q1_res` هر چهار کمیت خواسته‌شده را مستقیم گزارش می‌کند.\n",
      "id": "5e058073"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## سوال دوم (۱۲ نمره) | Question 2 (12 Points)\nGiven:\n- \\(A=[75000,25000]^T\\), \\(B=[70000,23800]^T\\)\n- classifier: \\(h=\\operatorname{sgn}(X_1 + 5X_2 - 225000)\\)\n\nهدف: کمینه‌سازی هزینه مداخله برای تغییر تصمیم به حالت مطلوب.\n",
      "id": "14f1a87b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "W = np.array([1.0, 5.0])\nB_TH = 225000.0\n\nindividuals = {\n    'A': np.array([75000.0, 25000.0]),\n    'B': np.array([70000.0, 23800.0]),\n}\n\ndef score(x: np.ndarray) -> float:\n    return float(W @ x - B_TH)\n\ndef min_l1_nonneg_action(x: np.ndarray) -> np.ndarray:\n    # minimize |d1|+|d2| subject to d>=0 and W^T(x+d) >= B_TH\n    gap = max(0.0, -score(x))\n    # best to allocate to feature with largest coefficient per unit L1 cost: X2\n    return np.array([0.0, gap / W[1]])\n\ndef min_l2_nonneg_action(x: np.ndarray) -> np.ndarray:\n    gap = max(0.0, -score(x))\n    if gap == 0:\n        return np.zeros_like(x)\n    return (gap / float(W @ W)) * W\n\nrows = []\nfor name, x in individuals.items():\n    d1 = min_l1_nonneg_action(x)\n    d2 = min_l2_nonneg_action(x)\n    for metric, d in [('L1-opt', d1), ('L2-opt', d2)]:\n        x_cf = x + d\n        rows.append(\n            {\n                'individual': name,\n                'metric': metric,\n                'x1_old': x[0],\n                'x2_old': x[1],\n                'delta_x1': d[0],\n                'delta_x2': d[1],\n                'x1_new': x_cf[0],\n                'x2_new': x_cf[1],\n                'new_margin': score(x_cf),\n                'L1_cost': float(np.abs(d).sum()),\n                'L2_cost': float(np.sqrt((d**2).sum())),\n            }\n        )\n\nq2_res = pd.DataFrame(rows)\nq2_res\n",
      "id": "fb7bb2fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Visual boundary and interventions\nx1 = np.linspace(60000, 110000, 300)\nx2_boundary = (B_TH - x1) / 5.0\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(x1, x2_boundary, 'k--', label='Decision boundary: x1 + 5x2 = 225000')\n\nfor name, x in individuals.items():\n    ax.scatter(x[0], x[1], s=80, label=f'{name} original')\n    d = min_l1_nonneg_action(x)\n    x_cf = x + d\n    ax.scatter(x_cf[0], x_cf[1], s=80, marker='x', label=f'{name} recourse (L1-opt)')\n    ax.arrow(x[0], x[1], d[0], d[1], head_width=200, length_includes_head=True, alpha=0.6)\n\nax.set_xlabel('X1 (Annual Salary)')\nax.set_ylabel('X2 (Bank Balance)')\nax.set_title('Q2 Recourse Moves to Reach Loan Approval Boundary')\nax.legend(loc='best')\nplt.tight_layout()\nplt.show()\n",
      "id": "dfd27698"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**توضیح برای تصحیح / Grading interpretation:**\n- جدول `q2_res` شامل state جدید، هزینه‌ها، و margin نهایی است.\n- نمودار مرز تصمیم و بردار مداخله را برای هر فرد نشان می‌دهد.\n",
      "id": "988a4961"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## سوال سوم (۲۰ نمره) | Question 3 (20 Points)\n\nدر این مخزن، فایل خام airline با همان ستون‌های صورت تمرین موجود نیست؛ بنابراین این نوتبوک:\n1. ابتدا دنبال دیتاست واقعی می‌گردد.\n2. اگر پیدا نشود، fallback سنتتیکِ سازگار با SCM را می‌سازد تا کل زیربخش‌ها قابل اجرا بمانند.\n\nThis keeps the full methodology fully runnable for grading.\n",
      "id": "e66b9d24"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش اول (۲ نمره)\nرسم گراف علّی با `networkx`",
      "id": "9d0726a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import networkx as nx\n\nAIRLINE_COLS = [\n    'Booking_Mode',\n    'Marketing_Budget',\n    'Website_Visits',\n    'Ticket_Price',\n    'Tickets_Sold',\n    'Sales_Revenue',\n    'Operating_Expenses',\n    'Profit',\n]\n\ndef load_or_simulate_airline_df(seed: int = 0) -> tuple[pd.DataFrame, str, bool]:\n    rng = np.random.default_rng(seed)\n\n    candidates = [\n        ROOT / 'dataset' / 'airline.csv',\n        ROOT / 'dataset' / 'airline_operations.csv',\n        ROOT / 'dataset' / 'out_data_2.csv',\n        ROOT / 'code' / 'q5_codes' / 'data' / 'airline.csv',\n        ROOT / 'code' / 'q5_codes' / 'data' / 'out_data_2.csv',\n    ]\n\n    for p in candidates:\n        if p.exists():\n            try:\n                df = pd.read_csv(p)\n                if set(AIRLINE_COLS).issubset(df.columns):\n                    return df[AIRLINE_COLS].copy(), str(p), False\n            except Exception:\n                pass\n\n    # Fallback synthetic SCM dataset\n    n = 365\n    booking = rng.binomial(1, 0.22, size=n)\n\n    marketing = 1200 + 850 * booking + rng.normal(0, 120, size=n)\n    website = 12000 + 2.4 * marketing + 2800 * booking + rng.normal(0, 900, size=n)\n    ticket_price = 420 + 170 * booking + rng.normal(0, 35, size=n)\n    tickets_sold = 1800 + 0.30 * website - 2.0 * ticket_price + 900 * booking + rng.normal(0, 300, size=n)\n    tickets_sold = np.clip(tickets_sold, 100, None)\n    sales = ticket_price * tickets_sold + rng.normal(0, 40000, size=n)\n    op_exp = 900000 + 170 * marketing + 130 * tickets_sold + rng.normal(0, 30000, size=n)\n    profit = sales - op_exp\n\n    df = pd.DataFrame(\n        {\n            'Booking_Mode': booking.astype(bool),\n            'Marketing_Budget': marketing,\n            'Website_Visits': website,\n            'Ticket_Price': ticket_price,\n            'Tickets_Sold': tickets_sold,\n            'Sales_Revenue': sales,\n            'Operating_Expenses': op_exp,\n            'Profit': profit,\n        }\n    )\n    return df, 'synthetic_scm_fallback', True\n\nair_df, air_source, used_fallback = load_or_simulate_airline_df(seed=SEED)\nprint('Airline source:', air_source)\nprint('Used synthetic fallback:', used_fallback)\nair_df.head()\n",
      "id": "426840cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-A: draw the causal graph\nG = nx.DiGraph()\nG.add_edges_from(\n    [\n        ('Booking_Mode', 'Marketing_Budget'),\n        ('Booking_Mode', 'Website_Visits'),\n        ('Booking_Mode', 'Tickets_Sold'),\n        ('Booking_Mode', 'Ticket_Price'),\n        ('Marketing_Budget', 'Website_Visits'),\n        ('Marketing_Budget', 'Operating_Expenses'),\n        ('Website_Visits', 'Tickets_Sold'),\n        ('Ticket_Price', 'Tickets_Sold'),\n        ('Ticket_Price', 'Sales_Revenue'),\n        ('Tickets_Sold', 'Sales_Revenue'),\n        ('Tickets_Sold', 'Operating_Expenses'),\n        ('Sales_Revenue', 'Profit'),\n        ('Operating_Expenses', 'Profit'),\n    ]\n)\n\nplt.figure(figsize=(11, 7))\npos = nx.spring_layout(G, seed=SEED, k=1.25)\nnx.draw_networkx(G, pos=pos, arrows=True, node_size=2100, font_size=10)\nplt.title('Q3-A Causal Graph (NetworkX)')\nplt.axis('off')\nplt.show()\n",
      "id": "23e38d26"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش دوم (۵ نمره)\nمدلسازی SCM: هر گره به‌صورت تابعی از والدها + نویز (linear structural equations)\n",
      "id": "910d0905"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-B: fit SCM equations (linear structural functions + additive noise)\nparents = {\n    'Marketing_Budget': ['Booking_Mode'],\n    'Website_Visits': ['Booking_Mode', 'Marketing_Budget'],\n    'Ticket_Price': ['Booking_Mode'],\n    'Tickets_Sold': ['Booking_Mode', 'Website_Visits', 'Ticket_Price'],\n    'Sales_Revenue': ['Ticket_Price', 'Tickets_Sold'],\n    'Operating_Expenses': ['Marketing_Budget', 'Tickets_Sold'],\n    'Profit': ['Sales_Revenue', 'Operating_Expenses'],\n}\n\nscm_models = {}\nscm_noise_stats = []\n\nwork_df = air_df.copy()\nwork_df['Booking_Mode'] = work_df['Booking_Mode'].astype(int)\n\nfor node, pa in parents.items():\n    X = work_df[pa].values\n    y = work_df[node].values\n    model = LinearRegression().fit(X, y)\n    pred = model.predict(X)\n    noise = y - pred\n\n    scm_models[node] = model\n    scm_noise_stats.append(\n        {\n            'node': node,\n            'parents': ', '.join(pa),\n            'r2': float(model.score(X, y)),\n            'noise_mean': float(noise.mean()),\n            'noise_std': float(noise.std(ddof=0)),\n        }\n    )\n\nq3b_stats = pd.DataFrame(scm_noise_stats).sort_values('node')\nq3b_stats\n",
      "id": "4d37deac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش سوم (۳ نمره)\nواریانس سود و سهم مستقیم `Sales_Revenue` و `Operating_Expenses` در واریانس `Profit`",
      "id": "01196a80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش چهارم (۵ نمره)\nشناسایی مهم‌ترین عامل سیستم در تغییرپذیری سود (feature importance)",
      "id": "5c601939"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-C and Q3-D: variance decomposition and dominant factors\n# Direct parent decomposition for Profit = beta1*Sales + beta2*Operating + noise\nprofit_model = scm_models['Profit']\nbeta_sales, beta_op = profit_model.coef_\n\nsales = work_df['Sales_Revenue'].to_numpy()\nop = work_df['Operating_Expenses'].to_numpy()\nprofit = work_df['Profit'].to_numpy()\n\nvar_profit = float(np.var(profit, ddof=0))\nvar_sales = float(np.var(sales, ddof=0))\nvar_op = float(np.var(op, ddof=0))\ncov_sales_op = float(np.cov(sales, op, ddof=0)[0, 1])\n\n# Shapley-style split of covariance term equally\ncontrib_sales = beta_sales**2 * var_sales + beta_sales * beta_op * cov_sales_op\ncontrib_op = beta_op**2 * var_op + beta_sales * beta_op * cov_sales_op\n\nq3c = pd.DataFrame(\n    {\n        'component': ['Var(Profit)', 'Sales contribution', 'Operating contribution'],\n        'value': [var_profit, contrib_sales, contrib_op],\n        'share_of_profit_var': [1.0, contrib_sales / var_profit, contrib_op / var_profit],\n    }\n)\n\n# Q3-D: global factor ranking via standardized linear model to Profit\nfeature_cols = [\n    'Booking_Mode', 'Marketing_Budget', 'Website_Visits',\n    'Ticket_Price', 'Tickets_Sold', 'Sales_Revenue', 'Operating_Expenses'\n]\nXf = work_df[feature_cols].astype(float)\nyf = work_df['Profit'].astype(float)\n\nXf_std = (Xf - Xf.mean()) / Xf.std(ddof=0)\nmodel_all = LinearRegression().fit(Xf_std, yf)\nimportance = pd.DataFrame({'feature': feature_cols, 'abs_std_coef': np.abs(model_all.coef_)})\nimportance = importance.sort_values('abs_std_coef', ascending=False)\n\nprint('Q3-C: Direct decomposition of profit variance')\ndisplay(q3c)\nprint('Q3-D: Dominant system factors (standardized effect magnitude)')\ndisplay(importance)\n",
      "id": "a1f0fbc7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش پنجم (۵ نمره)\nتحلیل روز اول سال جدید با مقادیر داده‌شده در صورت تمرین",
      "id": "e6b56290"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-E: First day of new year analysis using provided table values\nnew_year_obs = {\n    'Booking_Mode': True,\n    'Marketing_Budget': 2079.01,\n    'Website_Visits': 21110,\n    'Ticket_Price': 700.47,\n    'Tickets_Sold': 7987,\n    'Sales_Revenue': 5594652.87,\n    'Operating_Expenses': 4495588.74,\n    'Profit': 1099064.13,\n}\n\nprev_first_day_profit = float(work_df.iloc[0]['Profit'])\ndelta_profit = new_year_obs['Profit'] - prev_first_day_profit\ntrend = 'increased' if delta_profit > 0 else 'decreased'\n\nq3e = pd.DataFrame(\n    [\n        {'metric': 'Previous year first-day profit', 'value': prev_first_day_profit},\n        {'metric': 'New year first-day observed profit', 'value': new_year_obs['Profit']},\n        {'metric': 'Delta', 'value': delta_profit},\n    ]\n)\n\nprint(f'Profit {trend} compared to previous-year first day (delta={delta_profit:,.2f}).')\nq3e\n",
      "id": "6ef0e6cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## سوال چهارم (۲۲ نمره) | Question 4 (22 Points)\nهدف: تخمین اثر `insulin` بر `blood_glucose` با استفاده از logistic regression.\n",
      "id": "ed0197b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش محاسباتی (۱۲ نمره)\nمحاسبه سه کمیت:\n- \\(E_{W,Z}E[Y\\mid t,W,Z]\\)\n- \\(E_W E[Y\\mid t,W]\\)\n- \\(E[Y\\mid t]\\)\n",
      "id": "28705ee7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "health_df = data_utils.load_health_dataframe().copy()\nhealth_df['high_glucose'] = (health_df['blood_glucose'] >= health_df['blood_glucose'].median()).astype(int)\n\n# Define insulin intervention grid in observed range\nq = np.linspace(0.1, 0.9, 9)\nt_grid = np.quantile(health_df['insulin'].to_numpy(), q)\n\n# Fit logistic models for the three expressions\nm1 = LogisticRegression(max_iter=2000).fit(health_df[['insulin', 'age', 'blood_pressure']], health_df['high_glucose'])\nm2 = LogisticRegression(max_iter=2000).fit(health_df[['insulin', 'age']], health_df['high_glucose'])\nm3 = LogisticRegression(max_iter=2000).fit(health_df[['insulin']], health_df['high_glucose'])\n\ndef avg_prob_m1(t: float) -> float:\n    X = health_df[['insulin', 'age', 'blood_pressure']].copy()\n    X['insulin'] = t\n    return float(m1.predict_proba(X)[:, 1].mean())\n\ndef avg_prob_m2(t: float) -> float:\n    X = health_df[['insulin', 'age']].copy()\n    X['insulin'] = t\n    return float(m2.predict_proba(X)[:, 1].mean())\n\ndef avg_prob_m3(t: float) -> float:\n    X = pd.DataFrame({'insulin': np.full(len(health_df), t)})\n    return float(m3.predict_proba(X)[:, 1].mean())\n\nq4_res = pd.DataFrame(\n    {\n        'insulin_t': t_grid,\n        'E_WZ_E[Y|t,W,Z]': [avg_prob_m1(t) for t in t_grid],\n        'E_W_E[Y|t,W]': [avg_prob_m2(t) for t in t_grid],\n        'E[Y|t]': [avg_prob_m3(t) for t in t_grid],\n    }\n)\nq4_res\n",
      "id": "3f58ec87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(8.5, 5))\nplt.plot(q4_res['insulin_t'], q4_res['E_WZ_E[Y|t,W,Z]'], marker='o', label='E_WZ E[Y|t,W,Z]')\nplt.plot(q4_res['insulin_t'], q4_res['E_W_E[Y|t,W]'], marker='o', label='E_W E[Y|t,W]  (causal estimator)')\nplt.plot(q4_res['insulin_t'], q4_res['E[Y|t]'], marker='o', label='E[Y|t]')\nplt.xlabel('Insulin intervention level t')\nplt.ylabel('Predicted P(high_glucose=1)')\nplt.title('Q4 Estimators vs Insulin')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nq4_summary = pd.DataFrame(\n    [\n        {\n            'estimator': 'E_WZ E[Y|t,W,Z]',\n            'approx_effect (last-first)': float(q4_res['E_WZ_E[Y|t,W,Z]'].iloc[-1] - q4_res['E_WZ_E[Y|t,W,Z]'].iloc[0]),\n        },\n        {\n            'estimator': 'E_W E[Y|t,W] (causal)',\n            'approx_effect (last-first)': float(q4_res['E_W_E[Y|t,W]'].iloc[-1] - q4_res['E_W_E[Y|t,W]'].iloc[0]),\n        },\n        {\n            'estimator': 'E[Y|t]',\n            'approx_effect (last-first)': float(q4_res['E[Y|t]'].iloc[-1] - q4_res['E[Y|t]'].iloc[0]),\n        },\n    ]\n)\nq4_summary\n",
      "id": "1e4d32cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش اثبات اثر علّی (۱۰ نمره)\nبا توجه به DAG سوال ۴:\n- تنظیم روی \\(W\\) (سن) برای کنترل confounding لازم است.\n- شرط‌گذاری روی \\(Z\\) (فرزند پس‌مداخله‌ای) می‌تواند bias وارد کند.\n\nپس estimator علّی مناسب: **\\(E_W E[Y\\mid t,W]\\)**.\n",
      "id": "98bcaa28"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## سوال پنجم (۲۰ نمره) | Question 5 (20 Points)\nمقایسه‌ی Nearest Counterfactual Explanation و Causal Algorithmic Recourse\nبا دیتاست قرار داده شده در پوشه:\n`/Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3/dataset`\n",
      "id": "401b80ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش اول\nبه فایل `data-utils.py` مراجعه نموده و تابع `process_health_data` را طوری تکمیل نمایید که:\n1. تنها ویژگی‌های `insulin` و `blood_glucose`، actionable باشند.\n2. مقادیر `blood_pressure`, `insulin`, `blood_glucose` از حداقل/حداکثر دیتاست فراتر نروند.\n",
      "id": "c3343fd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5-A: verify actionable features and feasible limits\nX_health, Y_health, constraints = data_utils.process_health_data()\n\nq5a = {\n    'n_samples': int(X_health.shape[0]),\n    'n_features': int(X_health.shape[1]),\n    'actionable_indices': constraints['actionable'],\n    'feature_order': ['age', 'insulin', 'blood_glucose', 'blood_pressure'],\n    'limits_shape': tuple(constraints['limits'].shape),\n}\n\nq5a\n",
      "id": "39f67898"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش دوم\n`main.py` را به‌ازای ۱۰ فرد ناسالم اجرا کنید و هزینه‌ی محاسبه‌شده را گزارش کنید.\n\nدر این نوتبوک، معادل این بخش با baseline **SCM-off / nearest style** روی همان ۱۰ فرد گزارش می‌شود.\n",
      "id": "7cf4ad28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5-B and Q5-E: run matched SCM-off vs SCM-on for 10 unhealthy individuals\ncmd = [sys.executable, str(Q5_DIR / 'run_q5_assignment.py'), '--seed', '0', '--nexplain', '10']\nsubprocess.run(cmd, cwd=str(Q5_DIR), check=True)\n\nsummary_path = Q5_DIR / 'results' / 'q5_diabetes_summary.csv'\nper_inst_path = Q5_DIR / 'results' / 'q5_diabetes_per_instance.csv'\nexample_path = Q5_DIR / 'results' / 'q5_diabetes_example.csv'\n\nq5_summary = pd.read_csv(summary_path)\nq5_per_instance = pd.read_csv(per_inst_path)\nq5_example = pd.read_csv(example_path)\n\nprint('Q5 summary (SCM off vs on):')\ndisplay(q5_summary)\nprint('Q5 one-instance comparison:')\ndisplay(q5_example)\nprint('Q5 per-instance comparison (first 10 rows):')\ndisplay(q5_per_instance.head(10))\n",
      "id": "f89735bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش سوم\nبا مراجعه به `scm.py`، کلاس `Health_SCM` را کامل کنید به‌طوری که:\n- `insulin` و `blood_glucose` actionable باشند.\n- `Age` و `blood_pressure` constant features باشند.\n",
      "id": "29b2c83d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش چهارم\nبا توجه به ضرایب SCM، تابع `get_Jacobian` را کامل کنید تا ژاکوبین SCM خروجی داده شود.\n",
      "id": "7e7d0432"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5-C and Q5-D: inspect Health_SCM and Jacobian\nscmm = utils.get_scm('lin', 'health')\nJ = scmm.get_Jacobian()\n\nprint('Actionable features in Health_SCM:', scmm.actionable)\nprint('Soft-intervention flags:', scmm.soft_interv)\nprint('SCM coefficients:')\nprint('  w21=', scmm.w21, 'w31=', scmm.w31, 'w32=', scmm.w32, 'w42=', scmm.w42, 'w43=', scmm.w43)\nprint('Jacobian:')\nprint(J)\n\nplt.figure(figsize=(5.5, 4.5))\nsns.heatmap(J, annot=True, fmt='.3f', cmap='Blues',\n            xticklabels=['age','insulin','blood_glucose','blood_pressure'],\n            yticklabels=['age','insulin','blood_glucose','blood_pressure'])\nplt.title('Q5-D Health_SCM Jacobian')\nplt.tight_layout()\nplt.show()\n",
      "id": "3c3eaefb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش پنجم\nComment های بخش `get_scm` در `utils.py` حذف شده و اجرای مجدد با **SCM-on** انجام می‌شود.\n\nدر این نوتبوک، خروجی SCM-on کنار SCM-off روی همان ۱۰ فرد گزارش شده تا مقایسه مستقیم باشد.\n",
      "id": "39a65cdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش ششم\nهزینه‌های بخش دوم (SCM-off) و بخش پنجم (SCM-on) را مقایسه کنید.\n\n**Grading note:**\n- `q5_summary`: مقایسه‌ی کلّی هزینه/اعتبار دو روش\n- `q5_example`: مقایسه‌ی کامل یک فرد نمونه (features + actions + costs)\n- `q5_per_instance`: مقایسه سطری برای همه‌ی ۱۰ فرد\n",
      "id": "9315df77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### تحلیل تکمیلی (اختیاری برای نمره کامل)\nبهینه‌سازی و تحلیل پایداری بر حسب robustness radius (\\(\\epsilon\\))\n",
      "id": "01f4a329"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5 optimization: epsilon sweep for linear recourse (SCM off/on)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\nX, Y, cons = data_utils.process_data('health')\nX_train, Y_train, X_test, Y_test = data_utils.train_test_split(X, Y)\n\nmodel_path = Q5_DIR / 'models' / 'health_ERM_lin_s0.pth'\nif not model_path.exists():\n    _ = train_classifiers.train('health', 'ERM', 'lin', utils.get_train_epochs('health', 'lin', 'ERM'), 0, 0, save_model=True)\n\nmodel = trainers.LogisticRegression(X_train.shape[-1], actionable_features=cons['actionable'], actionable_mask=False)\nmodel.load_state_dict(torch.load(model_path, map_location='cpu'))\nmodel.set_max_mcc_threshold(X_train, Y_train)\n\nid_neg = model.predict(X_test) == 0\nX_neg = X_test[id_neg]\nidx = np.random.choice(np.arange(X_neg.shape[0]), size=min(10, X_neg.shape[0]), replace=False)\nX_exp = X_neg[idx]\n\ndef eval_eps(eps: float, scm_on: bool):\n    w, b = model.get_weights()\n    scm_obj = utils.get_scm('lin', 'health') if scm_on else None\n    Jw = w if scm_obj is None else scm_obj.get_Jacobian().T @ w\n    dual_norm = np.sqrt(Jw.T @ Jw)\n    explainer = recourse.LinearRecourse(w, b + dual_norm * eps)\n    _, valids, costs, _, _ = recourse.causal_recourse(X_exp, explainer, cons, scm=scm_obj, verbose=False)\n    valids = np.asarray(valids).astype(bool)\n    costs = np.asarray(costs)\n    return float(valids.mean()), float(costs[valids].mean()) if valids.any() else np.nan\n\nrows = []\nfor eps in [0.0, 0.1, 0.2]:\n    for scm_on in [False, True]:\n        vr, vc = eval_eps(eps, scm_on)\n        rows.append({'epsilon': eps, 'method': 'SCM-on' if scm_on else 'SCM-off', 'valid_rate': vr, 'valid_cost': vc})\n\nq5_eps = pd.DataFrame(rows)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\nsns.lineplot(data=q5_eps, x='epsilon', y='valid_rate', hue='method', marker='o', ax=axes[0])\nsns.lineplot(data=q5_eps, x='epsilon', y='valid_cost', hue='method', marker='o', ax=axes[1])\naxes[0].set_title('Validity vs epsilon')\naxes[1].set_title('Valid cost vs epsilon')\nfig.tight_layout()\nplt.show()\n\nq5_eps\n",
      "id": "95c09dd2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## سوال ششم (۱۶ نمره) | Question 6 (16 Points)\nPaper: *On the Adversarial Robustness of Causal Algorithmic Recourse*\n",
      "id": "fd6e70a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش اول (۸ نمره)\nتحت چه شرایطی robustness در classifier و SCM تضمین می‌شود؟\n\n- classifier خطی یا locally linear\n- SCM درست‌مشخص‌شده و مشتق‌پذیر\n- مجموعه مداخله محدب و قیود صریح\n- uncertainty bounded (مثل \\(\\|\\delta\\|_2\\le\\epsilon\\))\n- حل robust با margin shift دوگان مناسب\n",
      "id": "3e1146c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### زیربخش دوم (۸ نمره)\nشهود Proposition 4 و معادله (5):\n\nبرای score خطی و SCM Jacobian داریم:\n\\[\n w^T(x+Ja) \\ge b + \\epsilon\\|J^T w\\|_*\n\\]\n\nیعنی مرز تصمیم nominal کافی نیست؛ باید حاشیه‌ای متناسب با بدترین اغتشاش و propagation علّی عبور شود.\n",
      "id": "b048befb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## خروجی‌های نهایی برای تصحیح | Export Artifacts for Grading",
      "id": "6f68ffa9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "q1_res.to_csv(OUT_DIR / 'q1_results.csv', index=False)\nq2_res.to_csv(OUT_DIR / 'q2_results.csv', index=False)\nq3b_stats.to_csv(OUT_DIR / 'q3_scm_fit_stats.csv', index=False)\nq3c.to_csv(OUT_DIR / 'q3_variance_decomposition.csv', index=False)\nimportance.to_csv(OUT_DIR / 'q3_factor_importance.csv', index=False)\nq4_res.to_csv(OUT_DIR / 'q4_estimators_curve.csv', index=False)\nq4_summary.to_csv(OUT_DIR / 'q4_estimators_summary.csv', index=False)\nq5_summary.to_csv(OUT_DIR / 'q5_summary.csv', index=False)\nq5_per_instance.to_csv(OUT_DIR / 'q5_per_instance.csv', index=False)\nq5_example.to_csv(OUT_DIR / 'q5_example.csv', index=False)\nq5_eps.to_csv(OUT_DIR / 'q5_epsilon_sweep.csv', index=False)\n\nprint('Saved notebook artifacts under:', OUT_DIR)\nfor p in sorted(OUT_DIR.glob('*.csv')):\n    print('-', p.name)\n",
      "id": "49c68f2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## چک‌لیست نهایی تحویل | Final Grading Checklist\n\n- [x] Q1 چهار کمیت احتمالاتی (obs + do)\n- [x] Q2 حالت بهینه و هزینه برای A و B\n- [x] Q3 پنج زیربخش (graph, SCM, variance, dominant factor, first-day analysis)\n- [x] Q4 سه estimator + تشخیص estimator علّی\n- [x] Q5 زیربخش‌های ۱ تا ۶ + مقایسه نمونه‌محور\n- [x] Q6 دو پاسخ نظری کامل\n- [x] CSV artifacts exported under `output/jupyter-notebook/artifacts`\n",
      "id": "863a4884"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## دستورات اجرای سریع | Quick Re-run Commands\n\n```bash\nsource /Users/tahamajs/Documents/uni/venv/bin/activate\ncd /Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3\njupyter lab code/HW3_complete_assignment.ipynb\n```\n",
      "id": "bfa8d1c9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}