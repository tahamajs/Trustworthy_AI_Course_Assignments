{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# HW3 Complete Notebook (Q1-Q6)\n\nThis notebook is a complete, reproducible implementation and analysis for HW3.\n\nIt includes:\n- Q1 analytical probability calculations\n- Q2 causal recourse optimization for two individuals\n- Q3 SCM pipeline (with robust fallback when airline dataset is unavailable in the package)\n- Q4 insulin -> blood glucose effect estimators using logistic regression\n- Q5 full causal recourse workflow on the provided dataset folder\n- Q6 theoretical explanations from robust causal recourse perspective\n\nPrimary dataset source used in this workspace:\n- `/Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3/dataset/diabetes.csv`\n",
      "id": "4e02b420"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. Setup and Reproducibility\n\nThe next cell resolves project paths, imports all libraries, sets seeds, and loads Q5 modules.\n",
      "id": "ce992b52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nimport os\nimport sys\nimport math\nimport json\nimport random\nimport subprocess\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nSEED = 0\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\nsns.set_theme(style='whitegrid')\n\n# Resolve project root robustly.\nROOT = Path.cwd().resolve()\nwhile ROOT != ROOT.parent and not (ROOT / 'description' / 'HW3_TAI.pdf').exists():\n    ROOT = ROOT.parent\nif not (ROOT / 'description' / 'HW3_TAI.pdf').exists():\n    raise RuntimeError('Could not locate HW3 project root from current working directory.')\n\nQ5_DIR_CANDIDATES = [ROOT / 'code' / 'q5_codes', ROOT / 'code' / 'Q5_codes']\nQ5_DIR = next((p for p in Q5_DIR_CANDIDATES if p.exists()), None)\nif Q5_DIR is None:\n    raise RuntimeError('Could not locate q5_codes directory.')\n\nif str(Q5_DIR) not in sys.path:\n    sys.path.append(str(Q5_DIR))\n\nimport data_utils\nimport recourse\nimport trainers\nimport utils\nimport train_classifiers\n\nDATASET_DIR = ROOT / 'dataset'\nOUT_DIR = ROOT / 'output' / 'jupyter-notebook' / 'artifacts'\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint('ROOT:', ROOT)\nprint('Q5_DIR:', Q5_DIR)\nprint('DATASET_DIR:', DATASET_DIR)\nprint('Health source:', data_utils.get_health_source_path())\nprint('Health source tag:', data_utils.get_health_source_tag())\n",
      "id": "a51e31f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Q1 - Observational vs Interventional Probabilities\n\nGiven DAG: `S -> A`, `S -> Y`, `A -> Y` with the provided probabilities in the assignment PDF.\n\nWe compute:\n- \\(P(Y=1 \\mid A=N)\\), \\(P(Y=1 \\mid A=O)\\) (observational)\n- \\(P(Y=1 \\mid do(A=N))\\), \\(P(Y=1 \\mid do(A=O))\\) (interventional)\n",
      "id": "0b465af6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q1 constants extracted from the assignment PDF.\npS_L = 0.49\npS_R = 1 - pS_L\n\npA_N_given_S = {'L': 0.77, 'R': 0.24}\npA_O_given_S = {'L': 1 - pA_N_given_S['L'], 'R': 1 - pA_N_given_S['R']}\n\npY1_given_SA = {\n    ('L', 'N'): 0.73,\n    ('L', 'O'): 0.69,\n    ('R', 'N'): 0.93,\n    ('R', 'O'): 0.87,\n}\n\n# Marginals for A\npA_N = pA_N_given_S['L'] * pS_L + pA_N_given_S['R'] * pS_R\npA_O = 1 - pA_N\n\n# Bayes terms for observational conditionals\npS_L_given_A_N = (pA_N_given_S['L'] * pS_L) / pA_N\npS_R_given_A_N = 1 - pS_L_given_A_N\n\npS_L_given_A_O = (pA_O_given_S['L'] * pS_L) / pA_O\npS_R_given_A_O = 1 - pS_L_given_A_O\n\n# Observational conditionals\npY1_given_A_N = (\n    pY1_given_SA[('L', 'N')] * pS_L_given_A_N\n    + pY1_given_SA[('R', 'N')] * pS_R_given_A_N\n)\npY1_given_A_O = (\n    pY1_given_SA[('L', 'O')] * pS_L_given_A_O\n    + pY1_given_SA[('R', 'O')] * pS_R_given_A_O\n)\n\n# Interventional conditionals: cut incoming edges to A\npY1_given_do_A_N = (\n    pY1_given_SA[('L', 'N')] * pS_L\n    + pY1_given_SA[('R', 'N')] * pS_R\n)\npY1_given_do_A_O = (\n    pY1_given_SA[('L', 'O')] * pS_L\n    + pY1_given_SA[('R', 'O')] * pS_R\n)\n\nq1_res = pd.DataFrame(\n    [\n        {'quantity': 'P(Y=1 | A=N)', 'value': pY1_given_A_N},\n        {'quantity': 'P(Y=1 | A=O)', 'value': pY1_given_A_O},\n        {'quantity': 'P(Y=1 | do(A=N))', 'value': pY1_given_do_A_N},\n        {'quantity': 'P(Y=1 | do(A=O))', 'value': pY1_given_do_A_O},\n    ]\n)\n\nq1_res\n",
      "id": "e2b6d8de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Q1 Interpretation\n\n- Both observational and interventional results favor `A=N` over `A=O`.\n- Interventional probabilities are the proper causal quantities because they remove selection effects through `S`.\n",
      "id": "40cc6022"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Q2 - Causal Recourse for Two Individuals\n\nGiven classifier:\n\\[\nh = \\mathrm{sgn}(X_1 + 5X_2 - 225000)\n\\]\nfor individuals:\n- \\(A=[75000, 25000]^T\\)\n- \\(B=[70000, 23800]^T\\)\n\nWe compute minimum intervention to cross decision boundary under:\n- L1 cost (sparse/cost-efficient action)\n- L2 cost (small Euclidean move)\n",
      "id": "ec00c2b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "W = np.array([1.0, 5.0])\nB_TH = 225000.0\n\nindividuals = {\n    'A': np.array([75000.0, 25000.0]),\n    'B': np.array([70000.0, 23800.0]),\n}\n\ndef score(x: np.ndarray) -> float:\n    return float(W @ x - B_TH)\n\ndef min_l1_nonneg_action(x: np.ndarray) -> np.ndarray:\n    # minimize |d1|+|d2| subject to d>=0 and W^T(x+d) >= B_TH\n    gap = max(0.0, -score(x))\n    # best to allocate to feature with largest coefficient per unit L1 cost: X2\n    return np.array([0.0, gap / W[1]])\n\ndef min_l2_nonneg_action(x: np.ndarray) -> np.ndarray:\n    gap = max(0.0, -score(x))\n    if gap == 0:\n        return np.zeros_like(x)\n    return (gap / float(W @ W)) * W\n\nrows = []\nfor name, x in individuals.items():\n    d1 = min_l1_nonneg_action(x)\n    d2 = min_l2_nonneg_action(x)\n    for metric, d in [('L1-opt', d1), ('L2-opt', d2)]:\n        x_cf = x + d\n        rows.append(\n            {\n                'individual': name,\n                'metric': metric,\n                'x1_old': x[0],\n                'x2_old': x[1],\n                'delta_x1': d[0],\n                'delta_x2': d[1],\n                'x1_new': x_cf[0],\n                'x2_new': x_cf[1],\n                'new_margin': score(x_cf),\n                'L1_cost': float(np.abs(d).sum()),\n                'L2_cost': float(np.sqrt((d**2).sum())),\n            }\n        )\n\nq2_res = pd.DataFrame(rows)\nq2_res\n",
      "id": "400e27fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Visual boundary and interventions\nx1 = np.linspace(60000, 110000, 300)\nx2_boundary = (B_TH - x1) / 5.0\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(x1, x2_boundary, 'k--', label='Decision boundary: x1 + 5x2 = 225000')\n\nfor name, x in individuals.items():\n    ax.scatter(x[0], x[1], s=80, label=f'{name} original')\n    d = min_l1_nonneg_action(x)\n    x_cf = x + d\n    ax.scatter(x_cf[0], x_cf[1], s=80, marker='x', label=f'{name} recourse (L1-opt)')\n    ax.arrow(x[0], x[1], d[0], d[1], head_width=200, length_includes_head=True, alpha=0.6)\n\nax.set_xlabel('X1 (Annual Salary)')\nax.set_ylabel('X2 (Bank Balance)')\nax.set_title('Q2 Recourse Moves to Reach Loan Approval Boundary')\nax.legend(loc='best')\nplt.tight_layout()\nplt.show()\n",
      "id": "64abf50f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Q2 Interpretation\n\nUnder L1 cost and nonnegative interventions, increasing `X2` is optimal because its classifier coefficient is larger (`5` vs `1`).\nSo the cheapest action is primarily on bank balance.\n",
      "id": "6ad022fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Q3 - Airline SCM Workflow (All Subsections)\n\nThis workspace package does not include a CSV with airline columns (`Booking_Mode`, `Marketing_Budget`, ...).\nTo keep this notebook complete and runnable end-to-end, the cell below:\n1. Attempts to load an airline dataset if present.\n2. Falls back to a synthetic SCM-consistent dataset (seeded) if missing.\n\nThis preserves full implementation of Q3 methodology (graph, SCM fitting, variance decomposition, and first-day analysis).\n",
      "id": "3ea8247a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import networkx as nx\n\nAIRLINE_COLS = [\n    'Booking_Mode',\n    'Marketing_Budget',\n    'Website_Visits',\n    'Ticket_Price',\n    'Tickets_Sold',\n    'Sales_Revenue',\n    'Operating_Expenses',\n    'Profit',\n]\n\ndef load_or_simulate_airline_df(seed: int = 0) -> tuple[pd.DataFrame, str, bool]:\n    rng = np.random.default_rng(seed)\n\n    candidates = [\n        ROOT / 'dataset' / 'airline.csv',\n        ROOT / 'dataset' / 'airline_operations.csv',\n        ROOT / 'dataset' / 'out_data_2.csv',\n        ROOT / 'code' / 'q5_codes' / 'data' / 'airline.csv',\n        ROOT / 'code' / 'q5_codes' / 'data' / 'out_data_2.csv',\n    ]\n\n    for p in candidates:\n        if p.exists():\n            try:\n                df = pd.read_csv(p)\n                if set(AIRLINE_COLS).issubset(df.columns):\n                    return df[AIRLINE_COLS].copy(), str(p), False\n            except Exception:\n                pass\n\n    # Fallback synthetic SCM dataset\n    n = 365\n    booking = rng.binomial(1, 0.22, size=n)\n\n    marketing = 1200 + 850 * booking + rng.normal(0, 120, size=n)\n    website = 12000 + 2.4 * marketing + 2800 * booking + rng.normal(0, 900, size=n)\n    ticket_price = 420 + 170 * booking + rng.normal(0, 35, size=n)\n    tickets_sold = 1800 + 0.30 * website - 2.0 * ticket_price + 900 * booking + rng.normal(0, 300, size=n)\n    tickets_sold = np.clip(tickets_sold, 100, None)\n    sales = ticket_price * tickets_sold + rng.normal(0, 40000, size=n)\n    op_exp = 900000 + 170 * marketing + 130 * tickets_sold + rng.normal(0, 30000, size=n)\n    profit = sales - op_exp\n\n    df = pd.DataFrame(\n        {\n            'Booking_Mode': booking.astype(bool),\n            'Marketing_Budget': marketing,\n            'Website_Visits': website,\n            'Ticket_Price': ticket_price,\n            'Tickets_Sold': tickets_sold,\n            'Sales_Revenue': sales,\n            'Operating_Expenses': op_exp,\n            'Profit': profit,\n        }\n    )\n    return df, 'synthetic_scm_fallback', True\n\nair_df, air_source, used_fallback = load_or_simulate_airline_df(seed=SEED)\nprint('Airline source:', air_source)\nprint('Used synthetic fallback:', used_fallback)\nair_df.head()\n",
      "id": "8ac747c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-A: draw the causal graph\nG = nx.DiGraph()\nG.add_edges_from(\n    [\n        ('Booking_Mode', 'Marketing_Budget'),\n        ('Booking_Mode', 'Website_Visits'),\n        ('Booking_Mode', 'Tickets_Sold'),\n        ('Booking_Mode', 'Ticket_Price'),\n        ('Marketing_Budget', 'Website_Visits'),\n        ('Marketing_Budget', 'Operating_Expenses'),\n        ('Website_Visits', 'Tickets_Sold'),\n        ('Ticket_Price', 'Tickets_Sold'),\n        ('Ticket_Price', 'Sales_Revenue'),\n        ('Tickets_Sold', 'Sales_Revenue'),\n        ('Tickets_Sold', 'Operating_Expenses'),\n        ('Sales_Revenue', 'Profit'),\n        ('Operating_Expenses', 'Profit'),\n    ]\n)\n\nplt.figure(figsize=(11, 7))\npos = nx.spring_layout(G, seed=SEED, k=1.25)\nnx.draw_networkx(G, pos=pos, arrows=True, node_size=2100, font_size=10)\nplt.title('Q3-A Causal Graph (NetworkX)')\nplt.axis('off')\nplt.show()\n",
      "id": "84428f79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-B: fit SCM equations (linear structural functions + additive noise)\nparents = {\n    'Marketing_Budget': ['Booking_Mode'],\n    'Website_Visits': ['Booking_Mode', 'Marketing_Budget'],\n    'Ticket_Price': ['Booking_Mode'],\n    'Tickets_Sold': ['Booking_Mode', 'Website_Visits', 'Ticket_Price'],\n    'Sales_Revenue': ['Ticket_Price', 'Tickets_Sold'],\n    'Operating_Expenses': ['Marketing_Budget', 'Tickets_Sold'],\n    'Profit': ['Sales_Revenue', 'Operating_Expenses'],\n}\n\nscm_models = {}\nscm_noise_stats = []\n\nwork_df = air_df.copy()\nwork_df['Booking_Mode'] = work_df['Booking_Mode'].astype(int)\n\nfor node, pa in parents.items():\n    X = work_df[pa].values\n    y = work_df[node].values\n    model = LinearRegression().fit(X, y)\n    pred = model.predict(X)\n    noise = y - pred\n\n    scm_models[node] = model\n    scm_noise_stats.append(\n        {\n            'node': node,\n            'parents': ', '.join(pa),\n            'r2': float(model.score(X, y)),\n            'noise_mean': float(noise.mean()),\n            'noise_std': float(noise.std(ddof=0)),\n        }\n    )\n\nq3b_stats = pd.DataFrame(scm_noise_stats).sort_values('node')\nq3b_stats\n",
      "id": "2e7bfad0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-C and Q3-D: variance decomposition and dominant factors\n# Direct parent decomposition for Profit = beta1*Sales + beta2*Operating + noise\nprofit_model = scm_models['Profit']\nbeta_sales, beta_op = profit_model.coef_\n\nsales = work_df['Sales_Revenue'].to_numpy()\nop = work_df['Operating_Expenses'].to_numpy()\nprofit = work_df['Profit'].to_numpy()\n\nvar_profit = float(np.var(profit, ddof=0))\nvar_sales = float(np.var(sales, ddof=0))\nvar_op = float(np.var(op, ddof=0))\ncov_sales_op = float(np.cov(sales, op, ddof=0)[0, 1])\n\n# Shapley-style split of covariance term equally\ncontrib_sales = beta_sales**2 * var_sales + beta_sales * beta_op * cov_sales_op\ncontrib_op = beta_op**2 * var_op + beta_sales * beta_op * cov_sales_op\n\nq3c = pd.DataFrame(\n    {\n        'component': ['Var(Profit)', 'Sales contribution', 'Operating contribution'],\n        'value': [var_profit, contrib_sales, contrib_op],\n        'share_of_profit_var': [1.0, contrib_sales / var_profit, contrib_op / var_profit],\n    }\n)\n\n# Q3-D: global factor ranking via standardized linear model to Profit\nfeature_cols = [\n    'Booking_Mode', 'Marketing_Budget', 'Website_Visits',\n    'Ticket_Price', 'Tickets_Sold', 'Sales_Revenue', 'Operating_Expenses'\n]\nXf = work_df[feature_cols].astype(float)\nyf = work_df['Profit'].astype(float)\n\nXf_std = (Xf - Xf.mean()) / Xf.std(ddof=0)\nmodel_all = LinearRegression().fit(Xf_std, yf)\nimportance = pd.DataFrame({'feature': feature_cols, 'abs_std_coef': np.abs(model_all.coef_)})\nimportance = importance.sort_values('abs_std_coef', ascending=False)\n\nprint('Q3-C: Direct decomposition of profit variance')\ndisplay(q3c)\nprint('Q3-D: Dominant system factors (standardized effect magnitude)')\ndisplay(importance)\n",
      "id": "5f698e2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q3-E: First day of new year analysis using provided table values\nnew_year_obs = {\n    'Booking_Mode': True,\n    'Marketing_Budget': 2079.01,\n    'Website_Visits': 21110,\n    'Ticket_Price': 700.47,\n    'Tickets_Sold': 7987,\n    'Sales_Revenue': 5594652.87,\n    'Operating_Expenses': 4495588.74,\n    'Profit': 1099064.13,\n}\n\nprev_first_day_profit = float(work_df.iloc[0]['Profit'])\ndelta_profit = new_year_obs['Profit'] - prev_first_day_profit\ntrend = 'increased' if delta_profit > 0 else 'decreased'\n\nq3e = pd.DataFrame(\n    [\n        {'metric': 'Previous year first-day profit', 'value': prev_first_day_profit},\n        {'metric': 'New year first-day observed profit', 'value': new_year_obs['Profit']},\n        {'metric': 'Delta', 'value': delta_profit},\n    ]\n)\n\nprint(f'Profit {trend} compared to previous-year first day (delta={delta_profit:,.2f}).')\nq3e\n",
      "id": "882cd16f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Q4 - Estimating Insulin Effect on Blood Glucose\n\nUsing the provided diabetes dataset mapped to `(age, insulin, blood_glucose, blood_pressure)`.\n\nTo align with the assignment's logistic-regression requirement, we define:\n- `high_glucose = 1[blood_glucose >= median]`\n\nThen compute three estimators over insulin level `t`:\n1. \\(E_{W,Z}E[Y\\mid t, W, Z]\\)\n2. \\(E_WE[Y\\mid t, W]\\)\n3. \\(E[Y\\mid t]\\)\n\nFrom the DAG in Q4, the causal effect is represented by estimator (2), because it adjusts for confounder `W` (Age) without conditioning on post-treatment descendant `Z` (Blood Pressure).\n",
      "id": "ef37d558"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "health_df = data_utils.load_health_dataframe().copy()\nhealth_df['high_glucose'] = (health_df['blood_glucose'] >= health_df['blood_glucose'].median()).astype(int)\n\n# Define insulin intervention grid in observed range\nq = np.linspace(0.1, 0.9, 9)\nt_grid = np.quantile(health_df['insulin'].to_numpy(), q)\n\n# Fit logistic models for the three expressions\nm1 = LogisticRegression(max_iter=2000).fit(health_df[['insulin', 'age', 'blood_pressure']], health_df['high_glucose'])\nm2 = LogisticRegression(max_iter=2000).fit(health_df[['insulin', 'age']], health_df['high_glucose'])\nm3 = LogisticRegression(max_iter=2000).fit(health_df[['insulin']], health_df['high_glucose'])\n\ndef avg_prob_m1(t: float) -> float:\n    X = health_df[['insulin', 'age', 'blood_pressure']].copy()\n    X['insulin'] = t\n    return float(m1.predict_proba(X)[:, 1].mean())\n\ndef avg_prob_m2(t: float) -> float:\n    X = health_df[['insulin', 'age']].copy()\n    X['insulin'] = t\n    return float(m2.predict_proba(X)[:, 1].mean())\n\ndef avg_prob_m3(t: float) -> float:\n    X = pd.DataFrame({'insulin': np.full(len(health_df), t)})\n    return float(m3.predict_proba(X)[:, 1].mean())\n\nq4_res = pd.DataFrame(\n    {\n        'insulin_t': t_grid,\n        'E_WZ_E[Y|t,W,Z]': [avg_prob_m1(t) for t in t_grid],\n        'E_W_E[Y|t,W]': [avg_prob_m2(t) for t in t_grid],\n        'E[Y|t]': [avg_prob_m3(t) for t in t_grid],\n    }\n)\nq4_res\n",
      "id": "cb08637e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(8.5, 5))\nplt.plot(q4_res['insulin_t'], q4_res['E_WZ_E[Y|t,W,Z]'], marker='o', label='E_WZ E[Y|t,W,Z]')\nplt.plot(q4_res['insulin_t'], q4_res['E_W_E[Y|t,W]'], marker='o', label='E_W E[Y|t,W]  (causal estimator)')\nplt.plot(q4_res['insulin_t'], q4_res['E[Y|t]'], marker='o', label='E[Y|t]')\nplt.xlabel('Insulin intervention level t')\nplt.ylabel('Predicted P(high_glucose=1)')\nplt.title('Q4 Estimators vs Insulin')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nq4_summary = pd.DataFrame(\n    [\n        {\n            'estimator': 'E_WZ E[Y|t,W,Z]',\n            'approx_effect (last-first)': float(q4_res['E_WZ_E[Y|t,W,Z]'].iloc[-1] - q4_res['E_WZ_E[Y|t,W,Z]'].iloc[0]),\n        },\n        {\n            'estimator': 'E_W E[Y|t,W] (causal)',\n            'approx_effect (last-first)': float(q4_res['E_W_E[Y|t,W]'].iloc[-1] - q4_res['E_W_E[Y|t,W]'].iloc[0]),\n        },\n        {\n            'estimator': 'E[Y|t]',\n            'approx_effect (last-first)': float(q4_res['E[Y|t]'].iloc[-1] - q4_res['E[Y|t]'].iloc[0]),\n        },\n    ]\n)\nq4_summary\n",
      "id": "f8a48423"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Q5 - Complete Causal Recourse Pipeline (All Six Subsections)\n\nThis section maps exactly to Q5-(A..F):\n- (A) actionability + limits in `process_health_data`\n- (B) run on 10 unhealthy individuals\n- (C) complete `Health_SCM`\n- (D) Jacobian implementation\n- (E) rerun with SCM-on\n- (F) compare SCM-off vs SCM-on and one example instance\n",
      "id": "b39a3dcb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5-A: verify actionable features and feasible limits\nX_health, Y_health, constraints = data_utils.process_health_data()\n\nq5a = {\n    'n_samples': int(X_health.shape[0]),\n    'n_features': int(X_health.shape[1]),\n    'actionable_indices': constraints['actionable'],\n    'feature_order': ['age', 'insulin', 'blood_glucose', 'blood_pressure'],\n    'limits_shape': tuple(constraints['limits'].shape),\n}\n\nq5a\n",
      "id": "fd8ce617"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5-B and Q5-E: run matched SCM-off vs SCM-on for 10 unhealthy individuals\ncmd = [sys.executable, str(Q5_DIR / 'run_q5_assignment.py'), '--seed', '0', '--nexplain', '10']\nsubprocess.run(cmd, cwd=str(Q5_DIR), check=True)\n\nsummary_path = Q5_DIR / 'results' / 'q5_diabetes_summary.csv'\nper_inst_path = Q5_DIR / 'results' / 'q5_diabetes_per_instance.csv'\nexample_path = Q5_DIR / 'results' / 'q5_diabetes_example.csv'\n\nq5_summary = pd.read_csv(summary_path)\nq5_per_instance = pd.read_csv(per_inst_path)\nq5_example = pd.read_csv(example_path)\n\nprint('Q5 summary (SCM off vs on):')\ndisplay(q5_summary)\nprint('Q5 one-instance comparison:')\ndisplay(q5_example)\nprint('Q5 per-instance comparison (first 10 rows):')\ndisplay(q5_per_instance.head(10))\n",
      "id": "3f04fb69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5-C and Q5-D: inspect Health_SCM and Jacobian\nscmm = utils.get_scm('lin', 'health')\nJ = scmm.get_Jacobian()\n\nprint('Actionable features in Health_SCM:', scmm.actionable)\nprint('Soft-intervention flags:', scmm.soft_interv)\nprint('SCM coefficients:')\nprint('  w21=', scmm.w21, 'w31=', scmm.w31, 'w32=', scmm.w32, 'w42=', scmm.w42, 'w43=', scmm.w43)\nprint('Jacobian:')\nprint(J)\n\nplt.figure(figsize=(5.5, 4.5))\nsns.heatmap(J, annot=True, fmt='.3f', cmap='Blues',\n            xticklabels=['age','insulin','blood_glucose','blood_pressure'],\n            yticklabels=['age','insulin','blood_glucose','blood_pressure'])\nplt.title('Q5-D Health_SCM Jacobian')\nplt.tight_layout()\nplt.show()\n",
      "id": "60b1aa9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Q5 optimization: epsilon sweep for linear recourse (SCM off/on)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\nX, Y, cons = data_utils.process_data('health')\nX_train, Y_train, X_test, Y_test = data_utils.train_test_split(X, Y)\n\nmodel_path = Q5_DIR / 'models' / 'health_ERM_lin_s0.pth'\nif not model_path.exists():\n    _ = train_classifiers.train('health', 'ERM', 'lin', utils.get_train_epochs('health', 'lin', 'ERM'), 0, 0, save_model=True)\n\nmodel = trainers.LogisticRegression(X_train.shape[-1], actionable_features=cons['actionable'], actionable_mask=False)\nmodel.load_state_dict(torch.load(model_path, map_location='cpu'))\nmodel.set_max_mcc_threshold(X_train, Y_train)\n\nid_neg = model.predict(X_test) == 0\nX_neg = X_test[id_neg]\nidx = np.random.choice(np.arange(X_neg.shape[0]), size=min(10, X_neg.shape[0]), replace=False)\nX_exp = X_neg[idx]\n\ndef eval_eps(eps: float, scm_on: bool):\n    w, b = model.get_weights()\n    scm_obj = utils.get_scm('lin', 'health') if scm_on else None\n    Jw = w if scm_obj is None else scm_obj.get_Jacobian().T @ w\n    dual_norm = np.sqrt(Jw.T @ Jw)\n    explainer = recourse.LinearRecourse(w, b + dual_norm * eps)\n    _, valids, costs, _, _ = recourse.causal_recourse(X_exp, explainer, cons, scm=scm_obj, verbose=False)\n    valids = np.asarray(valids).astype(bool)\n    costs = np.asarray(costs)\n    return float(valids.mean()), float(costs[valids].mean()) if valids.any() else np.nan\n\nrows = []\nfor eps in [0.0, 0.1, 0.2]:\n    for scm_on in [False, True]:\n        vr, vc = eval_eps(eps, scm_on)\n        rows.append({'epsilon': eps, 'method': 'SCM-on' if scm_on else 'SCM-off', 'valid_rate': vr, 'valid_cost': vc})\n\nq5_eps = pd.DataFrame(rows)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\nsns.lineplot(data=q5_eps, x='epsilon', y='valid_rate', hue='method', marker='o', ax=axes[0])\nsns.lineplot(data=q5_eps, x='epsilon', y='valid_cost', hue='method', marker='o', ax=axes[1])\naxes[0].set_title('Validity vs epsilon')\naxes[1].set_title('Valid cost vs epsilon')\nfig.tight_layout()\nplt.show()\n\nq5_eps\n",
      "id": "f5b9f8a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Q5-F Interpretation\n\n- `q5_summary` directly compares SCM-off and SCM-on under matched sample conditions.\n- `q5_example` reports one individual with intervention features and costs for both methods.\n- `q5_per_instance` provides full row-level comparison and cost gaps (`off - on`) for auditing.\n",
      "id": "b56c831e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Q6 - Theoretical Answers (Robust Causal Recourse)\n\n### Q6-1: When is robustness guaranteed?\nRobustness guarantees in causal algorithmic recourse are strongest when:\n1. The classifier is linear (or locally well-approximated linearly around interventions).\n2. The SCM is correctly specified and differentiable (linear SCM gives strongest closed-form guarantees).\n3. Action sets are convex/closed with explicit feasibility constraints.\n4. Uncertainty is bounded (for example \\(\\|\\delta\\|_2 \\le \\epsilon\\)).\n5. Robust optimization is solved with the correct dual-norm margin shift.\n\nIntuition: robust recourse must remain valid under worst-case perturbations, so the decision boundary is tightened by a safety margin induced by uncertainty geometry and causal propagation.\n\n### Q6-2: Intuition behind Proposition 4 / Eq. (5)\nFor linear score \\(g(x)=w^Tx-b\\) and SCM Jacobian \\(J\\), uncertainty in intervention/factual space propagates along \\(J\\). Worst-case perturbation contributes the support-function term of the uncertainty set, yielding the robust shift:\n\\[\n w^T(x+Ja) \\ge b + \\epsilon\\|J^Tw\\|_*.\n\\]\nFor \\(\\ell_2\\)-bounded uncertainty, \\(\\|\\cdot\\|_* = \\|\\cdot\\|_2\\), so the margin is \\(\\epsilon\\|J^Tw\\|_2\\).\nThis explains Eq. (5): recourse must clear not just the nominal boundary but an uncertainty-amplified boundary that depends on classifier sensitivity projected through causal structure.\n",
      "id": "981875a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Save Key Artifacts\n\nThe next cell exports compact CSV summaries from this notebook run.\n",
      "id": "4230eddd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "q1_res.to_csv(OUT_DIR / 'q1_results.csv', index=False)\nq2_res.to_csv(OUT_DIR / 'q2_results.csv', index=False)\nq3b_stats.to_csv(OUT_DIR / 'q3_scm_fit_stats.csv', index=False)\nq3c.to_csv(OUT_DIR / 'q3_variance_decomposition.csv', index=False)\nimportance.to_csv(OUT_DIR / 'q3_factor_importance.csv', index=False)\nq4_res.to_csv(OUT_DIR / 'q4_estimators_curve.csv', index=False)\nq4_summary.to_csv(OUT_DIR / 'q4_estimators_summary.csv', index=False)\nq5_summary.to_csv(OUT_DIR / 'q5_summary.csv', index=False)\nq5_per_instance.to_csv(OUT_DIR / 'q5_per_instance.csv', index=False)\nq5_example.to_csv(OUT_DIR / 'q5_example.csv', index=False)\nq5_eps.to_csv(OUT_DIR / 'q5_epsilon_sweep.csv', index=False)\n\nprint('Saved notebook artifacts under:', OUT_DIR)\nfor p in sorted(OUT_DIR.glob('*.csv')):\n    print('-', p.name)\n",
      "id": "f930f9da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Re-run Commands\n\n```bash\nsource /Users/tahamajs/Documents/uni/venv/bin/activate\ncd /Users/tahamajs/Documents/uni/truthlyAI/HomeWorks/HW3\n\n# open notebook\njupyter lab output/jupyter-notebook/hw3_complete_assignment.ipynb\n```\n\nThis notebook is organized to run top-to-bottom deterministically.\n",
      "id": "18fb0b5a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}